{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from source.utils import *\n",
    "from source.classes import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and design the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Desktop/DLC_social_1/DLC_social_1_exp_conditions.pickle', 'rb') as handle:\n",
    "    Treatment_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which angles to compute?\n",
    "bp_dict = {'B_Nose':['B_Left_ear','B_Right_ear'],\n",
    "          'B_Left_ear':['B_Nose','B_Right_ear','B_Center','B_Left_flank'],\n",
    "          'B_Right_ear':['B_Nose','B_Left_ear','B_Center','B_Right_flank'],\n",
    "          'B_Center':['B_Left_ear','B_Right_ear','B_Left_flank','B_Right_flank','B_Tail_base'],\n",
    "          'B_Left_flank':['B_Left_ear','B_Center','B_Tail_base'],\n",
    "          'B_Right_flank':['B_Right_ear','B_Center','B_Tail_base'],\n",
    "          'B_Tail_base':['B_Center','B_Left_flank','B_Right_flank']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DLC_social_1 = project(path='../../Desktop/DLC_social_1/',#Path where to find the required files\n",
    "                   smooth_alpha=0.85,                    #Alpha value for exponentially weighted smoothing\n",
    "                   distances=['B_Center','B_Nose','B_Left_ear','B_Right_ear','B_Left_flank',\n",
    "                              'B_Right_flank','B_Tail_base'],\n",
    "                   ego=False,\n",
    "                   angles=True,\n",
    "                   connectivity=bp_dict,\n",
    "                   arena='circular',                  #Type of arena used in the experiments\n",
    "                   arena_dims=[380],                  #Dimensions of the arena. Just one if it's circular\n",
    "                   video_format='.mp4',\n",
    "                   table_format='.h5',\n",
    "                   exp_conditions=Treatment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DLC_social_1_coords = DLC_social_1.run(verbose=True)\n",
    "print(DLC_social_1_coords)\n",
    "type(DLC_social_1_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ptest = DLC_social_1_coords.get_coords(center=True, polar=False, speed=0, length='00:10:00')\n",
    "ptest._type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtest = DLC_social_1_coords.get_distances(speed=0, length='00:10:00')\n",
    "dtest._type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "atest = DLC_social_1_coords.get_angles(degrees=True, speed=0, length='00:10:00')\n",
    "atest._type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptest.plot_heatmaps(['B_Center', 'W_Center'], i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot animation of trajectory over time with different smoothings\n",
    "plt.plot(ptest['Day2Test13DLC']['B_Center'].iloc[:5000]['x'],\n",
    "         ptest['Day2Test13DLC']['B_Center'].iloc[:5000]['y'], label='alpha=0.85')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Mouse Center Trajectory using different exponential smoothings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ptest.pca(4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*pca[0].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtest = merge_tables(DLC_social_1_coords.get_coords(center=True, polar=True, length='00:10:00'))#,\n",
    "#                      DLC_social_1_coords.get_distances(speed=0, length='00:10:00'),\n",
    "#                      DLC_social_1_coords.get_angles(degrees=True, speed=0, length='00:10:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pptest = mtest.preprocess(window_size=51, filter='gaussian', sigma=10, shift=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pttest = mtest.preprocess(window_size=51, filter=None)\n",
    "pttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pttest[2,:,2], label='normal')\n",
    "plt.plot(pptest[2,:,2], label='gaussian')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained models playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq 2 seq Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pttest = pttest[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_filters = 64\n",
    "LSTM_units_1 = 128\n",
    "LSTM_units_2 = 64\n",
    "DENSE_1 = 64\n",
    "DENSE_2 = 32\n",
    "ENCODING = 20\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "original_dim = pttest.shape[1:]\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.hypermodels import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Lambda, Bidirectional, LSTM\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Encoder Layers\n",
    "Model_E0 = tf.keras.layers.Conv1D(\n",
    "    filters=CONV_filters,\n",
    "    kernel_size=5,\n",
    "    strides=1,\n",
    "    padding=\"causal\",\n",
    "    activation=\"relu\",\n",
    ")\n",
    "Model_E1 = Bidirectional(\n",
    "    LSTM(\n",
    "        LSTM_units_1,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=True,\n",
    "        kernel_constraint=UnitNorm(axis=0),\n",
    "    )\n",
    ")\n",
    "Model_E2 = Bidirectional(\n",
    "    LSTM(\n",
    "        LSTM_units_2,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=False,\n",
    "        kernel_constraint=UnitNorm(axis=0),\n",
    "    )\n",
    ")\n",
    "Model_E3 = Dense(DENSE_1, activation=\"relu\", kernel_constraint=UnitNorm(axis=0))\n",
    "Model_E4 = Dense(DENSE_2, activation=\"relu\", kernel_constraint=UnitNorm(axis=0))\n",
    "Model_E5 = Dense(\n",
    "            ENCODING,\n",
    "            activation=\"relu\",\n",
    "            kernel_constraint=UnitNorm(axis=1),\n",
    "            activity_regularizer=UncorrelatedFeaturesConstraint(3, weightage=1.0),\n",
    "        )\n",
    "\n",
    "# Decoder layers\n",
    "Model_D4 = Bidirectional(\n",
    "    LSTM(\n",
    "        LSTM_units_1,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=True,\n",
    "        kernel_constraint=UnitNorm(axis=1),\n",
    "    )\n",
    ")\n",
    "Model_D5 = Bidirectional(\n",
    "    LSTM(\n",
    "        LSTM_units_1,\n",
    "        activation=\"sigmoid\",\n",
    "        return_sequences=True,\n",
    "        kernel_constraint=UnitNorm(axis=1),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define and instanciate encoder\n",
    "x = Input(shape=original_dim)\n",
    "encoder = Model_E0(x)\n",
    "encoder = Model_E1(encoder)\n",
    "encoder = Model_E2(encoder)\n",
    "encoder = Model_E3(encoder)\n",
    "encoder = Dropout(DROPOUT_RATE)(encoder)\n",
    "encoder = Model_E4(encoder)\n",
    "encoder = Model_E5(encoder)\n",
    "z_mean = Dense(ENCODING)(encoder)\n",
    "z_log_sigma = Dense(ENCODING)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args, epsilon_std=1.):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=K.shape(z_mean),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and instanciate decoder\n",
    "decoder = DenseTranspose(Model_E5, activation=\"relu\", output_dim=ENCODING)(z)\n",
    "decoder = DenseTranspose(Model_E4, activation=\"relu\", output_dim=DENSE_2)(decoder)\n",
    "decoder = DenseTranspose(Model_E3, activation=\"relu\", output_dim=DENSE_1)(decoder)\n",
    "decoder = RepeatVector(pttest.shape[1])(decoder)\n",
    "decoder = Model_D4(decoder)\n",
    "decoder = Model_D5(decoder)\n",
    "x_decoded_mean = TimeDistributed(Dense(original_dim[1]))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "klvae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(ENCODING,))\n",
    "decoder = DenseTranspose(Model_E5, activation=\"relu\", output_dim=ENCODING)(decoder_input)\n",
    "decoder = DenseTranspose(Model_E4, activation=\"relu\", output_dim=DENSE_2)(decoder)\n",
    "decoder = DenseTranspose(Model_E3, activation=\"relu\", output_dim=DENSE_1)(decoder)\n",
    "decoder = RepeatVector(pttest.shape[1])(decoder)\n",
    "decoder = Model_D4(decoder)\n",
    "decoder = Model_D5(decoder)\n",
    "x_decoded_mean = TimeDistributed(Dense(original_dim[1]))(decoder)\n",
    "generator = Model(decoder_input, x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = K.shape(x)[0]\n",
    "    y_size = K.shape(y)[0]\n",
    "    dim = K.shape(x)[1] * K.shape(x)[2]\n",
    "    tiled_x = K.tile(K.reshape(x, K.stack([x_size, 1, dim])), K.stack([1, y_size, 1]))\n",
    "    tiled_y = K.tile(K.reshape(y, K.stack([1, y_size, dim])), K.stack([x_size, 1, 1]))\n",
    "    return K.exp(-tf.reduce_mean(K.square(tiled_x - tiled_y), axis=2) / K.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y, sigma_sqr=1.0):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    huber_loss = Huber(reduction=\"sum\", delta=100.0)\n",
    "    huber_loss = original_dim * huber_loss(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return K.mean(huber_loss + kl_loss[:, None])\n",
    "\n",
    "def vae_mmd_loss(x, x_decoded_mean):\n",
    "    huber_loss = Huber(reduction=\"sum\", delta=100.0)\n",
    "    huber_loss = original_dim * huber_loss(x, x_decoded_mean)\n",
    "    mmd_loss = compute_mmd(x, x_decoded_mean)\n",
    "    return huber_loss + mmd_loss\n",
    "\n",
    "klvae.compile(optimizer='adam', loss=vae_loss, experimental_run_tf_function=False, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(False)\n",
    "ptrain = pttest[np.random.choice(pttest.shape[0], 1000, replace=False), :, :]\n",
    "klhistory = klvae.fit(ptrain, ptrain, epochs=50, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history.history['mae'], label='Huber + MMD mae')\n",
    "plt.plot(klhistory.history['mae'], label='Huber + KL mae')\n",
    "plt.plot(hhistory.history['mae'], label='Huber mae')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Huber loss + MMD/ELBO in training data\n",
    "plt.plot(pttest[:500,0,0], label='data')\n",
    "#plt.plot(vae.predict(pttest[:500])[:,0,0], label='MMD reconstruction')\n",
    "plt.plot(klvae.predict(pttest[:500])[:,0,0], label='ELBO reconstruction')\n",
    "plt.plot(hvae.predict(pttest[:500])[:,0,0], label='Huber reconstruction')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
