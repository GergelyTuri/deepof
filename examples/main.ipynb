{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepof.data\n",
    "import deepof.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first batch\n",
    "dset11 = pd.ExcelFile(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_1/DLC_single_CDR1_1/1.Openfield_data-part1/JB05.1-OF-SI-part1.xlsx\"\n",
    ")\n",
    "dset12 = pd.ExcelFile(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_1/DLC_single_CDR1_1/2.Openfielddata-part2/AnimalID's-JB05.1-part2.xlsx\"\n",
    ")\n",
    "dset11 = pd.read_excel(dset11, \"Tabelle2\")\n",
    "dset12 = pd.read_excel(dset12, \"Tabelle2\")\n",
    "\n",
    "dset11.Test = dset11.Test.apply(lambda x: \"Test {}_s1.1\".format(x))\n",
    "dset12.Test = dset12.Test.apply(lambda x: \"Test {}_s1.2\".format(x))\n",
    "\n",
    "dset1 = {\"CSDS\":list(dset11.loc[dset11.Treatment.isin([\"CTR+CSDS\",\"NatCre+CSDS\"]), \"Test\"]) + \n",
    "                list(dset12.loc[dset12.Treatment.isin([\"CTR+CSDS\",\"NatCre+CSDS\"]), \"Test\"]),\n",
    "         \"NS\":  list(dset11.loc[dset11.Treatment.isin([\"CTR+nonstressed\",\"NatCre+nonstressed\"]), \"Test\"]) + \n",
    "                list(dset12.loc[dset12.Treatment.isin([\"CTR+nonstressed\",\"NatCre+nonstressed\"]), \"Test\"]),}\n",
    "\n",
    "dset1inv = {}\n",
    "for i in flatten(list(dset1.values())):\n",
    "    if i in dset1[\"CSDS\"]:\n",
    "        dset1inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset1inv[i] = \"NS\"\n",
    "        \n",
    "assert len(dset1inv) == dset11.shape[0] + dset12.shape[0], \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second batch\n",
    "dset21 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_2/Part1/2_Single/stressproject22.04.2020genotypes-openfieldday1.xlsx\"\n",
    ")\n",
    "dset22 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_2/Part2/2_Single/OpenFieldvideos-part2.xlsx\"\n",
    ")\n",
    "dset21.Test = dset21.Test.apply(lambda x: \"Test {}_s2.1\".format(x))\n",
    "dset22.Test = dset22.Test.apply(lambda x: \"Test {}_s2.2\".format(x))\n",
    "\n",
    "dset2 = {\"CSDS\":list(dset21.loc[dset21.Treatment == \"Stress\", \"Test\"]) + \n",
    "                list(dset22.loc[dset22.Treatment == \"Stressed\", \"Test\"]),\n",
    "         \"NS\":  list(dset21.loc[dset21.Treatment == \"Nonstressed\", \"Test\"]) +\n",
    "                list(dset22.loc[dset22.Treatment == \"Nonstressed\", \"Test\"])}\n",
    "\n",
    "dset2inv = {}\n",
    "for i in flatten(list(dset2.values())):\n",
    "    if i in dset2[\"CSDS\"]:\n",
    "        dset2inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset2inv[i] = \"NS\"\n",
    "        \n",
    "assert len(dset2inv) == dset21.shape[0] + dset22.shape[0], \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load third batch\n",
    "\n",
    "dset31 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_3/1.Day2OF-SIpart1/JB05 2Female-ELS-OF-SIpart1.xlsx\"\n",
    ")\n",
    "dset32 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_3/2.Day3OF-SIpart2/JB05 2FEMALE-ELS-OF-SIpart2.xlsx\"\n",
    ")\n",
    "dset31.Test = dset31.Test.apply(lambda x: \"Test {}_s3.1\".format(x))\n",
    "dset32.Test = dset32.Test.apply(lambda x: \"Test {}_s3.2\".format(x))\n",
    "\n",
    "dset3 = {\"CSDS\":[],\n",
    "         \"NS\":  list(dset31.loc[:, \"Test\"]) +\n",
    "                list(dset32.loc[:, \"Test\"])}\n",
    "\n",
    "dset3inv = {}\n",
    "for i in flatten(list(dset3.values())):\n",
    "    if i in dset3[\"CSDS\"]:\n",
    "        dset3inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset3inv[i] = \"NS\"\n",
    "        \n",
    "assert len(dset3inv) == dset31.shape[0] + dset32.shape[0], \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fourth batch\n",
    "dset41 = os.listdir(\"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_4/JB05.4-OpenFieldvideos/\")\n",
    "\n",
    "# Remove empty video!\n",
    "dset41 = [vid for vid in dset41 if \"52\" not in vid]\n",
    "\n",
    "dset4 = {\"CSDS\":[],\n",
    "         \"NS\":  [i[:-4]+\"_s4\" for i in dset41]}\n",
    "\n",
    "dset4inv = {}\n",
    "for i in flatten(list(dset4.values())):\n",
    "    if i in dset4[\"CSDS\"]:\n",
    "        dset4inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset4inv[i] = \"NS\"\n",
    "        \n",
    "assert len(dset4inv) == len(dset41), \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge phenotype dicts and serialise!\n",
    "aggregated_dset = {**dset1inv, **dset2inv, **dset3inv, **dset4inv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NS': 115, 'CSDS': 52})\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(aggregated_dset.values()))\n",
    "print(115+52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and run project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 1.94 s, total: 12.2 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deepof_main = deepof.data.project(path=os.path.join(\"..\",\"..\",\"Desktop\",\"deepof-data\",\"tagged_videos\",\"phenotest\"),\n",
    "                                  smooth_alpha=0.99,                                     \n",
    "                                  arena_dims=[380],\n",
    "                                  exp_conditions=dset2inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trajectories...\n",
      "Smoothing trajectories...\n",
      "Computing distances...\n",
      "Computing angles...\n",
      "Done!\n",
      "CPU times: user 10.3 s, sys: 729 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deepof_main = deepof_main.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quality = pd.concat([tab for tab in deepof_main.get_quality().values()]).droplevel(\"scorer\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quality.boxplot(rot=45)\n",
    "plt.ylim(0.99985, 1.00001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(quality_top=(0., 1., 0.01))\n",
    "def low_quality_tags(quality_top):\n",
    "    pd.DataFrame(pd.melt(all_quality).groupby(\"bodyparts\").value.apply(\n",
    "        lambda y: sum(y<quality_top) / len(y) * 100)\n",
    "                ).sort_values(by=\"value\", ascending=False).plot.bar(rot=45)\n",
    "    \n",
    "    plt.xlabel(\"body part\")\n",
    "    plt.ylabel(\"Tags with quality under {} (%)\".format(quality_top))\n",
    "    plt.tight_layout()\n",
    "    plt.legend([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "deepof_coords = deepof_main.get_coords(center=\"Center\", polar=False, speed=0, align=\"Spine_1\", align_inplace=True, propagate_labels=True)\n",
    "deepof_dists  = deepof_main.get_distances(propagate_labels=False)\n",
    "deepof_angles = deepof_main.get_angles(propagate_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfencs = pd.read_hdf('../../Desktop/dash_data_1_20201120-141341.h5')\n",
    "dfencs.cluster = dfencs.cluster.astype(str) + \"a\"\n",
    "clust_occur = pd.read_hdf('../../Desktop/dash_data_2_20201120-141341.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette(\"tab10\", n_colors=10)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "sns.barplot(data=clust_occur.loc[clust_occur.reset_index().epoch==1,:], x=\"cluster\", y=\"count\", ax=ax1, \n",
    "            palette=pal)\n",
    "sns.barplot(data=clust_occur.loc[clust_occur.reset_index().epoch==2,:], x=\"cluster\", y=\"count\", ax=ax2, \n",
    "            palette=pal)\n",
    "\n",
    "sns.scatterplot(data=dfencs.loc[dfencs.epoch==1,:], x=\"x\", y=\"y\", hue=\"cluster\", legend=False, cmap=\"jet\", ax=ax3, alpha=0.4,\n",
    "                palette=pal, size=1, edgecolor=None)\n",
    "sns.scatterplot(data=dfencs.loc[dfencs.epoch==2,:], x=\"x\", y=\"y\", hue=\"cluster\", legend=False, cmap=\"jet\", ax=ax4, alpha=0.4,\n",
    "                palette=pal, size=1, edgecolor=None)\n",
    "\n",
    "ax1.set_xlabel(\"Cluster number\")\n",
    "ax2.set_xlabel(\"Cluster number\")\n",
    "ax3.set_xlabel(\"UMAP 1\")\n",
    "ax4.set_xlabel(\"UMAP 1\")\n",
    "\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax3.set_ylabel(\"UMAP 2\")\n",
    "ax4.set_ylabel(\"UMAP 2\")\n",
    "\n",
    "ax1.set_ylim(0,2000)\n",
    "ax2.set_ylim(0,2000)\n",
    "\n",
    "ax3.set_xlim(0.58,0.88)\n",
    "ax3.set_ylim(0.35,0.65)\n",
    "\n",
    "ax4.set_xlim(0.1,1.1)\n",
    "ax4.set_ylim(0.1,1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(\"../../Desktop/udeepof-embedding.tiff\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = deepof_coords.plot_heatmaps(['Nose'], i=1, dpi=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Preprocessing training set...\")\n",
    "deepof_train = deepof_coords.preprocess(\n",
    "    window_size=11,\n",
    "    window_step=11,\n",
    "    conv_filter=None,\n",
    "    scale=\"standard\",\n",
    "    shuffle=True,\n",
    "    test_videos=0,\n",
    ")\n",
    "\n",
    "print(\"Loading pre-trained model...\")\n",
    "encoder, _, grouper, gmvaep, = deepof.models.SEQ_2_SEQ_GMVAE(\n",
    "    loss=\"ELBO\",\n",
    "    number_of_components=10,\n",
    "    kl_warmup_epochs=20,\n",
    "    mmd_warmup_epochs=0,\n",
    "    predictor=0,\n",
    "    phenotype_prediction=0,\n",
    ").build(deepof_train.shape)[:4]\n",
    "\n",
    "# gmvaep.load_weights(\n",
    "#     \"../../Desktop/deepof-data/trained_deepof_models/GMVAE_loss=ELBO_20201015-031219_final_weights.h5\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(gmvaep, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10000\n",
    "\n",
    "all_clusters = grouper.predict(deepof_train[:samples])\n",
    "all_encodings = encoder.predict(deepof_train[:samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_input = deepof.data.table_dict({\"Test 10_s2\":deepof_coords[\"Test 10_s2\"]}, typ=\"coords\").preprocess(\n",
    "                                                                                                window_size=11,\n",
    "                                                                                                window_step=1,\n",
    "                                                                                                conv_filter=None,\n",
    "                                                                                                scale=\"standard\",\n",
    "                                                                                                shuffle=False,\n",
    "                                                                                                test_videos=0,\n",
    "                                                                                            )\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(deepof_coords[\"Test 10_s2\"])\n",
    "\n",
    "# Get reconstruction\n",
    "video_pred = gmvaep.predict(video_input)[:, 6, :]\n",
    "\n",
    "# Get encodings\n",
    "video_clusters = grouper.predict(video_input)\n",
    "video_encodings = encoder.predict(video_input)\n",
    "\n",
    "scaled_video_pred = scaler.inverse_transform(video_pred)\n",
    "scaled_video_input = scaler.inverse_transform(video_input[:, 6, :])\n",
    "\n",
    "scaled_video_input = pd.DataFrame(scaled_video_input, columns=deepof_coords[\"Test 10_s2\"].columns)\n",
    "scaled_video_pred = pd.DataFrame(scaled_video_pred, columns=deepof_coords[\"Test 10_s2\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encodings(data, samples, n, clusters, threshold, highlight=None):\n",
    "    \n",
    "    reducer  = LinearDiscriminantAnalysis(n_components=n)\n",
    "    clusters = clusters[:samples, :]\n",
    "    \n",
    "    # filter   = np.max(np.mean(clusters, axis=0), axis=1) > threshold\n",
    "    \n",
    "    clusters = np.argmax(clusters, axis=1)#[filter]\n",
    "    rep = reducer.fit_transform(data[:samples], clusters)\n",
    "\n",
    "    if n == 2:\n",
    "        df = pd.DataFrame({\"encoding-1\":rep[:,0],\"encoding-2\":rep[:,1],\"clusters\":[\"A\"+str(i) for i in clusters]})\n",
    "\n",
    "        enc = px.scatter(data_frame=df, x=\"encoding-1\", y=\"encoding-2\",\n",
    "                           color=\"clusters\", width=600, height=600,\n",
    "                           color_discrete_sequence=px.colors.qualitative.T10)\n",
    "                \n",
    "        #if highlight:\n",
    "        #    ig.add_trace(go.Scatter(x=, y=)\n",
    "\n",
    "    elif n == 3:\n",
    "        df3d = pd.DataFrame({\"encoding-1\":rep[:,0],\"encoding-2\":rep[:,1],\"encoding-3\":rep[:,2],\n",
    "                         \"clusters\":[\"A\"+str(i) for i in clusters]})\n",
    "\n",
    "        enc = px.scatter_3d(data_frame=df3d, x=\"encoding-1\", y=\"encoding-2\", z=\"encoding-3\",\n",
    "                           color=\"clusters\", width=600, height=600,\n",
    "                           color_discrete_sequence=px.colors.qualitative.T10)\n",
    "\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_encodings(all_encodings, 10000, 2, all_clusters, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft: function to produce a video with the animal in motion using cv2\n",
    "import cv2\n",
    "\n",
    "w = 400\n",
    "h = 400\n",
    "factor = 2.5\n",
    "\n",
    "# Instantiate video\n",
    "writer = cv2.VideoWriter()\n",
    "writer.open(\n",
    "    \"test_video.avi\",\n",
    "    cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "    24,\n",
    "    (int(w * factor), int(h * factor)),\n",
    "    True,\n",
    ")\n",
    "\n",
    "for frame in tqdm.tqdm(range(500)):\n",
    "\n",
    "    image = np.zeros((h, w, 3), np.uint8) + 30\n",
    "    for bpart in video_input.columns.levels[0]:\n",
    "\n",
    "        try:\n",
    "            pos = (\n",
    "                (-int(video_input[bpart].loc[frame, \"x\"]) + w // 2),\n",
    "                (-int(video_input[bpart].loc[frame, \"y\"]) + h // 2),\n",
    "            )\n",
    "\n",
    "            pos_pred = (\n",
    "                (-int(video_pred[bpart].loc[frame, \"x\"]) + w // 2),\n",
    "                (-int(video_pred[bpart].loc[frame, \"y\"]) + h // 2),\n",
    "            )\n",
    "\n",
    "            cv2.circle(image, pos, 2, (0, 0, 255), -1)\n",
    "            cv2.circle(image, pos_pred, 2, (0, 255, 0), -1)\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    # draw skeleton\n",
    "    def draw_line(start, end, df, col):\n",
    "        for bpart in end:\n",
    "            cv2.line(\n",
    "                image,\n",
    "                tuple(-df[start].loc[frame, :].astype(int) + w // 2),\n",
    "                tuple(-df[bpart].loc[frame, :].astype(int) + h // 2),\n",
    "                col,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "    for df, col in zip([video_input, video_pred], [(0,0,255),(0,255,0)]):\n",
    "        draw_line(\"Nose\", [\"Left_ear\", \"Right_ear\"], df, col)\n",
    "        draw_line(\"Spine_1\", [\"Left_ear\", \"Right_ear\", \"Left_fhip\", \"Right_fhip\"], df, col)\n",
    "        draw_line(\"Spine_2\", [\"Spine_1\", \"Tail_base\", \"Left_bhip\", \"Right_bhip\"], df, col)\n",
    "        draw_line(\"Tail_1\", [\"Tail_base\", \"Tail_2\"], df, col)\n",
    "        draw_line(\"Tail_tip\", [\"Tail_2\"], df, col)\n",
    "\n",
    "    image = cv2.resize(image, (0, 0), fx=factor, fy=factor)\n",
    "    writer.write(image)\n",
    "\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, y_train, X_test, y_test = deepof_coords.preprocess(window_size=11, window_step=11, conv_filter=None, sigma=55,\n",
    "                                                            shift=0, scale='standard', align='all', shuffle=True, test_videos=5)\n",
    "print(\"Train dataset shape: \", X_train.shape)\n",
    "print(\"Train dataset shape: \", y_train.shape)\n",
    "print(\"Test dataset shape: \", X_test.shape)\n",
    "print(\"Test dataset shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models and get learning rate (1-cycle policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq 2 seq Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow.keras as k\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'Baseline_AE'\n",
    "log_dir = os.path.abspath(\n",
    "    \"logs/fit/{}_{}\".format(NAME, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    ")\n",
    "tensorboard_callback = k.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepof.models import SEQ_2_SEQ_AE, SEQ_2_SEQ_GMVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, ae = SEQ_2_SEQ_AE().build(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoder, generator, grouper, gmvaep, kl_warmup_callback, mmd_warmup_callback = SEQ_2_SEQ_GMVAE(loss='ELBO',\n",
    "                                                                               compile_model=True,\n",
    "                                                                               number_of_components=10,\n",
    "                                                                               kl_warmup_epochs=20,\n",
    "                                                                               mmd_warmup_epochs=0,\n",
    "                                                                               predictor=0,\n",
    "                                                                               phenotype_prediction=0,\n",
    "                                                                               architecture_hparams={\"encoding\":2}\n",
    "                                                                                ).build(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "rates, losses = deepof.model_utils.find_learning_rate(gmvaep, deepof_train[:512*10], deepof_test[:512*10], epochs=1, batch_size=batch_size)\n",
    "deepof.model_utils.plot_lr_vs_loss(rates, losses)\n",
    "plt.title(\"Learning rate tuning\")\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gmvaep.fit(\n",
    "                    x=X_train,\n",
    "                    y=X_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=128,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, [X_test, y_test]),\n",
    "                    callbacks=[kl_warmup_callback],\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pttest\n",
    "samples = 15000\n",
    "montecarlo = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \"GMVAE_components=30_loss=ELBO_kl_warmup=30_mmd_warmup=30_20200804-225526_final_weights.h5\"\n",
    "\n",
    "gmvaep.load_weights(weights)\n",
    "\n",
    "if montecarlo:\n",
    "    clusts = np.stack([grouper(data[:samples]) for sample in (tqdm(range(montecarlo)))])\n",
    "    clusters = clusts.mean(axis=0)\n",
    "    clusters = np.argmax(clusters, axis=1)\n",
    "    \n",
    "else:\n",
    "    clusters = grouper(data[:samples], training=False)\n",
    "\n",
    "    \n",
    "    clusters = np.argmax(clusters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encodings(data, samples, n, clusters, threshold):\n",
    "    \n",
    "    reducer  = PCA(n_components=n)\n",
    "    clusters = clusters[:, :samples]\n",
    "    filter   = np.max(np.mean(clusters, axis=0), axis=1) > threshold\n",
    "    encoder.predict(data[:samples][filter])\n",
    "    print(\"{}/{} samples used ({}%); confidence threshold={}\".format(sum(filter),\n",
    "                                                                     samples,\n",
    "                                                                     sum(filter)/samples*100,\n",
    "                                                                     threshold))\n",
    "    \n",
    "    clusters = np.argmax(np.mean(clusters, axis=0), axis=1)[filter]\n",
    "    rep = reducer.fit_transform(encoder.predict(data[:samples][filter]))\n",
    "\n",
    "    if n == 2:\n",
    "        df = pd.DataFrame({\"encoding-1\":rep[:,0],\"encoding-2\":rep[:,1],\"clusters\":[\"A\"+str(i) for i in clusters]})\n",
    "\n",
    "        enc = px.scatter(data_frame=df, x=\"encoding-1\", y=\"encoding-2\",\n",
    "                           color=\"clusters\", width=600, height=600,\n",
    "                           color_discrete_sequence=px.colors.qualitative.T10)\n",
    "\n",
    "\n",
    "    elif n == 3:\n",
    "        df3d = pd.DataFrame({\"encoding-1\":rep[:,0],\"encoding-2\":rep[:,1],\"encoding-3\":rep[:,2],\n",
    "                         \"clusters\":[\"A\"+str(i) for i in clusters]})\n",
    "\n",
    "        enc = px.scatter_3d(data_frame=df3d, x=\"encoding-1\", y=\"encoding-2\", z=\"encoding-3\",\n",
    "                           color=\"clusters\", width=600, height=600,\n",
    "                           color_discrete_sequence=px.colors.qualitative.T10)\n",
    "\n",
    "    return enc\n",
    "\n",
    "plot_encodings(data, 5000, 2, clusts, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution per cluster\n",
    "for cl in range(5):\n",
    "    cl_select = np.argmax(np.mean(clusts, axis=0), axis=1) == cl\n",
    "    dt = np.mean(clusts[:,cl_select,cl], axis=0)\n",
    "    sns.kdeplot(dt, shade=True, label=cl)\n",
    "    \n",
    "plt.xlabel('MC Dropout confidence')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animated_cluster_heatmap(data, clust, clusters, threshold=0.75, samples=False):\n",
    "    \n",
    "    if not samples:\n",
    "        samples = data.shape[0]\n",
    "    tpoints = data.shape[1]\n",
    "    bdparts = data.shape[2] // 2\n",
    "    \n",
    "    cls = clusters[:,:samples,:]\n",
    "    filt = np.max(np.mean(cls, axis=0), axis=1) > threshold\n",
    "    \n",
    "    cls = np.argmax(np.mean(cls, axis=0), axis=1)[filt]\n",
    "    clust_series = data[:samples][filt][cls==clust]\n",
    "        \n",
    "    rshape = clust_series.reshape(clust_series.shape[0]*clust_series.shape[1],\n",
    "                                  clust_series.shape[2])\n",
    "    \n",
    "    cluster_df = pd.DataFrame()\n",
    "    cluster_df['x'] = rshape[:,[0,2,4,6,8,10]].flatten(order='F')\n",
    "    cluster_df['y'] = rshape[:,[1,3,5,7,9,11]].flatten(order='F')\n",
    "    cluster_df['bpart'] = np.tile(np.repeat(np.arange(bdparts),\n",
    "                                            clust_series.shape[0]), tpoints)\n",
    "    cluster_df['frame'] = np.tile(np.repeat(np.arange(tpoints),\n",
    "                                            clust_series.shape[0]), bdparts)\n",
    "        \n",
    "    fig = px.density_contour(data_frame=cluster_df, x='x', y='y', animation_frame='frame',\n",
    "                     width=600, height=600, \n",
    "                     color='bpart',color_discrete_sequence=px.colors.qualitative.T10)\n",
    "\n",
    "    fig.update_traces(contours_coloring=\"fill\", \n",
    "                      contours_showlabels = True)\n",
    "    \n",
    "    fig.update_xaxes(range=[-3, 3])\n",
    "    fig.update_yaxes(range=[-3, 3])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animated_cluster_heatmap(pttest, 4, clusts, samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [i for i in os.listdir() if \"GMVAE\" in i and \".h5\" in i]\n",
    "mult_clusters = np.zeros([len(weights), samples])\n",
    "mean_conf = []\n",
    "\n",
    "for k,i in tqdm(enumerate(sorted(weights))):\n",
    "    print(i)\n",
    "    gmvaep.load_weights(i)\n",
    "\n",
    "    if montecarlo:\n",
    "        clusters = np.stack([grouper(data[:samples]) for sample in (tqdm(range(montecarlo)))])\n",
    "        clusters = clusters.mean(axis=0)\n",
    "        mean_conf.append(clusters.max(axis=1))\n",
    "        clusters = np.argmax(clusters, axis=1)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        clusters = grouper(data[:samples], training=False)\n",
    "        mean_conf.append(clusters.max(axis=1))\n",
    "        clusters = np.argmax(clusters, axis=1)\n",
    "        \n",
    "    mult_clusters[k] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.95\n",
    "ari_dist = []\n",
    "\n",
    "for i,k in enumerate(combinations(range(len(weights)),2)):\n",
    "    filt = ((mean_conf[k[0]] > thr) & (mean_conf[k[1]]>thr))\n",
    "    \n",
    "    ari = adjusted_rand_score(mult_clusters[k[0]][filt],\n",
    "                              mult_clusters[k[1]][filt])\n",
    "    \n",
    "    ari_dist.append(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ari = []\n",
    "for i in tqdm(range(6)):\n",
    "    random_ari.append(adjusted_rand_score(np.random.uniform(0,6,50).astype(int),\n",
    "                                          np.random.uniform(0,6,50).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ari_dist, label=\"ARI gmvaep\", shade=True)\n",
    "sns.kdeplot(random_ari, label=\"ARI random\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Normalised Adjusted Rand Index\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster differences across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "DLCS1_coords = DLC_social_1_coords.get_coords(center=\"B_Center\",polar=False, length='00:10:00', align='B_Nose')\n",
    "\n",
    "Treatment_coords = {}\n",
    "\n",
    "for cond in Treatment_dict.keys():\n",
    "    Treatment_coords[cond] = DLCS1_coords.filter(Treatment_dict[cond]).preprocess(window_size=13, \n",
    "                                                 window_step=10, filter=None, scale='standard', align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "montecarlo = 10\n",
    "\n",
    "Predictions_per_cond = {}\n",
    "Confidences_per_cond = {}\n",
    "\n",
    "for cond in Treatment_dict.keys():\n",
    "    \n",
    "    Predictions_per_cond[cond] = np.stack([grouper(Treatment_coords[cond]\n",
    "                         ) for sample in (tqdm(range(montecarlo)))])\n",
    "\n",
    "    Confidences_per_cond[cond] = np.mean(Predictions_per_cond[cond], axis=0)\n",
    "    Predictions_per_cond[cond] = np.argmax(Confidences_per_cond[cond], axis=1) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Predictions_per_condition = {k:{cl:[] for cl in range(1,31)} for k in Treatment_dict.keys()}\n",
    "\n",
    "for k in Predictions_per_cond.values():\n",
    "    print(Counter(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond in Treatment_dict.keys():\n",
    "    start = 0\n",
    "    for i,j in enumerate(DLCS1_coords.filter(Treatment_dict[cond]).values()):\n",
    "        \n",
    "        update  = start + j.shape[0]//10\n",
    "        counter = Counter(Predictions_per_cond[cond][start:update])\n",
    "        start  += j.shape[0]//10\n",
    "        \n",
    "        for num in counter.keys():\n",
    "            Predictions_per_condition[cond][num+1].append(counter[num+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "clusters = []\n",
    "conditions = []\n",
    "for cond,v in Predictions_per_condition.items():\n",
    "    for cluster,i in v.items():\n",
    "        counts+=i\n",
    "        clusters+=list(np.repeat(cluster, len(i)))\n",
    "        conditions+=list(np.repeat(cond, len(i)))\n",
    "        \n",
    "Prediction_per_cond_df = pd.DataFrame({'condition':conditions,\n",
    "                                       'cluster':clusters,\n",
    "                                       'count':counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame=Prediction_per_cond_df, x='cluster', y='count', color='condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(Counter(labels[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(labels[0], labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ari_dist)\n",
    "plt.xlabel(\"Adjusted Rand Index\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.5,0,0.5,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd.Categorical(np.array([0.5,0.5,0.5,0.5])).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = np.array([0.5,0,0.5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(np.log(pk), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.sum(pk*np.array([-0.69314718,        0, -0.69314718,        0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "entropy = K.sum(tf.multiply(pk, tf.where(~tf.math.is_inf(K.log(pk)), K.log(pk), 0)), axis=0)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.max(clusts, axis=1))\n",
    "sns.distplot(clusts.reshape(clusts.shape[0] * clusts.shape[1]))\n",
    "plt.axvline(1/10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_means = gmvaep.get_layer(name=\"dense_4\").get_weights()[0][:32]\n",
    "gauss_variances = tf.keras.activations.softplus(gmvaep.get_layer(name=\"dense_4\").get_weights()[0][32:]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_means.shape == gauss_variances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "n=100\n",
    "samples = []\n",
    "for i in range(k):\n",
    "    samples.append(np.random.normal(gauss_means[:,i], gauss_variances[:,i], size=(100,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "test_matrix = np.zeros([k,k])\n",
    "for i in range(k):\n",
    "    for j in range(k):\n",
    "        test_matrix[i][j] = np.mean(ttest_ind(samples[i], samples[j], equal_var=False)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.55\n",
    "np.sum(test_matrix > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Treatment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection - the model was trained in the WT - NS mice alone\n",
    "gmvaep.load_weights(\"GMVAE_components=10_loss=ELBO_kl_warmup=20_mmd_warmup=5_20200721-043310_final_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WT_NS = table_dict({k:v for k,v in mtest2.items() if k in Treatment_dict['WT+NS']}, typ=\"coords\")\n",
    "WT_WS = table_dict({k:v for k,v in mtest2.items() if k in Treatment_dict['WT+CSDS']}, typ=\"coords\")\n",
    "MU_NS = table_dict({k:v for k,v in mtest2.items() if k in Treatment_dict['NatCre+NS']}, typ=\"coords\")\n",
    "MU_WS = table_dict({k:v for k,v in mtest2.items() if k in Treatment_dict['NatCre+CSDS']}, typ=\"coords\")\n",
    "\n",
    "preps = [WT_NS.preprocess(window_size=11, window_step=10, filter=\"gaussian\", sigma=55,shift=0, scale=\"standard\", align=True),\n",
    "         WT_WS.preprocess(window_size=11, window_step=10, filter=\"gaussian\", sigma=55,shift=0, scale=\"standard\", align=True),\n",
    "         MU_NS.preprocess(window_size=11, window_step=10, filter=\"gaussian\", sigma=55,shift=0, scale=\"standard\", align=True),\n",
    "         MU_WS.preprocess(window_size=11, window_step=10, filter=\"gaussian\", sigma=55,shift=0, scale=\"standard\", align=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [gmvaep.predict(i) for i in preps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "reconst_error = {k:mean_absolute_error(preps[i].reshape(preps[i].shape[0]*preps[i].shape[1],12).T, \n",
    "                                       preds[i].reshape(preds[i].shape[0]*preds[i].shape[1],12).T,\n",
    "                                       multioutput='raw_values') for i,k in enumerate(Treatment_dict.keys())}\n",
    "\n",
    "reconst_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst_df = pd.concat([pd.DataFrame(np.concatenate([np.repeat(k, len(v)).reshape(len(v),1), v.reshape(len(v),1)],axis=1)) for k,v in reconst_error.items()])\n",
    "reconst_df = reconst_df.astype({0:str,1:float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=reconst_df, x=0, y=1, orient='vertical')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.ylim(0,0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frame rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
