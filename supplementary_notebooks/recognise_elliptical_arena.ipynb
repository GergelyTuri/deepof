{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circular arena recognition using transfer learning for image segmentation\n",
    "\n",
    "In this notebook, we fine-tune a unet-based CNN model on a set of open field video frames, to recognise an ellipse delimiting the arena where the animal/s are. Code is based on https://www.tensorflow.org/tutorials/images/segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 62250 images, 60000 of which are used for training, and 2250 for validation. In addition, data augmentation techniques are employed to improve generalization. We modify lighting conditions, flip and rotate the images on the fly using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook returns a trained model, that deepOF is capable of loading and using for segmenting the given images. More classical computer vision techniques are used a posteriori to fit an ellipse to the round arenas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: lucasmiranda42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for executing in the cloud using papermill\n",
    "vid_path = \"../../../Desktop/deepof_circ_arena_dataset/\"\n",
    "log_path = vid_path\n",
    "out_path = vid_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and settings\n",
    "import tensorflow as tf\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import Iterator, ImageDataGenerator\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get ellipse masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom version of deepof.utils.circular_arena_recognition\n",
    "def circular_arena_recognition(frame: np.array) -> np.array:\n",
    "    \"\"\"Returns x,y position of the center, the lengths of the major and minor axes, \n",
    "    and the angle of the recognised arena\n",
    "\n",
    "    Parameters:\n",
    "        - frame (np.array): numpy.array representing an individual frame of a video\n",
    "\n",
    "    Returns:\n",
    "        - circles (np.array): 3-element-array containing x,y positions of the center\n",
    "        of the arena, and a third value indicating the radius\"\"\"\n",
    "\n",
    "    # Convert image to greyscale, threshold it, blur it and apply open-close operations\n",
    "    kernel = np.ones((5,5))\n",
    "    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray_image, 255 // 4, 255, 0)\n",
    "    for _ in range(5):\n",
    "        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    cnts, _ = cv2.findContours(thresh.astype(np.int64), cv2.RETR_FLOODFILL, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "        \n",
    "    # Find contours in the processed image\n",
    "    main_cnt = np.argmax([len(c) for c in cnts])\n",
    "\n",
    "    # Detect the main ellipse containing the arena\n",
    "    ellipse_params = cv2.fitEllipse(cnts[main_cnt])\n",
    "    center_x, center_y = tuple([int(i) for i in ellipse_params[0]])\n",
    "    axes_L, axes_l = tuple([int(i) // 2 for i in ellipse_params[1]])\n",
    "    ellipse_angle = ellipse_params[2]\n",
    "\n",
    "    return int(center_x), int(center_y), int(axes_L), int(axes_l), ellipse_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipse_labels = np.array(\n",
    "    [circular_arena_recognition(frame) for frame in tqdm(res_frames)]\n",
    ")\n",
    "print(ellipse_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random images with ellipses on top\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    random_idx = np.random.randint(0, res_frames.shape[0])\n",
    "    temp_image = res_frames[random_idx].copy()\n",
    "    temp_ellipse = ellipse_labels[random_idx]\n",
    "\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 255, 0),\n",
    "        3,\n",
    "    )\n",
    "    \n",
    "    ax1.imshow(temp_image)\n",
    "\n",
    "print(\"Dataset examples with their labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('../../../Desktop/deepof-data/deepof_single_topview/Videos/Test 9_s41DLC_resnet50_deepof_single_topviewAug28shuffle1_1030000_labeled.mp4')\n",
    "frameCount = 100\n",
    "frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "buf = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "fc = 0\n",
    "ret = True\n",
    "\n",
    "while (fc < frameCount  and ret):\n",
    "    ret, buf[fc] = cap.read()\n",
    "    fc += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "buf_labels = np.array([np.array(circular_arena_recognition(frame)) for frame in buf])\n",
    "buf_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = buf[0].copy()\n",
    "temp_ellipse = np.median(buf_labels, axis=0)\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    cv2.ellipse(\n",
    "            temp,\n",
    "            tuple(temp_ellipse[:2].astype(int)),\n",
    "            tuple(temp_ellipse[2:4].astype(int)),\n",
    "            temp_ellipse[4],\n",
    "            0,\n",
    "            360,\n",
    "            (0, 255, 0),\n",
    "            3,\n",
    "        )\n",
    "\n",
    "plt.imshow(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    res_frames, ellipse_labels, test_size=2250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Generated a training set with {} images of shape {}\".format(\n",
    "        X_train.shape[0], X_train.shape[1:]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Generated a validation set with {} images of shape {}\".format(\n",
    "        X_val.shape[0], X_val.shape[1:]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Generated {} training labels with {} features\".format(\n",
    "        y_train.shape[0], y_train.shape[1:]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Generated {} validation labels with {} features\".format(\n",
    "        y_val.shape[0], y_val.shape[1:]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data augmentation using ImageDataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the images and generate samples with different brightness on the fly\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1 / 255, brightness_range=(0.5, 1.5),\n",
    ")\n",
    "train_datagen = train_datagen.flow(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for validation set\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1 / 255, brightness_range=(0.5, 1.5),\n",
    ")\n",
    "val_datagen = val_datagen.flow(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images from generator with labels\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    temp = next(val_datagen)\n",
    "    temp_image = temp[0][0]\n",
    "    temp_ellipse = temp[1][0]\n",
    "\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 1, 0),\n",
    "        3,\n",
    "    )\n",
    "    ax1.imshow(temp_image)\n",
    "\n",
    "print(\"ImageDataGenerator examples with their labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves (also available via tensorboard)\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), sharex=True)\n",
    "\n",
    "# ax1.plot(range(5), history.history[\"mae\"], label=\"training\")\n",
    "# ax1.plot(range(5), history.history[\"val_mae\"], label=\"validation\")\n",
    "# ax1.set_xlabel(\"epochs\")\n",
    "# ax1.set_ylabel(\"MAE\")\n",
    "\n",
    "# ax2.plot(range(5), history.history[\"mse\"], label=\"training\")\n",
    "# ax2.plot(range(5), history.history[\"val_mse\"], label=\"validation\")\n",
    "# ax2.set_xlabel(\"epochs\")\n",
    "# ax2.set_ylabel(\"MSE\")\n",
    "\n",
    "# ax1.legend()\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images from generator with predicted labels\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    temp = next(val_datagen)\n",
    "    temp_image = temp[0][0]\n",
    "    temp_ellipse = elliptical_arena_detection.predict(temp_image[np.newaxis, :])[0]\n",
    "\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 1, 0),\n",
    "        3,\n",
    "    )\n",
    "    ax1.imshow(temp_image)\n",
    "\n",
    "print(\"ImageDataGenerator examples with ellipses predicted by the model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reshape, predict and re-reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is trained! However, images had to be forced to a (224,224,3) shape in order to work properly.\n",
    "# The following code grabs an image, reshapes it, gets the ellipse labels,\n",
    "# and transforms them to match the original shape\n",
    "def reshape_predict_reshape(image, model):\n",
    "\n",
    "    input_shape = tuple(model.input.shape[1:-1])\n",
    "    image_temp = cv2.resize(image, input_shape)\n",
    "    image_temp = image_temp / 255\n",
    "\n",
    "    predicted_arena = model.predict(image_temp[np.newaxis, :])[0]\n",
    "\n",
    "    rescaled_x, rescaled_y = predicted_arena[:2] * image.shape[:2][::-1] / input_shape\n",
    "    rescaled_L, rescaled_l = predicted_arena[2:4] * image.shape[:2][::-1] / input_shape\n",
    "    rescaled_angle = predicted_arena[4]\n",
    "    rescaled_ellipse = np.array(\n",
    "        [rescaled_x, rescaled_y, rescaled_L, rescaled_l, rescaled_angle]\n",
    "    )\n",
    "\n",
    "    return rescaled_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(3, 3)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "# # Plot images with labels in their original frame\n",
    "for i in range(9):\n",
    "    \n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    idx = np.random.randint(0, len(frames))\n",
    "    \n",
    "    temp_image = cv2.imread(frames[idx])\n",
    "    temp_ellipse = reshape_predict_reshape(temp_image, elliptical_arena_detection)\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 255, 0),\n",
    "        3,\n",
    "    )\n",
    "    ax1.imshow(temp_image)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
