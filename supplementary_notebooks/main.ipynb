{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import deepof.data\n",
    "import deepof.models\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first batch\n",
    "dset11 = pd.ExcelFile(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_1/DLC_single_CDR1_1/1.Openfield_data-part1/JB05.1-OF-SI-part1.xlsx\"\n",
    ")\n",
    "dset12 = pd.ExcelFile(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_1/DLC_single_CDR1_1/2.Openfielddata-part2/AnimalID's-JB05.1-part2.xlsx\"\n",
    ")\n",
    "dset11 = pd.read_excel(dset11, \"Tabelle2\")\n",
    "dset12 = pd.read_excel(dset12, \"Tabelle2\")\n",
    "\n",
    "dset11.Test = dset11.Test.apply(lambda x: \"Test {}_s11\".format(x))\n",
    "dset12.Test = dset12.Test.apply(lambda x: \"Test {}_s12\".format(x))\n",
    "\n",
    "dset1 = {\n",
    "    \"CSDS\": list(dset11.loc[dset11.Treatment.isin([\"CTR+CSDS\", \"NatCre+CSDS\"]), \"Test\"])\n",
    "    + list(dset12.loc[dset12.Treatment.isin([\"CTR+CSDS\", \"NatCre+CSDS\"]), \"Test\"]),\n",
    "    \"NS\": list(\n",
    "        dset11.loc[\n",
    "            dset11.Treatment.isin([\"CTR+nonstressed\", \"NatCre+nonstressed\"]), \"Test\"\n",
    "        ]\n",
    "    )\n",
    "    + list(\n",
    "        dset12.loc[\n",
    "            dset12.Treatment.isin([\"CTR+nonstressed\", \"NatCre+nonstressed\"]), \"Test\"\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "dset1inv = {}\n",
    "for i in flatten(list(dset1.values())):\n",
    "    if i in dset1[\"CSDS\"]:\n",
    "        dset1inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset1inv[i] = \"NS\"\n",
    "\n",
    "assert len(dset1inv) == dset11.shape[0] + dset12.shape[0], \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second batch\n",
    "dset21 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_2/Part1/2_Single/stressproject22.04.2020genotypes-openfieldday1.xlsx\"\n",
    ")\n",
    "dset22 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_2/Part2/2_Single/OpenFieldvideos-part2.xlsx\"\n",
    ")\n",
    "dset21.Test = dset21.Test.apply(lambda x: \"Test {}_s21\".format(x))\n",
    "dset22.Test = dset22.Test.apply(lambda x: \"Test {}_s22\".format(x))\n",
    "\n",
    "dset2 = {\n",
    "    \"CSDS\": list(dset21.loc[dset21.Treatment == \"Stress\", \"Test\"])\n",
    "    + list(dset22.loc[dset22.Treatment == \"Stressed\", \"Test\"]),\n",
    "    \"NS\": list(dset21.loc[dset21.Treatment == \"Nonstressed\", \"Test\"])\n",
    "    + list(dset22.loc[dset22.Treatment == \"Nonstressed\", \"Test\"]),\n",
    "}\n",
    "\n",
    "dset2inv = {}\n",
    "for i in flatten(list(dset2.values())):\n",
    "    if i in dset2[\"CSDS\"]:\n",
    "        dset2inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset2inv[i] = \"NS\"\n",
    "\n",
    "assert len(dset2inv) == dset21.shape[0] + dset22.shape[0], \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load third batch\n",
    "\n",
    "dset31 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_3/1.Day2OF-SIpart1/JB05 2Female-ELS-OF-SIpart1.xlsx\",\n",
    "    sheet_name=1,\n",
    ")\n",
    "dset32 = pd.read_excel(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_3/2.Day3OF-SIpart2/JB05 2FEMALE-ELS-OF-SIpart2.xlsx\",\n",
    "    sheet_name=1,\n",
    ")\n",
    "dset31.Test = dset31.Test.apply(lambda x: \"Test {}_s31\".format(x))\n",
    "dset32.Test = dset32.Test.apply(lambda x: \"Test {}_s32\".format(x))\n",
    "\n",
    "dset3 = {\"CSDS\": [], \"NS\": list(dset31.loc[:, \"Test\"]) + list(dset32.loc[:, \"Test\"])}\n",
    "\n",
    "dset3inv = {}\n",
    "for i in flatten(list(dset3.values())):\n",
    "    if i in dset3[\"CSDS\"]:\n",
    "        dset3inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset3inv[i] = \"NS\"\n",
    "\n",
    "assert len(dset3inv) == dset31.shape[0] + dset32.shape[0], \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fourth batch\n",
    "dset41 = os.listdir(\n",
    "    \"../../Desktop/deepof-data/tagged_videos/Individual_datasets/DLC_batch_4/JB05.4-OpenFieldvideos/\"\n",
    ")\n",
    "\n",
    "# Remove empty video!\n",
    "dset41 = [vid for vid in dset41 if \"52\" not in vid]\n",
    "\n",
    "dset4 = {\"CSDS\": [], \"NS\": [i[:-4] + \"_s41\" for i in dset41]}\n",
    "\n",
    "dset4inv = {}\n",
    "for i in flatten(list(dset4.values())):\n",
    "    if i in dset4[\"CSDS\"]:\n",
    "        dset4inv[i] = \"CSDS\"\n",
    "    else:\n",
    "        dset4inv[i] = \"NS\"\n",
    "\n",
    "assert len(dset4inv) == len(dset41), \"You missed some labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge phenotype dicts and serialise!\n",
    "aggregated_dset = {**dset1inv, **dset2inv, **dset3inv, **dset4inv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NS': 115, 'CSDS': 52})\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(aggregated_dset.values()))\n",
    "print(115 + 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated dataset to disk\n",
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"../../Desktop/deepof-data/deepof_single_topview/deepof_exp_conditions.pkl\", \"wb\"\n",
    ") as handle:\n",
    "    pickle.dump(aggregated_dset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and run project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 22 ms, total: 302 ms\n",
      "Wall time: 254 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deepof_main = deepof.data.project(\n",
    "    path=os.path.join(\"..\", \"..\", \"Desktop\", \"deepoftesttemp\"),\n",
    "    smooth_alpha=0.99,\n",
    "    arena_dims=[380],\n",
    "    exclude_bodyparts=[\"Tail_1\", \"Tail_2\", \"Tail_tip\", \"Tail_base\"],\n",
    "    exp_conditions=aggregated_dset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trajectories...\n",
      "Smoothing trajectories...\n",
      "Interpolating outliers...\n",
      "Iterative imputation of ocluded bodyparts...\n",
      "Computing distances...\n",
      "Computing angles...\n",
      "Done!\n",
      "Coordinates of 2 videos across 2 conditions\n",
      "CPU times: user 2.8 s, sys: 106 ms, total: 2.91 s\n",
      "Wall time: 938 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deepof_main = deepof_main.run(verbose=True)\n",
    "print(deepof_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quality = pd.concat([tab for tab in deepof_main.get_quality().values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_quality.boxplot(rot=45)\n",
    "plt.ylim(0.99985, 1.00001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919359bb43eb46fab313991cdccea83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='quality_top', max=1.0, step=0.01), Output()), _dom_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(quality_top=(0.0, 1.0, 0.01))\n",
    "def low_quality_tags(quality_top):\n",
    "    pd.DataFrame(\n",
    "        pd.melt(all_quality)\n",
    "        .groupby(\"bodyparts\")\n",
    "        .value.apply(lambda y: sum(y < quality_top) / len(y) * 100)\n",
    "    ).sort_values(by=\"value\", ascending=False).plot.bar(rot=45)\n",
    "\n",
    "    plt.xlabel(\"body part\")\n",
    "    plt.ylabel(\"Tags with quality under {} (%)\".format(quality_top * 100))\n",
    "    plt.tight_layout()\n",
    "    plt.legend([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 670 ms, sys: 3.12 ms, total: 674 ms\n",
      "Wall time: 673 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deepof_coords = deepof_main.get_coords(\n",
    "    center=\"Center\",\n",
    "    polar=False,\n",
    "    speed=0,\n",
    "    align=\"Spine_1\",\n",
    "    align_inplace=True,\n",
    "    propagate_labels=False,\n",
    ")\n",
    "# deepof_dists  = deepof_main.get_distances(propagate_labels=False)\n",
    "# deepof_angles = deepof_main.get_angles(propagate_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "CPU times: user 15.8 ms, sys: 5.57 ms, total: 21.4 ms\n",
      "Wall time: 20.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Preprocessing training set...\")\n",
    "deepof_train = deepof_coords.preprocess(\n",
    "    window_size=24,\n",
    "    window_step=24,\n",
    "    conv_filter=None,\n",
    "    scale=\"standard\",\n",
    "    shuffle=False,\n",
    "    test_videos=0,\n",
    ")[0]\n",
    "\n",
    "# print(\"Loading pre-trained model...\")\n",
    "# encoder, decoder, grouper, gmvaep, = deepof.models.SEQ_2_SEQ_GMVAE(\n",
    "#     loss=\"ELBO\",\n",
    "#     number_of_components=20,\n",
    "#     compile_model=True,\n",
    "#     kl_warmup_epochs=20,\n",
    "#     montecarlo_kl=10,\n",
    "#     encoding=6,\n",
    "#     mmd_warmup_epochs=20,\n",
    "#     predictor=0,\n",
    "#     phenotype_prediction=0,\n",
    "# ).build(deepof_train.shape)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Desktop/GMVAE_loss=ELBO_encoding=6_k=25_latreg=none_20210312-084005_final_weights.h5',\n",
       " '../../Desktop/GMVAE_loss=ELBO_encoding=6_k=25_latreg=variance_20210312-090508_final_weights.h5',\n",
       " '../../Desktop/GMVAE_loss=ELBO_encoding=6_k=25_latreg=categorical+variance_20210312-085926_final_weights.h5',\n",
       " '../../Desktop/GMVAE_loss=ELBO_encoding=6_k=25_latreg=categorical_20210312-093339_final_weights.h5']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [\n",
    "    \"../../Desktop/\" + i\n",
    "    for i in os.listdir(\"../../Desktop/\")\n",
    "    if i.endswith(\"h5\")\n",
    "]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Desktop/GMVAE_loss=ELBO_encoding=6_k=25_latreg=categorical+variance_20210312-085926_final_weights.h5\n"
     ]
    }
   ],
   "source": [
    "trained_network = weights[2]\n",
    "print(trained_network)\n",
    "l = int(re.findall(\"encoding=(\\d+)_\", trained_network)[0])\n",
    "k = int(re.findall(\"k=(\\d+)_\", trained_network)[0])\n",
    "pheno = 0\n",
    "\n",
    "encoder, decoder, grouper, gmvaep, = deepof.models.SEQ_2_SEQ_GMVAE(\n",
    "    loss=\"ELBO\",\n",
    "    number_of_components=k,\n",
    "    compile_model=True,\n",
    "    kl_warmup_epochs=20,\n",
    "    montecarlo_kl=10,\n",
    "    encoding=l,\n",
    "    mmd_warmup_epochs=20,\n",
    "    predictor=0,\n",
    "    phenotype_prediction=pheno,\n",
    "    reg_cat_clusters=(\"categorical\" in trained_network),\n",
    "    reg_cluster_variance=(\"variance\" in trained_network),\n",
    ").build(deepof_train.shape)[:4]\n",
    "\n",
    "gmvaep.load_weights(trained_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data to pass through the models\n",
    "trained_distribution = encoder(deepof_train)\n",
    "categories = tf.keras.models.Model(encoder.input, encoder.layers[15].output)(\n",
    "    deepof_train\n",
    ").numpy()\n",
    "\n",
    "# Fit a scaler to unscale the reconstructions later on\n",
    "video_key = np.random.choice(list(deepof_coords.keys()), 1)[0]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.array(pd.concat(list(deepof_coords.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latent distribution parameters and sample from posterior\n",
    "def get_median_params(component, categories, cluster, param):\n",
    "    # means = [np.median(component.mean().numpy(), axis=0) for component in mix_components]\n",
    "    # stddevs = [np.median(component.stddev().numpy(), axis=0) for component in mix_components]\n",
    "    if param == \"mean\":\n",
    "        component = component.mean().numpy()\n",
    "    elif param == \"stddev\":\n",
    "        component = component.stddev().numpy()\n",
    "\n",
    "    cluster_select = np.argmax(categories, axis=1) == cluster\n",
    "    if np.sum(cluster_select) == 0:\n",
    "        return None\n",
    "    component = component[cluster_select]\n",
    "    return np.median(component, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_latent_parameters(\n",
    "    distribution, reduce=False, plot=False, categories=None, filt=0, save=True\n",
    "):\n",
    "    mix_components = distribution.components\n",
    "\n",
    "    # The main problem is here! We need to select only those training instances in which a given cluster was selected.\n",
    "    # Then compute the median for those only\n",
    "\n",
    "    means = [\n",
    "        get_median_params(component, categories, i, \"mean\")\n",
    "        for i, component in enumerate(mix_components)\n",
    "    ]\n",
    "    stddevs = [\n",
    "        get_median_params(component, categories, i, \"stddev\")\n",
    "        for i, component in enumerate(mix_components)\n",
    "    ]\n",
    "    means = [i for i in means if i is not None]\n",
    "    stddevs = [i for i in stddevs if i is not None]\n",
    "\n",
    "    if filter:\n",
    "        filts = np.max(categories, axis=0) > filt\n",
    "        means = [i for i, j in zip(means, filts) if j]\n",
    "        stddevs = [i for i, j in zip(stddevs, filts) if j]\n",
    "\n",
    "    if reduce:\n",
    "        data = [\n",
    "            np.random.normal(size=[1000, len(means[0])], loc=meanvec, scale=stddevvec)[\n",
    "                :, np.newaxis\n",
    "            ]\n",
    "            for meanvec, stddevvec in zip(means, stddevs)\n",
    "        ]\n",
    "        data = np.concatenate(data, axis=1).reshape([1000 * len(means), len(means[0])])\n",
    "        reducer = PCA(n_components=3)\n",
    "        data = reducer.fit_transform(data)\n",
    "        data = data.reshape([1000, len(means), 3])\n",
    "\n",
    "    if plot == 2:\n",
    "        for i in range(len(means)):\n",
    "            plt.scatter(data[:, i, 0], data[:, i, 1], label=i)\n",
    "        plt.title(\n",
    "            \"Mean representation of latent space - K={}/{} - L={} - filt={}\".format(\n",
    "                len(means), len(mix_components), len(means[0]), filt\n",
    "            )\n",
    "        )\n",
    "        plt.xlabel(\"PCA 1\")\n",
    "        plt.ylabel(\"PCA 2\")\n",
    "        # plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(\n",
    "                \"Mean representation of latent space - K={}.{} - L={} - filt={}.png\".format(\n",
    "                    len(means), len(mix_components), len(means[0]), filt\n",
    "                ).replace(\n",
    "                    \" \", \"_\"\n",
    "                )\n",
    "            )\n",
    "        plt.show()\n",
    "\n",
    "    elif plot == 3:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        for i in range(len(means)):\n",
    "            ax.scatter(data[:, i, 0], data[:, i, 1], data[:, i, 2], label=i)\n",
    "        plt.title(\n",
    "            \"Mean representation of latent space - K={}/{} - L={} - filt={}\".format(\n",
    "                len(means), len(mix_components), len(means[0]), filt\n",
    "            )\n",
    "        )\n",
    "        ax.set_xlabel(\"PCA 1\")\n",
    "        ax.set_ylabel(\"PCA 2\")\n",
    "        ax.set_zlabel(\"PCA 3\")\n",
    "        # plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(\n",
    "                \"Mean representation of latent space - K={}.{} - L={} - filt={}.png\".format(\n",
    "                    len(means), len(mix_components), len(means[0]), filt\n",
    "                ).replace(\n",
    "                    \" \", \"_\"\n",
    "                )\n",
    "            )\n",
    "        plt.show()\n",
    "\n",
    "    elif plot > 3:\n",
    "        raise ValueError(\"Can't plot in more than 3 dimensions!\")\n",
    "\n",
    "    return means, stddevs\n",
    "\n",
    "\n",
    "def sample_from_posterior(\n",
    "    decoder, parameters, component, enable_variance=False, video_output=False, samples=1\n",
    "):\n",
    "    means, stddevs = parameters\n",
    "    sample = np.random.normal(\n",
    "        size=[samples, len(means[component])],\n",
    "        loc=means[component],\n",
    "        scale=(stddevs[component] if enable_variance else 0),\n",
    "    )\n",
    "    reconstruction = decoder(sample).mean()\n",
    "\n",
    "    if video_output:\n",
    "        scaled_video_rec = scaler.inverse_transform(reconstruction)\n",
    "        scaled_video_rec = scaled_video_rec.reshape(\n",
    "            [samples * scaled_video_rec.shape[1], scaled_video_rec.shape[2]]\n",
    "        )\n",
    "        columns = deepof_coords[list(deepof_coords.keys())[0]].columns\n",
    "        scaled_video_rec = pd.DataFrame(scaled_video_rec, columns=columns)\n",
    "\n",
    "        ### VIDEO OUTPUT ###\n",
    "        w = 400\n",
    "        h = 400\n",
    "        factor = 2.5\n",
    "\n",
    "        # Instantiate video\n",
    "        writer = cv2.VideoWriter()\n",
    "        writer.open(\n",
    "            \"Reconstruction_test_L={}_k={}_pheno={}_component={}_video.avi\".format(\n",
    "                l, k, pheno, component\n",
    "            ),\n",
    "            cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "            24,\n",
    "            (int(w * factor), int(h * factor)),\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        for frame in tqdm.tqdm(range(scaled_video_rec.shape[0])):\n",
    "\n",
    "            image = np.zeros((h, w, 3), np.uint8) + 30\n",
    "            for bpart in scaled_video_rec.columns.levels[0]:\n",
    "\n",
    "                try:\n",
    "                    pos = (\n",
    "                        (-int(scaled_video_rec[bpart].loc[frame, \"x\"]) + w // 2),\n",
    "                        (-int(scaled_video_rec[bpart].loc[frame, \"y\"]) + h // 2),\n",
    "                    )\n",
    "\n",
    "                    cv2.circle(image, pos, 2, (0, 0, 255), -1)\n",
    "\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            # draw skeleton\n",
    "            def draw_line(start, end, df, col):\n",
    "                for bpart in end:\n",
    "                    cv2.line(\n",
    "                        image,\n",
    "                        tuple(-df[start].loc[frame, :].asisinstance(int) + w // 2),\n",
    "                        tuple(-df[bpart].loc[frame, :].asisinstance(int) + h // 2),\n",
    "                        col,\n",
    "                        1,\n",
    "                    )\n",
    "\n",
    "            col = (0, 0, 255)\n",
    "            draw_line(\"Nose\", [\"Left_ear\", \"Right_ear\"], scaled_video_rec, col)\n",
    "            draw_line(\n",
    "                \"Spine_1\",\n",
    "                [\"Left_ear\", \"Right_ear\", \"Left_fhip\", \"Right_fhip\"],\n",
    "                scaled_video_rec,\n",
    "                col,\n",
    "            )\n",
    "            draw_line(\n",
    "                \"Spine_2\", [\"Spine_1\", \"Left_bhip\", \"Right_bhip\"], scaled_video_rec, col\n",
    "            )\n",
    "            # draw_line(\"Tail_1\", [\"Tail_base\", \"Tail_2\"], scaled_video_rec, col)\n",
    "            # draw_line(\"Tail_tip\", [\"Tail_2\"], scaled_video_rec, col)\n",
    "\n",
    "            image = cv2.resize(image, (0, 0), fx=factor, fy=factor)\n",
    "            writer.write(image)\n",
    "\n",
    "        writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f13a38354619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfilt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# for i in range(0, 25):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ab39d376a10e>\u001b[0m in \u001b[0;36mretrieve_latent_parameters\u001b[0;34m(distribution, reduce, plot, categories, filt, save)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmeanvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddevvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddevs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         ]\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mreducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "means, stddevs = retrieve_latent_parameters(\n",
    "    trained_distribution,\n",
    "    categories=categories,\n",
    "    reduce=True,\n",
    "    plot=2,\n",
    "    filt=0.9,\n",
    "    save=True,\n",
    ")\n",
    "# for i in range(0, 25):\n",
    "#    reconst = sample_from_posterior(decoder, (means, stddevs), i, enable_variance=True, video_output=True, samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./rule_based_labels/Test 1_s12_rulebased_annot.tsv',\n",
       " './rule_based_labels/Test 1_s11_rulebased_annot.tsv']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load rule based labels for the videos at play\n",
    "tag_path = \"./rule_based_labels/\"\n",
    "rule_based_tags = [\n",
    "    tag_path + i\n",
    "    for i in os.listdir(tag_path)\n",
    "    for j in list(deepof_main._tables.keys())\n",
    "    if j in i\n",
    "]\n",
    "rule_based_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30002,)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {k: pd.read_csv(k, sep=\"\\t\") for k in rule_based_tags}\n",
    "concat_tags = np.concatenate(list(tags.values()))\n",
    "concat_tags = concat_tags[:, 3]\n",
    "concat_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving = (\n",
    "    pd.Series(concat_tags)\n",
    "    .rolling(window=24,)\n",
    "    .apply(lambda x: np.any(x > 2))[::24][1:]\n",
    "    .asisinstance(bool)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24        True\n",
       "48        True\n",
       "72        True\n",
       "96        True\n",
       "120       True\n",
       "         ...  \n",
       "29904    False\n",
       "29928     True\n",
       "29952     True\n",
       "29976     True\n",
       "30000     True\n",
       "Length: 1250, dtype: bool"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass training set through the grouper to obtain cluster assignments\n",
    "clusters = grouper.predict(deepof_train)\n",
    "argmax_clusters = np.argmax(clusters, axis=1)\n",
    "confid_clusters = np.max(clusters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 92 0.8587\n",
      "8 65 0.87692\n",
      "9 49 0.89796\n",
      "10 60 0.75\n",
      "14 542 0.83948\n",
      "17 169 0.86391\n",
      "21 132 0.86364\n"
     ]
    }
   ],
   "source": [
    "for i in range(max(argmax_clusters)):\n",
    "    if i in argmax_clusters[confid_clusters > 0.9]:\n",
    "        print(\n",
    "            i,\n",
    "            np.sum(argmax_clusters == i, axis=0),\n",
    "            np.round(\n",
    "                sum(moving[argmax_clusters == i])\n",
    "                / np.sum(argmax_clusters == i, axis=0),\n",
    "                5,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1_s12\n",
      "(14977, 24, 18)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965d46256e7b49d5b4d0e482107c6440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827ae8871ff4403a8368730a1db79dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95232e3eadd9480099610dd9653963d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f45bebdb734dad922130712d18ca9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f54a1c114a94bb48a99052a5dc3f8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_key = np.random.choice(list(deepof_coords.keys()), 1)[0]\n",
    "print(video_key)\n",
    "video_input = deepof.data.table_dict({video_key:deepof_coords[video_key]}, typ=\"coords\").preprocess(\n",
    "                                                                                                window_size=24,\n",
    "                                                                                                window_step=1,\n",
    "                                                                                                conv_filter=None,\n",
    "                                                                                                scale=\"standard\",\n",
    "                                                                                                shuffle=False,\n",
    "                                                                                                test_videos=0,\n",
    "                                                                                            )[0]\n",
    "\n",
    "print(video_input.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.array(pd.concat(list(deepof_coords.values()))))\n",
    "\n",
    "for trained_network in tqdm.tqdm(weights):\n",
    "\n",
    "    l = int(re.findall(\"encoding=(\\d+)_\", trained_network)[0])\n",
    "    k = int(re.findall(\"k=(\\d+)_\", trained_network)[0])\n",
    "    #pheno = float(re.findall(\"pheno=(.+?)_\", trained_network)[0])\n",
    "\n",
    "    encoder, decoder, grouper, gmvaep, = deepof.models.SEQ_2_SEQ_GMVAE(\n",
    "        loss=\"ELBO\",\n",
    "        number_of_components=k,\n",
    "        compile_model=True,\n",
    "        kl_warmup_epochs=20,\n",
    "        montecarlo_kl=10,\n",
    "        encoding=l,\n",
    "        mmd_warmup_epochs=20,\n",
    "        predictor=0,\n",
    "        phenotype_prediction=0,\n",
    "    ).build(video_input.shape)[:4]\n",
    "\n",
    "    gmvaep.load_weights(trained_network)\n",
    "\n",
    "    # Get reconstruction\n",
    "    video_pred = gmvaep.predict(video_input)[:, 6, :]\n",
    "\n",
    "    # Get encodings\n",
    "    # video_clusters = grouper.predict(video_input)\n",
    "    # video_encodings = encoder.predict(video_input)\n",
    "\n",
    "    scaled_video_pred = scaler.inverse_transform(video_pred)\n",
    "    scaled_video_input = scaler.inverse_transform(video_input[:, 6, :])\n",
    "\n",
    "    scaled_video_input = pd.DataFrame(scaled_video_input, columns=deepof_coords[video_key].columns)\n",
    "    scaled_video_pred = pd.DataFrame(scaled_video_pred, columns=deepof_coords[video_key].columns)\n",
    "\n",
    "    ### VIDEO OUTPUT ###\n",
    "    w = 400\n",
    "    h = 400\n",
    "    factor = 2.5\n",
    "\n",
    "    # Instantiate video\n",
    "    writer = cv2.VideoWriter()\n",
    "    writer.open(\n",
    "        \"L={}_k={}_pheno={}_run0_video.avi\".format(l,k,pheno),\n",
    "        cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "        24,\n",
    "        (int(w * factor), int(h * factor)),\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    for frame in tqdm.tqdm(range(250)):\n",
    "\n",
    "        image = np.zeros((h, w, 3), np.uint8) + 30\n",
    "        for bpart in scaled_video_input.columns.levels[0]:\n",
    "\n",
    "            try:\n",
    "                pos = (\n",
    "                    (-int(scaled_video_input[bpart].loc[frame, \"x\"]) + w // 2),\n",
    "                    (-int(scaled_video_input[bpart].loc[frame, \"y\"]) + h // 2),\n",
    "                )\n",
    "\n",
    "                pos_pred = (\n",
    "                    (-int(scaled_video_pred[bpart].loc[frame, \"x\"]) + w // 2),\n",
    "                    (-int(scaled_video_pred[bpart].loc[frame, \"y\"]) + h // 2),\n",
    "                )\n",
    "\n",
    "                cv2.circle(image, pos, 2, (0, 0, 255), -1)\n",
    "                cv2.circle(image, pos_pred, 2, (0, 255, 0), -1)\n",
    "\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        # draw skeleton\n",
    "        def draw_line(start, end, df, col):\n",
    "            for bpart in end:\n",
    "                cv2.line(\n",
    "                    image,\n",
    "                    tuple(-df[start].loc[frame, :].astype(int) + w // 2),\n",
    "                    tuple(-df[bpart].loc[frame, :].astype(int) + h // 2),\n",
    "                    col,\n",
    "                    1,\n",
    "                )\n",
    "\n",
    "        \n",
    "        for df, col in zip([scaled_video_input, scaled_video_pred], [(0,0,255),(0,255,0)]):\n",
    "            draw_line(\"Nose\", [\"Left_ear\", \"Right_ear\"], df, col)\n",
    "            draw_line(\"Spine_1\", [\"Left_ear\", \"Right_ear\", \"Left_fhip\", \"Right_fhip\"], df, col)\n",
    "            draw_line(\"Spine_2\", [\"Spine_1\", \"Left_bhip\", \"Right_bhip\"], df, col)\n",
    "            \n",
    "        \n",
    "#         for df, col in zip([scaled_video_input, scaled_video_pred], [(0,0,255),(0,255,0)]):\n",
    "#             draw_line(\"Nose\", [\"Left_ear\", \"Right_ear\"], df, col)\n",
    "#             draw_line(\"Spine_1\", [\"Left_ear\", \"Right_ear\", \"Left_fhip\", \"Right_fhip\"], df, col)\n",
    "#             draw_line(\"Spine_2\", [\"Spine_1\", \"Tail_base\", \"Left_bhip\", \"Right_bhip\"], df, col)\n",
    "#             draw_line(\"Tail_1\", [\"Tail_base\", \"Tail_2\"], df, col)\n",
    "#             draw_line(\"Tail_tip\", [\"Tail_2\"], df, col)\n",
    "\n",
    "        image = cv2.resize(image, (0, 0), fx=factor, fy=factor)\n",
    "        writer.write(image)\n",
    "\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_corrs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Plot latent space!\n",
    "X_train = deepof_coords.preprocess(\n",
    "    window_size=11,\n",
    "    window_step=1,\n",
    "    conv_filter=None,\n",
    "    scale=\"standard\",\n",
    "    shuffle=True,\n",
    "    test_videos=0,\n",
    ")[0]\n",
    "\n",
    "samples = 10000\n",
    "X_train = X_train[:samples]\n",
    "\n",
    "for trained_network in tqdm.tqdm(weights):\n",
    "    print(trained_network)\n",
    "\n",
    "    l = int(re.findall(\"encoding=(\\d+)_\", trained_network)[0])\n",
    "    k = int(re.findall(\"k=(\\d+)_\", trained_network)[0])\n",
    "    pheno = float(re.findall(\"pheno=(.+?)_\", trained_network)[0])\n",
    "\n",
    "    encoder, decoder, grouper, gmvaep, = deepof.models.SEQ_2_SEQ_GMVAE(\n",
    "        loss=\"ELBO\",\n",
    "        number_of_components=k,\n",
    "        compile_model=True,\n",
    "        kl_warmup_epochs=20,\n",
    "        montecarlo_kl=10,\n",
    "        encoding=l,\n",
    "        mmd_warmup_epochs=20,\n",
    "        predictor=0,\n",
    "        phenotype_prediction=pheno,\n",
    "    ).build(X_train.shape)[:4]\n",
    "\n",
    "    gmvaep.load_weights(trained_network)\n",
    "\n",
    "    # Get encodings\n",
    "    pheno_pred = gmvaep.predict(X_train)[1]\n",
    "    clusters = grouper.predict(X_train)\n",
    "    encodings = encoder.predict(X_train)\n",
    "\n",
    "    #     # For each cluster, compute correlation between pheno prediction and cluster weight\n",
    "    #     pheno_corr = []\n",
    "    #     for i in range(k):\n",
    "    #         pheno_corr.append(np.corrcoef(clusters[:,i], np.squeeze(pheno_pred))[0,1])\n",
    "    #     pheno_corrs[\"L={}_k={}_pheno={}_run0\".format(l,k, pheno)] = pheno_corr\n",
    "\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    encodings = reducer.fit_transform(encodings)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        encodings[:, 0],\n",
    "        encodings[:, 1],\n",
    "        hue=np.squeeze(\n",
    "            pheno_pred\n",
    "        ),  # np.argmax(clusters, axis=1).asisinstance(int).asisinstance(str),\n",
    "        # palette=(\"jet\" if k>1 else None), legend=\"none\")\n",
    "    )\n",
    "    plt.title(\"GMVAE Latent space representation: L={}; k={}\".format(l, k))\n",
    "    plt.xlabel(\"UMAP 1\")\n",
    "    plt.ylabel(\"UMAP 2\")\n",
    "    plt.legend([], [], frameon=False)\n",
    "    plt.savefig(\"L={}_k={}_pheno={}_run0_latent_space_phenohue.pdf\".format(l, k, pheno))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pheno_pred.shape)\n",
    "print(clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation density plots\n",
    "pweights = [0.01, 0.1, 0.5, 0.25, 1, 2, 4, 10, 100]\n",
    "for i in pweights:\n",
    "    corrs = {k: v for k, v in pheno_corrs.items() if str(i) in k}\n",
    "    sns.kdeplot(np.concatenate([i for i in corrs.values()]), label=str(i))\n",
    "    plt.xlabel(\"pearson correlation coefficient\")\n",
    "    plt.ylabel(\"density\")\n",
    "\n",
    "plt.savefig(\"deepof_pheno_fullcorrhistogram.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation density plots\n",
    "pweights = [0.01, 0.1, 0.5, 0.25, 1, 2, 4, 10, 100]\n",
    "for i in pweights:\n",
    "    corrs = {k: v for k, v in pheno_corrs.items() if str(i) in k}\n",
    "    sns.kdeplot(np.concatenate([i for i in corrs.values()]), label=str(i))\n",
    "    plt.xlabel(\"pearson correlation coefficient\")\n",
    "    plt.ylabel(\"density\")\n",
    "\n",
    "plt.savefig(\"deepof_pheno_parccorrhistogram.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    gmvaep,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True,\n",
    "    dpi=70,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encodings(data, samples, n, clusters, threshold, highlight=None):\n",
    "\n",
    "    reducer = LinearDiscriminantAnalysis(n_components=n)\n",
    "    clusters = clusters[:samples, :]\n",
    "\n",
    "    # filter   = np.max(np.mean(clusters, axis=0), axis=1) > threshold\n",
    "\n",
    "    clusters = np.argmax(clusters, axis=1)  # [filter]\n",
    "    rep = reducer.fit_transform(data[:samples], clusters)\n",
    "\n",
    "    if n == 2:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"encoding-1\": rep[:, 0],\n",
    "                \"encoding-2\": rep[:, 1],\n",
    "                \"clusters\": [\"A\" + str(i) for i in clusters],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        enc = px.scatter(\n",
    "            data_frame=df,\n",
    "            x=\"encoding-1\",\n",
    "            y=\"encoding-2\",\n",
    "            color=\"clusters\",\n",
    "            width=600,\n",
    "            height=600,\n",
    "            color_discrete_sequence=px.colors.qualitative.T10,\n",
    "        )\n",
    "\n",
    "        # if highlight:\n",
    "        #    ig.add_trace(go.Scatter(x=, y=)\n",
    "\n",
    "    elif n == 3:\n",
    "        df3d = pd.DataFrame(\n",
    "            {\n",
    "                \"encoding-1\": rep[:, 0],\n",
    "                \"encoding-2\": rep[:, 1],\n",
    "                \"encoding-3\": rep[:, 2],\n",
    "                \"clusters\": [\"A\" + str(i) for i in clusters],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        enc = px.scatter_3d(\n",
    "            data_frame=df3d,\n",
    "            x=\"encoding-1\",\n",
    "            y=\"encoding-2\",\n",
    "            z=\"encoding-3\",\n",
    "            color=\"clusters\",\n",
    "            width=600,\n",
    "            height=600,\n",
    "            color_discrete_sequence=px.colors.qualitative.T10,\n",
    "        )\n",
    "\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_encodings(encoder.predict(deepof_train[:10000]), 1000, 2, categories, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, y_train, X_test, y_test = deepof_coords.preprocess(\n",
    "    window_size=11,\n",
    "    window_step=11,\n",
    "    conv_filter=None,\n",
    "    sigma=55,\n",
    "    shift=0,\n",
    "    scale=\"standard\",\n",
    "    align=\"all\",\n",
    "    shuffle=True,\n",
    "    test_videos=5,\n",
    ")\n",
    "print(\"Train dataset shape: \", X_train.shape)\n",
    "print(\"Train dataset shape: \", y_train.shape)\n",
    "print(\"Test dataset shape: \", X_test.shape)\n",
    "print(\"Test dataset shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models and get learning rate (1-cycle policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq 2 seq Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow.keras as k\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Baseline_AE\"\n",
    "log_dir = os.path.abspath(\n",
    "    \"logs/fit/{}_{}\".format(NAME, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    ")\n",
    "tensorboard_callback = k.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepof.models import SEQ_2_SEQ_AE, SEQ_2_SEQ_GMVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, ae = SEQ_2_SEQ_AE().build(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "(\n",
    "    encoder,\n",
    "    generator,\n",
    "    grouper,\n",
    "    gmvaep,\n",
    "    kl_warmup_callback,\n",
    "    mmd_warmup_callback,\n",
    ") = SEQ_2_SEQ_GMVAE(\n",
    "    loss=\"ELBO\",\n",
    "    compile_model=True,\n",
    "    number_of_components=10,\n",
    "    kl_warmup_epochs=20,\n",
    "    mmd_warmup_epochs=0,\n",
    "    predictor=0,\n",
    "    phenotype_prediction=0,\n",
    "    architecture_hparams={\"encoding\": 2},\n",
    ").build(\n",
    "    X_train.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "rates, losses = deepof.model_utils.find_learning_rate(\n",
    "    gmvaep,\n",
    "    deepof_train[: 512 * 10],\n",
    "    deepof_test[: 512 * 10],\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "deepof.model_utils.plot_lr_vs_loss(rates, losses)\n",
    "plt.title(\"Learning rate tuning\")\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gmvaep.fit(\n",
    "    x=X_train,\n",
    "    y=X_train,\n",
    "    epochs=1,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, [X_test, y_test]),\n",
    "    callbacks=[kl_warmup_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pttest\n",
    "samples = 15000\n",
    "montecarlo = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \"GMVAE_components=30_loss=ELBO_kl_warmup=30_mmd_warmup=30_20200804-225526_final_weights.h5\"\n",
    "\n",
    "gmvaep.load_weights(weights)\n",
    "\n",
    "if montecarlo:\n",
    "    clusts = np.stack([grouper(data[:samples]) for sample in (tqdm(range(montecarlo)))])\n",
    "    clusters = clusts.mean(axis=0)\n",
    "    clusters = np.argmax(clusters, axis=1)\n",
    "\n",
    "else:\n",
    "    clusters = grouper(data[:samples], training=False)\n",
    "\n",
    "    clusters = np.argmax(clusters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encodings(data, samples, n, clusters, threshold):\n",
    "\n",
    "    reducer = PCA(n_components=n)\n",
    "    clusters = clusters[:, :samples]\n",
    "    filter = np.max(np.mean(clusters, axis=0), axis=1) > threshold\n",
    "    encoder.predict(data[:samples][filter])\n",
    "    print(\n",
    "        \"{}/{} samples used ({}%); confidence threshold={}\".format(\n",
    "            sum(filter), samples, sum(filter) / samples * 100, threshold\n",
    "        )\n",
    "    )\n",
    "\n",
    "    clusters = np.argmax(np.mean(clusters, axis=0), axis=1)[filter]\n",
    "    rep = reducer.fit_transform(encoder.predict(data[:samples][filter]))\n",
    "\n",
    "    if n == 2:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"encoding-1\": rep[:, 0],\n",
    "                \"encoding-2\": rep[:, 1],\n",
    "                \"clusters\": [\"A\" + str(i) for i in clusters],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        enc = px.scatter(\n",
    "            data_frame=df,\n",
    "            x=\"encoding-1\",\n",
    "            y=\"encoding-2\",\n",
    "            color=\"clusters\",\n",
    "            width=600,\n",
    "            height=600,\n",
    "            color_discrete_sequence=px.colors.qualitative.T10,\n",
    "        )\n",
    "\n",
    "    elif n == 3:\n",
    "        df3d = pd.DataFrame(\n",
    "            {\n",
    "                \"encoding-1\": rep[:, 0],\n",
    "                \"encoding-2\": rep[:, 1],\n",
    "                \"encoding-3\": rep[:, 2],\n",
    "                \"clusters\": [\"A\" + str(i) for i in clusters],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        enc = px.scatter_3d(\n",
    "            data_frame=df3d,\n",
    "            x=\"encoding-1\",\n",
    "            y=\"encoding-2\",\n",
    "            z=\"encoding-3\",\n",
    "            color=\"clusters\",\n",
    "            width=600,\n",
    "            height=600,\n",
    "            color_discrete_sequence=px.colors.qualitative.T10,\n",
    "        )\n",
    "\n",
    "    return enc\n",
    "\n",
    "\n",
    "plot_encodings(data, 5000, 2, clusts, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution per cluster\n",
    "for cl in range(5):\n",
    "    cl_select = np.argmax(np.mean(clusts, axis=0), axis=1) == cl\n",
    "    dt = np.mean(clusts[:, cl_select, cl], axis=0)\n",
    "    sns.kdeplot(dt, shade=True, label=cl)\n",
    "\n",
    "plt.xlabel(\"MC Dropout confidence\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animated_cluster_heatmap(data, clust, clusters, threshold=0.75, samples=False):\n",
    "\n",
    "    if not samples:\n",
    "        samples = data.shape[0]\n",
    "    tpoints = data.shape[1]\n",
    "    bdparts = data.shape[2] // 2\n",
    "\n",
    "    cls = clusters[:, :samples, :]\n",
    "    filt = np.max(np.mean(cls, axis=0), axis=1) > threshold\n",
    "\n",
    "    cls = np.argmax(np.mean(cls, axis=0), axis=1)[filt]\n",
    "    clust_series = data[:samples][filt][cls == clust]\n",
    "\n",
    "    rshape = clust_series.reshape(\n",
    "        clust_series.shape[0] * clust_series.shape[1], clust_series.shape[2]\n",
    "    )\n",
    "\n",
    "    cluster_df = pd.DataFrame()\n",
    "    cluster_df[\"x\"] = rshape[:, [0, 2, 4, 6, 8, 10]].flatten(order=\"F\")\n",
    "    cluster_df[\"y\"] = rshape[:, [1, 3, 5, 7, 9, 11]].flatten(order=\"F\")\n",
    "    cluster_df[\"bpart\"] = np.tile(\n",
    "        np.repeat(np.arange(bdparts), clust_series.shape[0]), tpoints\n",
    "    )\n",
    "    cluster_df[\"frame\"] = np.tile(\n",
    "        np.repeat(np.arange(tpoints), clust_series.shape[0]), bdparts\n",
    "    )\n",
    "\n",
    "    fig = px.density_contour(\n",
    "        data_frame=cluster_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        animation_frame=\"frame\",\n",
    "        width=600,\n",
    "        height=600,\n",
    "        color=\"bpart\",\n",
    "        color_discrete_sequence=px.colors.qualitative.T10,\n",
    "    )\n",
    "\n",
    "    fig.update_traces(contours_coloring=\"fill\", contours_showlabels=True)\n",
    "\n",
    "    fig.update_xaxes(range=[-3, 3])\n",
    "    fig.update_yaxes(range=[-3, 3])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animated_cluster_heatmap(pttest, 4, clusts, samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [i for i in os.listdir() if \"GMVAE\" in i and \".h5\" in i]\n",
    "mult_clusters = np.zeros([len(weights), samples])\n",
    "mean_conf = []\n",
    "\n",
    "for k, i in tqdm(enumerate(sorted(weights))):\n",
    "    print(i)\n",
    "    gmvaep.load_weights(i)\n",
    "\n",
    "    if montecarlo:\n",
    "        clusters = np.stack(\n",
    "            [grouper(data[:samples]) for sample in (tqdm(range(montecarlo)))]\n",
    "        )\n",
    "        clusters = clusters.mean(axis=0)\n",
    "        mean_conf.append(clusters.max(axis=1))\n",
    "        clusters = np.argmax(clusters, axis=1)\n",
    "\n",
    "    else:\n",
    "        clusters = grouper(data[:samples], training=False)\n",
    "        mean_conf.append(clusters.max(axis=1))\n",
    "        clusters = np.argmax(clusters, axis=1)\n",
    "\n",
    "    mult_clusters[k] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.95\n",
    "ari_dist = []\n",
    "\n",
    "for i, k in enumerate(combinations(range(len(weights)), 2)):\n",
    "    filt = (mean_conf[k[0]] > thr) & (mean_conf[k[1]] > thr)\n",
    "\n",
    "    ari = adjusted_rand_score(mult_clusters[k[0]][filt], mult_clusters[k[1]][filt])\n",
    "\n",
    "    ari_dist.append(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ari = []\n",
    "for i in tqdm(range(6)):\n",
    "    random_ari.append(\n",
    "        adjusted_rand_score(\n",
    "            np.random.uniform(0, 6, 50).asisinstance(int),\n",
    "            np.random.uniform(0, 6, 50).asisinstance(int),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ari_dist, label=\"ARI gmvaep\", shade=True)\n",
    "sns.kdeplot(random_ari, label=\"ARI random\", shade=True)\n",
    "\n",
    "plt.xlabel(\"Normalised Adjusted Rand Index\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster differences across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "DLCS1_coords = DLC_social_1_coords.get_coords(\n",
    "    center=\"B_Center\", polar=False, length=\"00:10:00\", align=\"B_Nose\"\n",
    ")\n",
    "\n",
    "Treatment_coords = {}\n",
    "\n",
    "for cond in Treatment_dict.keys():\n",
    "    Treatment_coords[cond] = DLCS1_coords.filter(Treatment_dict[cond]).preprocess(\n",
    "        window_size=13, window_step=10, filter=None, scale=\"standard\", align=\"center\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "montecarlo = 10\n",
    "\n",
    "Predictions_per_cond = {}\n",
    "Confidences_per_cond = {}\n",
    "\n",
    "for cond in Treatment_dict.keys():\n",
    "\n",
    "    Predictions_per_cond[cond] = np.stack(\n",
    "        [grouper(Treatment_coords[cond]) for sample in (tqdm(range(montecarlo)))]\n",
    "    )\n",
    "\n",
    "    Confidences_per_cond[cond] = np.mean(Predictions_per_cond[cond], axis=0)\n",
    "    Predictions_per_cond[cond] = np.argmax(Confidences_per_cond[cond], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Predictions_per_condition = {\n",
    "    k: {cl: [] for cl in range(1, 31)} for k in Treatment_dict.keys()\n",
    "}\n",
    "\n",
    "for k in Predictions_per_cond.values():\n",
    "    print(Counter(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond in Treatment_dict.keys():\n",
    "    start = 0\n",
    "    for i, j in enumerate(DLCS1_coords.filter(Treatment_dict[cond]).values()):\n",
    "\n",
    "        update = start + j.shape[0] // 10\n",
    "        counter = Counter(Predictions_per_cond[cond][start:update])\n",
    "        start += j.shape[0] // 10\n",
    "\n",
    "        for num in counter.keys():\n",
    "            Predictions_per_condition[cond][num + 1].append(counter[num + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "clusters = []\n",
    "conditions = []\n",
    "for cond, v in Predictions_per_condition.items():\n",
    "    for cluster, i in v.items():\n",
    "        counts += i\n",
    "        clusters += list(np.repeat(cluster, len(i)))\n",
    "        conditions += list(np.repeat(cond, len(i)))\n",
    "\n",
    "Prediction_per_cond_df = pd.DataFrame(\n",
    "    {\"condition\": conditions, \"cluster\": clusters, \"count\": counts}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame=Prediction_per_cond_df, x=\"cluster\", y=\"count\", color=\"condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(Counter(labels[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(labels[0], labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ari_dist)\n",
    "plt.xlabel(\"Adjusted Rand Index\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.5, 0, 0.5, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd.Categorical(np.array([0.5, 0.5, 0.5, 0.5])).entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = np.array([0.5, 0, 0.5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(np.log(pk), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.sum(pk * np.array([-0.69314718, 0, -0.69314718, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "entropy = K.sum(\n",
    "    tf.multiply(pk, tf.where(~tf.math.is_inf(K.log(pk)), K.log(pk), 0)), axis=0\n",
    ")\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.max(clusts, axis=1))\n",
    "sns.distplot(clusts.reshape(clusts.shape[0] * clusts.shape[1]))\n",
    "plt.axvline(1 / 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_means = gmvaep.get_layer(name=\"dense_4\").get_weights()[0][:32]\n",
    "gauss_variances = tf.keras.activations.softplus(\n",
    "    gmvaep.get_layer(name=\"dense_4\").get_weights()[0][32:]\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_means.shape == gauss_variances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "samples = []\n",
    "for i in range(k):\n",
    "    samples.append(\n",
    "        np.random.normal(gauss_means[:, i], gauss_variances[:, i], size=(100, 32))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "test_matrix = np.zeros([k, k])\n",
    "for i in range(k):\n",
    "    for j in range(k):\n",
    "        test_matrix[i][j] = np.mean(\n",
    "            ttest_ind(samples[i], samples[j], equal_var=False)[1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.55\n",
    "np.sum(test_matrix > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Treatment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection - the model was trained in the WT - NS mice alone\n",
    "gmvaep.load_weights(\n",
    "    \"GMVAE_components=10_loss=ELBO_kl_warmup=20_mmd_warmup=5_20200721-043310_final_weights.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WT_NS = table_dict(\n",
    "    {k: v for k, v in mtest2.items() if k in Treatment_dict[\"WT+NS\"]}, typ=\"coords\"\n",
    ")\n",
    "WT_WS = table_dict(\n",
    "    {k: v for k, v in mtest2.items() if k in Treatment_dict[\"WT+CSDS\"]}, typ=\"coords\"\n",
    ")\n",
    "MU_NS = table_dict(\n",
    "    {k: v for k, v in mtest2.items() if k in Treatment_dict[\"NatCre+NS\"]}, typ=\"coords\"\n",
    ")\n",
    "MU_WS = table_dict(\n",
    "    {k: v for k, v in mtest2.items() if k in Treatment_dict[\"NatCre+CSDS\"]},\n",
    "    typ=\"coords\",\n",
    ")\n",
    "\n",
    "preps = [\n",
    "    WT_NS.preprocess(\n",
    "        window_size=11,\n",
    "        window_step=10,\n",
    "        filter=\"gaussian\",\n",
    "        sigma=55,\n",
    "        shift=0,\n",
    "        scale=\"standard\",\n",
    "        align=True,\n",
    "    ),\n",
    "    WT_WS.preprocess(\n",
    "        window_size=11,\n",
    "        window_step=10,\n",
    "        filter=\"gaussian\",\n",
    "        sigma=55,\n",
    "        shift=0,\n",
    "        scale=\"standard\",\n",
    "        align=True,\n",
    "    ),\n",
    "    MU_NS.preprocess(\n",
    "        window_size=11,\n",
    "        window_step=10,\n",
    "        filter=\"gaussian\",\n",
    "        sigma=55,\n",
    "        shift=0,\n",
    "        scale=\"standard\",\n",
    "        align=True,\n",
    "    ),\n",
    "    MU_WS.preprocess(\n",
    "        window_size=11,\n",
    "        window_step=10,\n",
    "        filter=\"gaussian\",\n",
    "        sigma=55,\n",
    "        shift=0,\n",
    "        scale=\"standard\",\n",
    "        align=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [gmvaep.predict(i) for i in preps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "reconst_error = {\n",
    "    k: mean_absolute_error(\n",
    "        preps[i].reshape(preps[i].shape[0] * preps[i].shape[1], 12).T,\n",
    "        preds[i].reshape(preds[i].shape[0] * preds[i].shape[1], 12).T,\n",
    "        multioutput=\"raw_values\",\n",
    "    )\n",
    "    for i, k in enumerate(Treatment_dict.keys())\n",
    "}\n",
    "\n",
    "reconst_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            np.concatenate(\n",
    "                [np.repeat(k, len(v)).reshape(len(v), 1), v.reshape(len(v), 1)], axis=1\n",
    "            )\n",
    "        )\n",
    "        for k, v in reconst_error.items()\n",
    "    ]\n",
    ")\n",
    "reconst_df = reconst_df.asisinstance({0: str, 1: float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=reconst_df, x=0, y=1, orient=\"vertical\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.ylim(0, 0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frame rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
