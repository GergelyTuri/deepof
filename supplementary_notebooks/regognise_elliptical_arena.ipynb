{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circular arena recognition using transfer learning\n",
    "\n",
    "In this notebook, we train a resnet50-based CNN model on a set of open field video frames, to recognise an ellipse delimiting the arena where the animal/s are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 60000 images, 55000 of which are used for training, and 5000 for validation. In addition, data augmentation techniques are employed to improve generalization. We modify lighting conditions, flip and rotate the images on the fly using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook returns a trained model, that deepOF is capable of loading and using for predicting an ellipse in terms of five parameters: the x, y coordinates of the centers, the d and D length values of the major and minor axes, and an alpha value indicating the rotation in radians of the major axes with respect to the x axis of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: lucasmiranda42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for executing in the cloud using papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and settings\n",
    "import tensorflow as tf\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import Iterator, ImageDataGenerator\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = os.path.join(\"..\", \"..\", \"..\", \"Desktop\", \"deepof_circ_arena_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [os.path.join(vid_path, i) for i in os.listdir(vid_path) if i.endswith(\".jpg\")]\n",
    "print(\"Extracted {} images from deepof videos\".format(len(frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_frames = np.array([cv2.resize(cv2.imread(i), (224,224)) for i in tqdm(frames)])\n",
    "# print(res_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save np.ndarray to avoid reruning above cells\n",
    "#np.save(os.path.join(vid_path, \"circ_arena_dataset.npy\"), res_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment cells above and load data in numpy format to avoid recomputing\n",
    "res_frames = np.load(os.path.join(vid_path, \"circ_arena_dataset.npy\"))\n",
    "print(res_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random images\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.imshow(res_frames[np.random.randint(0, res_frames.shape[0])])\n",
    "\n",
    "print(\"Dataset examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get ellipse labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom version of deepof.utils.circular_arena_recognition\n",
    "def circular_arena_recognition(frame: np.array) -> np.array:\n",
    "    \"\"\"Returns x,y position of the center, the lengths of the major and minor axes, \n",
    "    and the angle of the recognised arena\n",
    "\n",
    "    Parameters:\n",
    "        - frame (np.array): numpy.array representing an individual frame of a video\n",
    "\n",
    "    Returns:\n",
    "        - circles (np.array): 3-element-array containing x,y positions of the center\n",
    "        of the arena, and a third value indicating the radius\"\"\"\n",
    "\n",
    "    # Convert image to greyscale, threshold it, blur it and apply open-close operations\n",
    "    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray_image, 255//4, 255, 0)\n",
    "\n",
    "    # Find contours in the processed image\n",
    "    cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    main_cnt = np.argmax([len(c) for c in cnts])\n",
    "\n",
    "    # Detect the main ellipse containing the arena\n",
    "    ellipse_params = cv2.fitEllipse(cnts[main_cnt])\n",
    "    center_x, center_y = tuple([int(i) for i in ellipse_params[0]])\n",
    "    axes_L, axes_l = tuple([int(i) // 2 for i in ellipse_params[1]])\n",
    "    ellipse_angle = ellipse_params[2]\n",
    "\n",
    "    return int(center_x), int(center_y), int(axes_L), int(axes_l), ellipse_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipse_labels = np.array([circular_arena_recognition(frame) for frame in tqdm(res_frames)])\n",
    "print(ellipse_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random images with ellipses on top\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    random_idx = np.random.randint(0, res_frames.shape[0])\n",
    "    temp_image = res_frames[random_idx].copy()\n",
    "    temp_ellipse = ellipse_labels[random_idx]\n",
    "\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 255, 0),\n",
    "        3,\n",
    "    )\n",
    "    ax1.imshow(temp_image)\n",
    "\n",
    "print(\"Dataset examples with their labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(res_frames, ellipse_labels, test_size=2250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated a training set with {} images of shape {}\".format(X_train.shape[0], X_train.shape[1:]))\n",
    "print(\"Generated a validation set with {} images of shape {}\".format(X_val.shape[0], X_val.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated {} training labels with {} features\".format(y_train.shape[0], y_train.shape[1:]))\n",
    "print(\"Generated {} validation labels with {} features\".format(y_val.shape[0], y_val.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data augmentation using ImageDataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the images and generate samples with different brightness on the fly\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    brightness_range=(0.5,1.5),\n",
    ")\n",
    "train_datagen = train_datagen.flow(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for validation set\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    brightness_range=(0.5,1.5),\n",
    ")\n",
    "val_datagen = val_datagen.flow(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images from generator with labels\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    temp = next(val_datagen)\n",
    "    temp_image = temp[0][0]\n",
    "    temp_ellipse = temp[1][0]\n",
    "        \n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 1, 0),\n",
    "        3,\n",
    "    )\n",
    "    ax1.imshow(temp_image)\n",
    "\n",
    "print(\"ImageDataGenerator examples with their labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train_params = lambda x: int(\n",
    "    np.sum([np.prod(v.get_shape().as_list()) for v in x.trainable_variables])\n",
    ")\n",
    "count_non_train_params = lambda x: int(\n",
    "    np.sum([np.prod(v.get_shape().as_list()) for v in x.non_trainable_variables])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base ResNet50 CNN model\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Before freezing, the base model has {} trainable and {} non-trainable params\".format(\n",
    "        count_train_params(base_model), count_non_train_params(base_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Freeze all layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\n",
    "    \"Before freezing, the base model has {} trainable and {} non-trainable params\".format(\n",
    "        count_train_params(base_model), count_non_train_params(base_model)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a custom top with 5 output neurons.\n",
    "elliptical_arena_detection = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(224, 224, 3)),\n",
    "        base_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    ],\n",
    "    name=\"elliptical_arena_detection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptical_arena_detection.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "elliptical_arena_detection.compile(\n",
    "    loss=tf.keras.losses.mean_squared_error,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[\"mae\", \"mse\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training callbacks\n",
    "run_ID = \"ResNet_ellipse_detection_{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "log_dir = os.path.abspath(os.path.join(\"ellipse_fit\", run_ID))\n",
    "tensorboard_callback = tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1, profile_batch=2,\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = [tensorboard_callback, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "history = elliptical_arena_detection.fit(\n",
    "    train_datagen, epochs=250, validation_data=val_datagen, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "elliptical_arena_detection.save(\"elliptical_arena_detection_{}.tf\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves (also available via tensorboard)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,10), sharex=True)\n",
    "\n",
    "ax1.plot(range(5), history.history[\"mae\"], label=\"training\")\n",
    "ax1.plot(range(5), history.history[\"val_mae\"], label=\"validation\")\n",
    "ax1.set_xlabel(\"epochs\")\n",
    "ax1.set_ylabel(\"MAE\")\n",
    "\n",
    "ax2.plot(range(5), history.history[\"mse\"], label=\"training\")\n",
    "ax2.plot(range(5), history.history[\"val_mse\"], label=\"validation\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylabel(\"MSE\")\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images from generator with predicted labels\n",
    "plt.figure(figsize=(15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "\n",
    "    temp = next(val_datagen)\n",
    "    temp_image = temp[0][0]\n",
    "    temp_ellipse = elliptical_arena_detection.predict(temp_image[np.newaxis, :])[0]\n",
    "\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis(\"on\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    cv2.ellipse(\n",
    "        temp_image,\n",
    "        tuple(temp_ellipse[:2].astype(int)),\n",
    "        tuple(temp_ellipse[2:4].astype(int)),\n",
    "        temp_ellipse[4],\n",
    "        0,\n",
    "        360,\n",
    "        (0, 1, 0),\n",
    "        3,\n",
    "    )\n",
    "    ax1.imshow(temp_image)\n",
    "\n",
    "print(\"ImageDataGenerator examples with their labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
