{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d41506e",
   "metadata": {},
   "source": [
    "# DeepOF unsupervised pipeline: exploring the behavioral space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b46f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")\n",
    "import deepof.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed73656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21595771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"../train_models/deepof_unsupervised_VQVAE_encodings_input=coords_k=100_latdim=8_kmeans_loss=0.0_run=1.pkl\", \"rb\") as handle:\n",
    "    vqvae_solution = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from deepof.post_hoc import get_transitions\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def merge_and_smooth_clusters(\n",
    "    n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "):\n",
    "    \"\"\"Merges the current clusters using a hierarchical agglomerative approach, and smoothens using a Gaussian HMM.\n",
    "\n",
    "    Args:\n",
    "        n_clusters (int): number of clusters to report.\n",
    "        centroids (np.ndarray): precomputed means per cluster.\n",
    "        embedding (tabdict): original deepof.TableDict object containing unsupervised embeddings.\n",
    "        concat_embedding (np.ndarray): concatenated list of embeddings per animal in the dataset.\n",
    "        cluster_assignments (tabdict): original deepof.TableDict object containing cluster assignments.\n",
    "\n",
    "    Returns:\n",
    "        new_soft_assignments (np.ndarray): concatenated postprocessed assignments for all animals in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge clusters ussing a hierarchical agglomerative approach\n",
    "    new_hard_assignments = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, compute_distances=True\n",
    "    ).fit_predict(centroids)\n",
    "    cluster_predictor = LinearDiscriminantAnalysis().fit(\n",
    "        centroids, new_hard_assignments\n",
    "    )\n",
    "    centroids = cluster_predictor.means_\n",
    "    new_soft_assignments = cluster_predictor.predict_proba(concat_embedding)\n",
    "\n",
    "    # Rebuild the soft assignments dictionary per experimental animal\n",
    "    new_soft_assignments = np.split(\n",
    "        new_soft_assignments,\n",
    "        np.cumsum([i.shape[0] for i in embedding.values()]),\n",
    "    )\n",
    "    new_soft_assignments = {\n",
    "        key: val for key, val in zip(cluster_assignments.keys(), new_soft_assignments)\n",
    "    }\n",
    "\n",
    "    # Smooth assignments across time using a Gaussian HMM on the embeddings, with priors based on the clustering results\n",
    "    for key, val in tqdm(new_soft_assignments.items()):\n",
    "\n",
    "        hmm = GaussianHMM(\n",
    "            startprob_prior=np.unique(val.argmax(axis=1), return_counts=True)[1],\n",
    "            transmat_prior=get_transitions(val.argmax(axis=1), n_states=n_clusters) + 10,\n",
    "            means_prior=centroids,\n",
    "            n_components=n_clusters,\n",
    "            covariance_type=\"diag\",\n",
    "            n_iter=100,\n",
    "            tol=0.0001,\n",
    "        )\n",
    "        \n",
    "        hmm.fit(embedding[key].numpy())\n",
    "        new_soft_assignments[key] = hmm.predict_proba(embedding[key].numpy())\n",
    "\n",
    "    return new_soft_assignments\n",
    "\n",
    "\n",
    "def cluster_postprocessing(embedding, cluster_assignments, n_clusters=\"auto\"):\n",
    "    \"\"\"Merges clusters using a hierarchical approach.\n",
    "\n",
    "    Args:\n",
    "        embedding (list): list of embeddings per animal in the dataset.\n",
    "        cluster_assignments (list): list of cluster assignments per animal in the dataset.\n",
    "        n_clusters (int): number of clusters to report.\n",
    "\n",
    "    Returns:\n",
    "        new_soft_assignments (list): list of new (merged) cluster assignments.\n",
    "\n",
    "    \"\"\"\n",
    "    # Concatenate embeddings and cluster assignments in to unique np.ndarray objects\n",
    "    concat_embedding = np.concatenate([tensor.numpy() for tensor in embedding.values()])\n",
    "    hard_assignments = np.concatenate(\n",
    "        [tensor.numpy().argmax(axis=1) for tensor in cluster_assignments.values()]\n",
    "    )\n",
    "\n",
    "    assert concat_embedding.shape[0] == hard_assignments.shape[0]\n",
    "\n",
    "    # Get cluster centroids from the concatenated embeddings\n",
    "    centroids = []\n",
    "    for cluster in range(np.max(hard_assignments)):\n",
    "        centroid = concat_embedding[hard_assignments == cluster]\n",
    "        if len(centroid) == 0:\n",
    "            continue\n",
    "        centroid = np.mean(centroid, axis=0)\n",
    "        centroids.append(centroid)\n",
    "\n",
    "    centroids = np.stack(centroids)\n",
    "\n",
    "    # Merge centroids using a hierarchical approach with the given resolution, and soft-assign instances to clusters\n",
    "    if isinstance(n_clusters, int):\n",
    "        new_soft_assignments = merge_and_smooth_clusters(\n",
    "            n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return new_soft_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# new_ass = cluster_postprocessing(\n",
    "#     vqvae_solution[0], \n",
    "#     vqvae_solution[1],\n",
    "#     n_clusters=12\n",
    "# )\n",
    "# hcc = new_ass['20191203_Day1_SI_JB08_Test_54'].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4d221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# # Cluster on the original embedding space\n",
    "# new_emb = umap.UMAP(n_components=2, n_neighbors=75).fit_transform(vqvae_solution[0]['20191203_Day1_SI_JB08_Test_54'])\n",
    "\n",
    "# sns.scatterplot(x=new_emb[:, 0], y=new_emb[:, 1], hue=hcc, palette=\"tab20\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How prevalent are these clusters?\n",
    "# from collections import Counter\n",
    "# print(Counter(hcc))\n",
    "\n",
    "# new_ass = hcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How often does the model change clusters?\n",
    "# from collections import defaultdict\n",
    "\n",
    "# lengths = defaultdict(list)\n",
    "# cur = 0\n",
    "# for i in range(1, len(new_ass)):\n",
    "#     if new_ass[i-1] == new_ass[i]:\n",
    "#         cur += 1\n",
    "#     else:\n",
    "#         lengths[new_ass[i-1]].append(cur)\n",
    "#         cur = 1\n",
    "\n",
    "# {key:np.mean(val) for key, val in lengths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Duration histograms per cluster\n",
    "# lengths_df = pd.DataFrame([lengths]).melt().explode(\"value\").astype(int)\n",
    "# sns.violinplot(data=lengths_df, x=\"variable\", y=\"value\")\n",
    "\n",
    "# plt.axhline(25, linestyle=\"--\", color=\"black\")\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce3f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project = deepof.data.load(\"../../Desktop/deepOF_CSDS_tutorial_dataset/deepof_tutorial_saved_project_1672667128.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52430042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scales across animals. Can we detect to which animal a given time series belongs to?\n",
    "# Once happy with a solution, check that all animals show comparable cluster interpretations.\n",
    "\n",
    "# Add preprocessing options to include multiple animals, concatenated and together in a graph\n",
    "\n",
    "tt = my_deepof_project.get_coords(center=\"Center\", align=\"Spine_1\")\n",
    "# ss = my_deepof_project.get_coords(speed=1)\n",
    "\n",
    "# tt = cc.merge(ss)\n",
    "\n",
    "tt = tt.preprocess(\n",
    "    window_size=25,\n",
    "    window_step=1,\n",
    "    test_videos=1,\n",
    "    scale=\"standard\",\n",
    "    handle_ids=\"split\", # \"concat\" uses bps from != animals as features, \"split\"\n",
    ")\n",
    "\n",
    "tt = (tt[0][:25000], tt[1][:25000], tt[2][:25000], tt[3][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85d25a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/msmt6cvx5xl9t0p2qjd1tn65jtl6l8/T/ipykernel_12104/2015015358.py:13: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G).todense()\n"
     ]
    }
   ],
   "source": [
    "from deepof.utils import connect_mouse_topview\n",
    "import networkx as nx\n",
    "\n",
    "pp = my_deepof_project.get_graph_dataset(\n",
    "    animal_id=\"B\",\n",
    "    center=\"Center\",\n",
    "    align=\"Spine_1\",\n",
    "    preprocess=True,\n",
    "    scale=\"standard\"\n",
    ")\n",
    "\n",
    "G = connect_mouse_topview(animal_ids=[\"B\"], exclude_bodyparts=[\"Tail_1\", \"Tail_2\", \"Tail_tip\"])\n",
    "adj = nx.adjacency_matrix(G).todense()\n",
    "pp = (pp[0][:25000], pp[1][:25000], pp[2][:25000], pp[3][:25000], pp[4][:25000], pp[5][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a06a411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d50a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:10:00.490775: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-01-03 15:10:00.490786: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2023-01-03 15:10:00.490882: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:10:02.160424: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 25s 74ms/step - total_loss: 50.8482 - reconstruction_loss: 48.9943 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.0497 - kl_divergence: -6.7141 - kmeans_loss: 1.6878 - number_of_populated_clusters: 14.9436 - confidence_in_selected_cluster: 0.4783\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 16s 82ms/step - total_loss: 46.6356 - reconstruction_loss: 45.9653 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.1497 - kl_divergence: -6.0499 - kmeans_loss: 1.3420 - number_of_populated_clusters: 14.9897 - confidence_in_selected_cluster: 0.4126\n",
      "Epoch 3/10\n",
      "  1/195 [..............................] - ETA: 22s - total_loss: 42.1775 - reconstruction_loss: 41.8540 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.2000 - kl_divergence: -5.8449 - kmeans_loss: 1.3096 - number_of_populated_clusters: 15.0000 - confidence_in_selected_cluster: 0.3850"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:10:42.934361: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 16s 80ms/step - total_loss: 45.8676 - reconstruction_loss: 45.8540 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.2497 - kl_divergence: -5.7249 - kmeans_loss: 1.2819 - number_of_populated_clusters: 14.9949 - confidence_in_selected_cluster: 0.3903\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 17s 89ms/step - total_loss: 45.2553 - reconstruction_loss: 45.8251 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.3497 - kl_divergence: -5.6650 - kmeans_loss: 1.2657 - number_of_populated_clusters: 14.9897 - confidence_in_selected_cluster: 0.3868\n",
      "Epoch 5/10\n",
      "  1/195 [..............................] - ETA: 27s - total_loss: 46.2827 - reconstruction_loss: 47.1904 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.4000 - kl_divergence: -5.7315 - kmeans_loss: 1.2273 - number_of_populated_clusters: 15.0000 - confidence_in_selected_cluster: 0.3609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:11:15.993809: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 15s 77ms/step - total_loss: 44.6556 - reconstruction_loss: 45.8186 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.4497 - kl_divergence: -5.8869 - kmeans_loss: 1.3014 - number_of_populated_clusters: 14.9949 - confidence_in_selected_cluster: 0.4012\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 16s 82ms/step - total_loss: 43.9580 - reconstruction_loss: 45.7814 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.5497 - kl_divergence: -6.3366 - kmeans_loss: 1.3807 - number_of_populated_clusters: 14.9846 - confidence_in_selected_cluster: 0.4303\n",
      "Epoch 7/10\n",
      "  1/195 [..............................] - ETA: 25s - total_loss: 41.7762 - reconstruction_loss: 43.9682 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.6000 - kl_divergence: -6.6221 - kmeans_loss: 1.4290 - number_of_populated_clusters: 15.0000 - confidence_in_selected_cluster: 0.4322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:11:47.288774: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 15s 78ms/step - total_loss: 43.1850 - reconstruction_loss: 45.7778 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.6497 - kl_divergence: -7.0135 - kmeans_loss: 1.5001 - number_of_populated_clusters: 14.9385 - confidence_in_selected_cluster: 0.4732\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 16s 83ms/step - total_loss: 42.2391 - reconstruction_loss: 45.7280 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.7497 - kl_divergence: -8.1746 - kmeans_loss: 1.7436 - number_of_populated_clusters: 14.6667 - confidence_in_selected_cluster: 0.5548\n",
      "Epoch 9/10\n",
      "  1/195 [..............................] - ETA: 22s - total_loss: 52.1037 - reconstruction_loss: 56.1550 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.8000 - kl_divergence: -8.8627 - kmeans_loss: 1.8298 - number_of_populated_clusters: 15.0000 - confidence_in_selected_cluster: 0.5976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:12:18.958510: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 16s 83ms/step - total_loss: 41.2382 - reconstruction_loss: 45.7182 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.8497 - kl_divergence: -9.4469 - kmeans_loss: 2.0454 - number_of_populated_clusters: 14.0974 - confidence_in_selected_cluster: 0.6373\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 15s 78ms/step - total_loss: 40.2150 - reconstruction_loss: 45.7614 - clustering_loss: 0.0000e+00 - prior_loss: 0.0000e+00 - kl_weight: 0.9497 - kl_divergence: -10.4375 - kmeans_loss: 2.2876 - number_of_populated_clusters: 13.8051 - confidence_in_selected_cluster: 0.6910\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:12:52.342067: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/195 [..............................] - ETA: 10:28 - total_loss: 44.4326 - reconstruction_loss: 37.9789 - clustering_loss: -1.0000 - prior_loss: 2.7080 - kl_weight: 0.0000e+00 - kl_divergence: -10.8601 - kmeans_loss: 2.3813 - number_of_populated_clusters: 4.0000 - confidence_in_selected_cluster: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:12:55.692480: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-01-03 15:12:55.692489: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/195 [..............................] - ETA: 6:25 - total_loss: 47.7165 - reconstruction_loss: 41.2788 - clustering_loss: -0.9950 - prior_loss: 2.7080 - kl_weight: 7.6923e-04 - kl_divergence: -10.8530 - kmeans_loss: 2.3820 - number_of_populated_clusters: 4.0000 - confidence_in_selected_cluster: 0.9967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:13:01.586760: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-01-03 15:13:01.587378: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2023-01-03 15:13:01.588350: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01\n",
      "\n",
      "2023-01-03 15:13:01.589270: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.trace.json.gz\n",
      "2023-01-03 15:13:01.589866: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01\n",
      "\n",
      "2023-01-03 15:13:01.589947: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.memory_profile.json.gz\n",
      "2023-01-03 15:13:01.590304: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01\n",
      "Dumped tool data for xplane.pb to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /Users/lucas_miranda/PycharmProjects/deepof/unsupervised_trained_models/fit/deepof_unsupervised_VaDE_TCN_encodings_input_type=coords_kmeans_loss=0.0_encoding=8_k=15_20230103-151000/plugins/profile/2023_01_03_15_13_01/MC-C9791E.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 25s 113ms/step - total_loss: 50.2422 - reconstruction_loss: 45.6163 - clustering_loss: -0.9939 - prior_loss: 2.7080 - kl_weight: 0.0497 - kl_divergence: -9.1609 - kmeans_loss: 1.9713 - number_of_populated_clusters: 4.1692 - confidence_in_selected_cluster: 0.9957 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - ETA: 0s - total_loss: 48.4425 - reconstruction_loss: 45.5906 - clustering_loss: -0.9905 - prior_loss: 2.7080 - kl_weight: 0.1497 - kl_divergence: -7.5018 - kmeans_loss: 1.6298 - number_of_populated_clusters: 4.4821 - confidence_in_selected_cluster: 0.9934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:13:34.139225: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 17s 87ms/step - total_loss: 48.4425 - reconstruction_loss: 45.5906 - clustering_loss: -0.9905 - prior_loss: 2.7080 - kl_weight: 0.1497 - kl_divergence: -7.5018 - kmeans_loss: 1.6298 - number_of_populated_clusters: 4.4821 - confidence_in_selected_cluster: 0.9934 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "106/195 [===============>..............] - ETA: 6s - total_loss: 46.9287 - reconstruction_loss: 44.7818 - clustering_loss: -0.9904 - prior_loss: 2.7080 - kl_weight: 0.2269 - kl_divergence: -6.9467 - kmeans_loss: 1.5650 - number_of_populated_clusters: 4.8679 - confidence_in_selected_cluster: 0.9932"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cons = my_deepof_project.deep_unsupervised_embedding(\n",
    "    pp,\n",
    "    adjacency_matrix=adj,\n",
    "    embedding_model=\"VaDE\",\n",
    "    epochs=10,\n",
    "    encoder_type=\"TCN\",\n",
    "    n_components=15,\n",
    "    latent_dim=8,\n",
    "    kl_warmup=10,\n",
    "    kl_annealing_mode=\"linear\",\n",
    "    batch_size=128,\n",
    "    kmeans_loss=0.0,\n",
    "    reg_cat_clusters=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556420",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = cons.encoder([pp[0][:25000], pp[1][:25000]])\n",
    "cls = cons.grouper([pp[0][:25000], pp[1][:25000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73357da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap = umap.UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=50,\n",
    "    min_dist=1.0,\n",
    ").fit_transform(emb.numpy())\n",
    "# umap = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cls.numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd99b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tt = GaussianMixture(n_components=5, covariance_type=\"diag\", reg_covar=1e-04).fit(emb.numpy())\n",
    "#means = tt.means_\n",
    "means = cons.get_gmm_params['means'].numpy()\n",
    "\n",
    "sns.scatterplot(x=umap[:, 0], y=umap[:, 1], hue=cls.numpy().argmax(axis=1), palette=\"tab20\")\n",
    "means = cons.get_layer(\"grouper\").get_layer(\"gaussian_mixture_latent\").c_mu.numpy()\n",
    "#sns.scatterplot(x=means[:,0], y=means[:,1], s=250, c=\"black\")\n",
    "\n",
    "plt.title(\"GMVAE embeddings\")\n",
    "\n",
    "# plt.legend(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21006d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.get_gmm_params['weights'].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.split(np.concatenate(tt), np.cumsum([i.shape[0] for k,i in vqvae_solution[0].items() if k in list(cc.keys())]))\n",
    "\n",
    "for i in tt:\n",
    "    print(i.shape)\n",
    "    print(np.max(np.abs(i.mean(axis=0))))\n",
    "    print(np.mean(np.abs(i.std(axis=0))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
