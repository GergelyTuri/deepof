{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b46f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")\n",
    "import deepof.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed73656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41506e",
   "metadata": {},
   "source": [
    "# DeepOF unsupervised pipeline: exploring the behavioral space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e24f1d",
   "metadata": {},
   "source": [
    "##### Tutorial index:\n",
    "    \n",
    "* Brief introduction to unsupervised analysis.\n",
    "* Load your previous project.\n",
    "* Running an unsupervised analysis with default parameters.\n",
    "* Understanding the different available models.\n",
    "* Cluster number selection.\n",
    "* Temporal and global embeddings.\n",
    "* Global separation dynamics.\n",
    "* Exploring cluster enrichment across conditions.\n",
    "* Exploring cluster dynamics across conditions.\n",
    "* Interpreting clusters using SHAP.\n",
    "* Exporting cluster video snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bcf28",
   "metadata": {},
   "source": [
    "### Brief introduction to unsupervised analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a7f23",
   "metadata": {},
   "source": [
    "### Load your previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce3f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project = deepof.data.load_project(\"../../Desktop/deepOF_CSDS_tutorial_dataset/deepof_tutorial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52430042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE LAST SECTION OF THE FIRST TUTORIAL!\n",
    "\n",
    "# Check scales across animals. Can we detect to which animal a given time series belongs to?\n",
    "# Once happy with a solution, check that all animals show comparable cluster interpretations.\n",
    "\n",
    "# Add preprocessing options to include multiple animals, concatenated and together in a graph\n",
    "\n",
    "tt = my_deepof_project.get_coords(center=\"Center\", align=\"Spine_1\")\n",
    "# ss = my_deepof_project.get_coords(speed=1)\n",
    "\n",
    "# tt = cc.merge(ss)\n",
    "\n",
    "tt = tt.preprocess(\n",
    "    window_size=25,\n",
    "    window_step=1,\n",
    "    test_videos=1,\n",
    "    scale=\"standard\",\n",
    "    handle_ids=\"split\", # \"concat\" uses bps from != animals as features, \"split\"\n",
    ")\n",
    "\n",
    "tt = (tt[0][:25000], tt[1][:25000], tt[2][:25000], tt[3][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d25a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/msmt6cvx5xl9t0p2qjd1tn65jtl6l8/T/ipykernel_32419/2876476372.py:14: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G).todense()\n"
     ]
    }
   ],
   "source": [
    "# SAME HERE: Last section of the last tutorial should explore tensor and graph preprocessing\n",
    "\n",
    "from deepof.utils import connect_mouse_topview\n",
    "import networkx as nx\n",
    "\n",
    "pp, G, to_preprocess = my_deepof_project.get_graph_dataset(\n",
    "   # animal_id=\"B\",\n",
    "    center=\"Center\",\n",
    "    align=\"Spine_1\",\n",
    "    preprocess=True,\n",
    "    scale=\"standard\"\n",
    ")\n",
    "\n",
    "adj = nx.adjacency_matrix(G).todense()\n",
    "pp = (pp[0][:25000], pp[1][:25000], pp[2][:25000], pp[3][:25000], pp[4][:25000], pp[5][:25000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bc6d3",
   "metadata": {},
   "source": [
    "### Running an unsupervised analysis with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64d50a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 15:11:07.502642: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-01-10 15:11:07.503283: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2023-01-10 15:11:07.505311: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Exception encountered when calling layer \"time_distributed_16\" (type TimeDistributed).\n\nPlease run in eager mode or implement the `compute_output_shape` method on your layer (TransformerEncoder).\n\nCall arguments received by layer \"time_distributed_16\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(None, 22, 25, 3), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/data.py:1590\u001b[0m, in \u001b[0;36mCoordinates.deep_unsupervised_embedding\u001b[0;34m(preprocessed_object, adjacency_matrix, embedding_model, encoder_type, batch_size, latent_dim, epochs, log_history, log_hparams, n_components, kmeans_loss, temperature, contrastive_similarity_function, contrastive_loss_function, beta, tau, output_path, pretrained, save_checkpoints, save_weights, input_type, run, strategy, kl_annealing_mode, kl_warmup, reg_cat_clusters)\u001b[0m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeep_unsupervised_embedding\u001b[39m(\n\u001b[1;32m   1523\u001b[0m     preprocessed_object: Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     reg_cat_clusters: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1549\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;124;03m\"\"\"Annotates coordinates using a deep unsupervised autoencoder.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \n\u001b[1;32m   1552\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;124;03m        Tuple: Tuple containing all trained models. See specific model documentation under deepof.models for details.\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m     trained_models \u001b[38;5;241m=\u001b[39m \u001b[43mdeepof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder_fitting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessed_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessed_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_hparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_hparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkmeans_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkmeans_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrastive_similarity_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrastive_similarity_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrastive_loss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrastive_loss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_annealing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_annealing_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_warmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_warmup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreg_cat_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_cat_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;66;03m# returns a list of trained tensorflow models\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/model_utils.py:1394\u001b[0m, in \u001b[0;36mautoencoder_fitting\u001b[0;34m(preprocessed_object, adjacency_matrix, embedding_model, encoder_type, batch_size, latent_dim, epochs, log_history, log_hparams, n_components, output_path, kmeans_loss, pretrained, save_checkpoints, save_weights, input_type, kl_annealing_mode, kl_warmup, reg_cat_clusters, temperature, contrastive_similarity_function, contrastive_loss_function, beta, tau, run, strategy)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVQVAE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1394\u001b[0m         ae_full_model \u001b[38;5;241m=\u001b[39m \u001b[43mdeepof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVQVAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m            \u001b[49m\u001b[43medge_feature_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m            \u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_object\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkmeans_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkmeans_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1404\u001b[0m         ae_full_model\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mNadam(\n\u001b[1;32m   1405\u001b[0m             learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, clipvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m\n\u001b[1;32m   1406\u001b[0m         )\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m embedding_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVaDE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/models.py:880\u001b[0m, in \u001b[0;36mVQVAE.__init__\u001b[0;34m(self, input_shape, edge_feature_shape, adjacency_matrix, latent_dim, n_components, beta, kmeans_loss, use_gnn, encoder_type, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_type \u001b[38;5;241m=\u001b[39m encoder_type\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Define VQ_VAE model\u001b[39;00m\n\u001b[1;32m    874\u001b[0m (\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder,\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder,\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper,\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_grouper,\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae,\n\u001b[0;32m--> 880\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mget_vqvae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_feature_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Define metrics to track\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_loss_tracker \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMean(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/models.py:805\u001b[0m, in \u001b[0;36mget_vqvae\u001b[0;34m(input_shape, edge_feature_shape, adjacency_matrix, latent_dim, use_gnn, n_components, beta, kmeans_loss, encoder_type)\u001b[0m\n\u001b[1;32m    802\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m get_TCN_decoder(input_shape\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m:], latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m encoder_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 805\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_transformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_feature_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_feature_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m get_transformer_decoder(input_shape[\u001b[38;5;241m1\u001b[39m:], latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# Connect encoder and quantizer\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/models.py:491\u001b[0m, in \u001b[0;36mget_transformer_encoder\u001b[0;34m(input_shape, edge_feature_shape, adjacency_matrix, latent_dim, use_gnn, num_layers, num_heads, dff, dropout_rate)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     x_reshaped \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 491\u001b[0m transformer_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_position_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gnn:\n\u001b[1;32m    504\u001b[0m \n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Embed edge features too\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     transformer_a_embedding \u001b[38;5;241m=\u001b[39m TimeDistributed(\n\u001b[1;32m    507\u001b[0m         deepof\u001b[38;5;241m.\u001b[39mmodel_utils\u001b[38;5;241m.\u001b[39mTransformerEncoder(\n\u001b[1;32m    508\u001b[0m             num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m     )(a_reshaped, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/keras/engine/base_layer.py:827\u001b[0m, in \u001b[0;36mLayer.compute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    822\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe could not automatically infer the static shape of the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    823\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms output. Please implement the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    824\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`compute_output_shape` method on your layer (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    825\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    826\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mshape, outputs)\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease run in eager mode or implement the `compute_output_shape` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod on your layer (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling layer \"time_distributed_16\" (type TimeDistributed).\n\nPlease run in eager mode or implement the `compute_output_shape` method on your layer (TransformerEncoder).\n\nCall arguments received by layer \"time_distributed_16\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(None, 22, 25, 3), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cons = my_deepof_project.deep_unsupervised_embedding(\n",
    "    pp,\n",
    "    adjacency_matrix=adj,\n",
    "    embedding_model=\"VQVAE\",\n",
    "    epochs=1,\n",
    "    encoder_type=\"transformer\",\n",
    "    n_components=15,\n",
    "    latent_dim=4,\n",
    "    kl_warmup=10,\n",
    "    kl_annealing_mode=\"linear\",\n",
    "    batch_size=128,\n",
    "    kmeans_loss=0.0,\n",
    "    reg_cat_clusters=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e0b",
   "metadata": {},
   "source": [
    "### Understanding the different available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff9e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VaDE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 25, 66)]     0           []                               \n",
      "                                                                                                  \n",
      " encoder_edge_features (InputLa  [(None, 25, 26)]    0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " recurrent_encoder (Functional)  (None, 4)           3900        ['input_26[0][0]',               \n",
      "                                                                  'encoder_edge_features[0][0]']  \n",
      "                                                                                                  \n",
      " gaussian_mixture_latent (Gauss  ((None, 4),         161         ['recurrent_encoder[0][0]']      \n",
      " ianMixtureLatent)               (None, 15))                                                      \n",
      "                                                                                                  \n",
      " recurrent_decoder (Functional)  (None, 25, 66)      2402        ['gaussian_mixture_latent[0][0]',\n",
      "                                                                  'input_26[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,463\n",
      "Trainable params: 6,462\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cons.vade.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ec89d",
   "metadata": {},
   "source": [
    "### Cluster number selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b207299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3175278c",
   "metadata": {},
   "source": [
    "### Visualizing temporal and global embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af44272",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project.get_exp_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad890",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "soft_counts = {}\n",
    "\n",
    "for key in my_deepof_project.get_exp_conditions.keys():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    dd, _, _ = my_deepof_project.get_graph_dataset(\n",
    "       # animal_id=\"B\",\n",
    "        precomputed_tab_dict=to_preprocess.filter_videos([key]),\n",
    "        preprocess=True,\n",
    "        scale=\"standard\",\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    dd = (dd[0], dd[1], dd[2], dd[3], dd[4], dd[5])\n",
    "\n",
    "#     dd = my_deepof_project.get_coords(\n",
    "#         center=\"Center\", align=\"Spine_1\"\n",
    "#     ).filter_id(\"B\").filter_videos([key]).preprocess(\n",
    "#         shuffle=False,\n",
    "#         test_videos=0,\n",
    "#     )\n",
    "#     dd = [dd[0], dd[0]]\n",
    "    \n",
    "    embeddings[key] = cons.encoder([dd[0], dd[1]])\n",
    "    soft_counts[key] = cons.grouper([dd[0], dd[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_counts['20191204_Day2_SI_JB08_Test_56'].numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION: Load precomputed embeddings. \n",
    "# Solve this later by defaulting embedding saving to a known directory within the project path.\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# emb=\"TCN\"\n",
    "# run=2\n",
    "# with open(f\"../../../Results/CSDS_embeddings/deepof_unsupervised_VaDE_encoder_{emb}_encodings_input=graph_k=25_latdim=8_changepoints_False_kmeans_loss=0.0_run={run}.pkl\", \"rb\") as handle:\n",
    "#     embeddings, soft_counts, breaks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70433618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patched_breaks(key):\n",
    "        \n",
    "    tt, g, prep = my_deepof_project.get_graph_dataset(center=\"Center\", align=\"Spine_1\", animal_id=\"B\", shuffle=False)\n",
    "    tt = my_deepof_project.get_graph_dataset(\n",
    "        precomputed_tab_dict=prep.filter_videos([key]),\n",
    "        animal_id=\"B\",\n",
    "        window_size=25,\n",
    "        window_step=1,\n",
    "        shuffle=False,\n",
    "        automatic_changepoints=False,\n",
    "        scale=\"standard\",\n",
    "        test_videos=0,\n",
    "    )[0]\n",
    "\n",
    "    return (~np.all(tt[0] == 0, axis=2)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "breaks = {\n",
    "    key: get_patched_breaks(key) for key in my_deepof_project.get_exp_conditions.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50ace5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project, \n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=False,\n",
    "    ax=ax1,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot\n",
    ")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project,\n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=\"mean\",\n",
    "    ax=ax2,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a99d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    "    frame_limit=250,\n",
    "    #selected_cluster=1,\n",
    "    dpi=60,\n",
    "    #center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f1116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    "    frame_limit=250,\n",
    "    selected_cluster=4,\n",
    "    dpi=60,\n",
    "    #center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n",
    "\n",
    "#TODO: Correct for misalignment. The way this is implemented, \n",
    "#      makes the last data point the representative of the sequence, not the middle one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0e11",
   "metadata": {},
   "source": [
    "### Global separation dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec60eee",
   "metadata": {},
   "source": [
    "### Exploring cluster enrichment across conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950730c",
   "metadata": {},
   "source": [
    "### Exploring cluster dynamics across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_gantt(\n",
    "    my_deepof_project,\n",
    "    soft_counts=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffb1d3",
   "metadata": {},
   "source": [
    "### Interpreting clusters using SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c18109",
   "metadata": {},
   "source": [
    "### Exporting cluster video snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: they should be saved to a directory that makes sense!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = [i for i in os.listdir(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\")]\n",
    "\n",
    "for f in files:\n",
    "    cur = pd.read_hdf(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f)\n",
    "    \n",
    "    cur.to_csv(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f[:-2] + \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556420",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = cons.encoder([pp[0][:25000], pp[1][:25000]])\n",
    "cls = cons.quantizer([pp[0][:25000], pp[1][:25000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73357da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap = umap.UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=250,\n",
    "    min_dist=1.0,\n",
    ").fit_transform(emb.numpy())\n",
    "# umap = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cls.numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd99b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tt = GaussianMixture(n_components=5, covariance_type=\"diag\", reg_covar=1e-04).fit(emb.numpy())\n",
    "#means = tt.means_\n",
    "#means = cons.get_gmm_params['means'].numpy()\n",
    "\n",
    "sns.scatterplot(x=umap[:, 0], y=umap[:, 1], hue=cls.numpy().argmax(axis=1), palette=\"tab20\")\n",
    "# means = cons.get_layer(\"grouper\").get_layer(\"gaussian_mixture_latent\").c_mu.numpy()\n",
    "# sns.scatterplot(x=means[:,0], y=means[:,1], s=250, c=\"black\")\n",
    "\n",
    "plt.title(\"DeepOF embeddings\")\n",
    "\n",
    "# plt.legend(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21006d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.get_gmm_params['weights'].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.split(np.concatenate(tt), np.cumsum([i.shape[0] for k,i in vqvae_solution[0].items() if k in list(cc.keys())]))\n",
    "\n",
    "for i in tt:\n",
    "    print(i.shape)\n",
    "    print(np.max(np.abs(i.mean(axis=0))))\n",
    "    print(np.mean(np.abs(i.std(axis=0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21595771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# with open(\"../train_models/deepof_unsupervised_VQVAE_encodings_input=coords_k=100_latdim=8_kmeans_loss=0.0_run=1.pkl\", \"rb\") as handle:\n",
    "#     vqvae_solution = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from deepof.post_hoc import get_transitions\n",
    "# from hmmlearn.hmm import GaussianHMM\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# def merge_and_smooth_clusters(\n",
    "#     n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "# ):\n",
    "#     \"\"\"Merges the current clusters using a hierarchical agglomerative approach, and smoothens using a Gaussian HMM.\n",
    "\n",
    "#     Args:\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "#         centroids (np.ndarray): precomputed means per cluster.\n",
    "#         embedding (tabdict): original deepof.TableDict object containing unsupervised embeddings.\n",
    "#         concat_embedding (np.ndarray): concatenated list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (tabdict): original deepof.TableDict object containing cluster assignments.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (np.ndarray): concatenated postprocessed assignments for all animals in the dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Merge clusters ussing a hierarchical agglomerative approach\n",
    "#     new_hard_assignments = AgglomerativeClustering(\n",
    "#         n_clusters=n_clusters, compute_distances=True\n",
    "#     ).fit_predict(centroids)\n",
    "#     cluster_predictor = LinearDiscriminantAnalysis().fit(\n",
    "#         centroids, new_hard_assignments\n",
    "#     )\n",
    "#     centroids = cluster_predictor.means_\n",
    "#     new_soft_assignments = cluster_predictor.predict_proba(concat_embedding)\n",
    "\n",
    "#     # Rebuild the soft assignments dictionary per experimental animal\n",
    "#     new_soft_assignments = np.split(\n",
    "#         new_soft_assignments,\n",
    "#         np.cumsum([i.shape[0] for i in embedding.values()]),\n",
    "#     )\n",
    "#     new_soft_assignments = {\n",
    "#         key: val for key, val in zip(cluster_assignments.keys(), new_soft_assignments)\n",
    "#     }\n",
    "\n",
    "#     # Smooth assignments across time using a Gaussian HMM on the embeddings, with priors based on the clustering results\n",
    "#     for key, val in tqdm(new_soft_assignments.items()):\n",
    "\n",
    "#         hmm = GaussianHMM(\n",
    "#             startprob_prior=np.unique(val.argmax(axis=1), return_counts=True)[1],\n",
    "#             transmat_prior=get_transitions(val.argmax(axis=1), n_states=n_clusters) + 10,\n",
    "#             means_prior=centroids,\n",
    "#             n_components=n_clusters,\n",
    "#             covariance_type=\"diag\",\n",
    "#             n_iter=100,\n",
    "#             tol=0.0001,\n",
    "#         )\n",
    "        \n",
    "#         hmm.fit(embedding[key].numpy())\n",
    "#         new_soft_assignments[key] = hmm.predict_proba(embedding[key].numpy())\n",
    "\n",
    "#     return new_soft_assignments\n",
    "\n",
    "\n",
    "# def cluster_postprocessing(embedding, cluster_assignments, n_clusters=\"auto\"):\n",
    "#     \"\"\"Merges clusters using a hierarchical approach.\n",
    "\n",
    "#     Args:\n",
    "#         embedding (list): list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (list): list of cluster assignments per animal in the dataset.\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (list): list of new (merged) cluster assignments.\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Concatenate embeddings and cluster assignments in to unique np.ndarray objects\n",
    "#     concat_embedding = np.concatenate([tensor.numpy() for tensor in embedding.values()])\n",
    "#     hard_assignments = np.concatenate(\n",
    "#         [tensor.numpy().argmax(axis=1) for tensor in cluster_assignments.values()]\n",
    "#     )\n",
    "\n",
    "#     assert concat_embedding.shape[0] == hard_assignments.shape[0]\n",
    "\n",
    "#     # Get cluster centroids from the concatenated embeddings\n",
    "#     centroids = []\n",
    "#     for cluster in range(np.max(hard_assignments)):\n",
    "#         centroid = concat_embedding[hard_assignments == cluster]\n",
    "#         if len(centroid) == 0:\n",
    "#             continue\n",
    "#         centroid = np.mean(centroid, axis=0)\n",
    "#         centroids.append(centroid)\n",
    "\n",
    "#     centroids = np.stack(centroids)\n",
    "\n",
    "#     # Merge centroids using a hierarchical approach with the given resolution, and soft-assign instances to clusters\n",
    "#     if isinstance(n_clusters, int):\n",
    "#         new_soft_assignments = merge_and_smooth_clusters(\n",
    "#             n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     return new_soft_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# new_ass = cluster_postprocessing(\n",
    "#     vqvae_solution[0], \n",
    "#     vqvae_solution[1],\n",
    "#     n_clusters=12\n",
    "# )\n",
    "# hcc = new_ass['20191203_Day1_SI_JB08_Test_54'].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4d221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# # Cluster on the original embedding space\n",
    "# new_emb = umap.UMAP(n_components=2, n_neighbors=75).fit_transform(vqvae_solution[0]['20191203_Day1_SI_JB08_Test_54'])\n",
    "\n",
    "# sns.scatterplot(x=new_emb[:, 0], y=new_emb[:, 1], hue=hcc, palette=\"tab20\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How prevalent are these clusters?\n",
    "# from collections import Counter\n",
    "# print(Counter(hcc))\n",
    "\n",
    "# new_ass = hcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How often does the model change clusters?\n",
    "# from collections import defaultdict\n",
    "\n",
    "# lengths = defaultdict(list)\n",
    "# cur = 0\n",
    "# for i in range(1, len(new_ass)):\n",
    "#     if new_ass[i-1] == new_ass[i]:\n",
    "#         cur += 1\n",
    "#     else:\n",
    "#         lengths[new_ass[i-1]].append(cur)\n",
    "#         cur = 1\n",
    "\n",
    "# {key:np.mean(val) for key, val in lengths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Duration histograms per cluster\n",
    "# lengths_df = pd.DataFrame([lengths]).melt().explode(\"value\").astype(int)\n",
    "# sns.violinplot(data=lengths_df, x=\"variable\", y=\"value\")\n",
    "\n",
    "# plt.axhline(25, linestyle=\"--\", color=\"black\")\n",
    "        \n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
