{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b46f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")\n",
    "import deepof.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed73656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41506e",
   "metadata": {},
   "source": [
    "# DeepOF unsupervised pipeline: exploring the behavioral space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e24f1d",
   "metadata": {},
   "source": [
    "##### Tutorial index:\n",
    "    \n",
    "* Brief introduction to unsupervised analysis.\n",
    "* Load your previous project.\n",
    "* Running an unsupervised analysis with default parameters.\n",
    "* Understanding the different available models.\n",
    "* Cluster number selection.\n",
    "* Temporal and global embeddings.\n",
    "* Global separation dynamics.\n",
    "* Exploring cluster enrichment across conditions.\n",
    "* Exploring cluster dynamics across conditions.\n",
    "* Interpreting clusters using SHAP.\n",
    "* Exporting cluster video snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bcf28",
   "metadata": {},
   "source": [
    "### Brief introduction to unsupervised analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a7f23",
   "metadata": {},
   "source": [
    "### Load your previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project = deepof.data.load_project(\"../../Desktop/deepOF_CSDS_tutorial_dataset/deepof_tutorial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52430042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE LAST SECTION OF THE FIRST TUTORIAL!\n",
    "\n",
    "# Check scales across animals. Can we detect to which animal a given time series belongs to?\n",
    "# Once happy with a solution, check that all animals show comparable cluster interpretations.\n",
    "\n",
    "# Add preprocessing options to include multiple animals, concatenated and together in a graph\n",
    "\n",
    "tt = my_deepof_project.get_coords(center=\"Center\", align=\"Spine_1\")\n",
    "# ss = my_deepof_project.get_coords(speed=1)\n",
    "\n",
    "# tt = cc.merge(ss)\n",
    "\n",
    "tt, _ = tt.preprocess(\n",
    "    window_size=25,\n",
    "    window_step=1,\n",
    "    test_videos=1,\n",
    "    scale=\"standard\",\n",
    "    handle_ids=\"split\", # \"concat\" uses bps from != animals as features, \"split\"\n",
    ")\n",
    "\n",
    "tt = (tt[0][:25000], tt[1][:25000], tt[2][:25000], tt[3][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d25a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME HERE: Last section of the last tutorial should explore tensor and graph preprocessing\n",
    "\n",
    "from deepof.utils import connect_mouse_topview\n",
    "import networkx as nx\n",
    "\n",
    "pp, G, to_preprocess, global_scaler = my_deepof_project.get_graph_dataset(\n",
    "    animal_id=\"B\",\n",
    "    center=\"Center\",\n",
    "    align=\"Spine_1\",\n",
    "    preprocess=True,\n",
    "    scale=\"standard\"\n",
    ")\n",
    "\n",
    "adj = nx.adjacency_matrix(G).todense()\n",
    "pp = (pp[0][:25000], pp[1][:25000], pp[2][:25000], pp[3][:25000], pp[4][:25000], pp[5][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98060f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens if we use a two-stage normalization approach?\n",
    "# 1) Normalize within animals to get rid of body size effects\n",
    "# 2) Normalize across animals to have similar stats across animals, and get the same clusters regardless of the animal\n",
    "# ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bc6d3",
   "metadata": {},
   "source": [
    "### Running an unsupervised analysis with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d50a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cons = my_deepof_project.deep_unsupervised_embedding(\n",
    "    pp,\n",
    "    adjacency_matrix=adj,\n",
    "    embedding_model=\"VaDE\",\n",
    "    epochs=10,\n",
    "    encoder_type=\"recurrent\",\n",
    "    n_components=15,\n",
    "    latent_dim=4,\n",
    "    kl_warmup=10,\n",
    "    kl_annealing_mode=\"linear\",\n",
    "    batch_size=128,\n",
    "    kmeans_loss=0.0,\n",
    "    reg_cat_clusters=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e0b",
   "metadata": {},
   "source": [
    "### Understanding the different available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.vade.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ec89d",
   "metadata": {},
   "source": [
    "### Cluster number selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b207299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3175278c",
   "metadata": {},
   "source": [
    "### Visualizing temporal and global embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af44272",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project.get_exp_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad890",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "soft_counts = {}\n",
    "\n",
    "for key in my_deepof_project.get_exp_conditions.keys():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    dd, _, _, _ = my_deepof_project.get_graph_dataset(\n",
    "        animal_id=\"B\",\n",
    "        precomputed_tab_dict=to_preprocess.filter_videos([key]),\n",
    "        preprocess=True,\n",
    "        scale=\"standard\",\n",
    "        shuffle=False,\n",
    "        pretrained_scaler=global_scaler,\n",
    "    )\n",
    "\n",
    "    dd = (dd[0], dd[1], dd[2], dd[3], dd[4], dd[5])\n",
    "\n",
    "#     dd = my_deepof_project.get_coords(\n",
    "#         center=\"Center\", align=\"Spine_1\"\n",
    "#     ).filter_id(\"B\").filter_videos([key]).preprocess(\n",
    "#         shuffle=False,\n",
    "#         test_videos=0,\n",
    "#     )\n",
    "#     dd = [dd[0], dd[0]]\n",
    "    \n",
    "    embeddings[key] = cons.encoder([dd[0], dd[1]])\n",
    "    soft_counts[key] = cons.grouper([dd[0], dd[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(soft_counts['20191204_Day2_SI_JB08_Test_56'].numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION: Load precomputed embeddings. \n",
    "# Solve this later by defaulting embedding saving to a known directory within the project path.\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# emb=\"TCN\"\n",
    "# run=2\n",
    "# with open(f\"../../../Results/CSDS_embeddings/deepof_unsupervised_VaDE_encoder_{emb}_encodings_input=graph_k=25_latdim=8_changepoints_False_kmeans_loss=0.0_run={run}.pkl\", \"rb\") as handle:\n",
    "#     embeddings, soft_counts, breaks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70433618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patched_breaks(key):\n",
    "        \n",
    "    tt, g, prep, _ = my_deepof_project.get_graph_dataset(center=\"Center\", align=\"Spine_1\", animal_id=\"B\", shuffle=False)\n",
    "    tt = my_deepof_project.get_graph_dataset(\n",
    "        precomputed_tab_dict=prep.filter_videos([key]),\n",
    "        animal_id=\"B\",\n",
    "        window_size=25,\n",
    "        window_step=1,\n",
    "        shuffle=False,\n",
    "        automatic_changepoints=False,\n",
    "        scale=\"standard\",\n",
    "        test_videos=0,\n",
    "    )[0]\n",
    "\n",
    "    return (~np.all(tt[0] == 0, axis=2)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "breaks = {\n",
    "    key: get_patched_breaks(key) for key in my_deepof_project.get_exp_conditions.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50ace5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project, \n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=False,\n",
    "    ax=ax1,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot\n",
    ")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project,\n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=\"median\",\n",
    "    ax=ax2,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a99d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "    animal_id=\"B\",\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    "    frame_limit=150,\n",
    "    selected_cluster=7,\n",
    "    dpi=60,\n",
    "    #center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e61418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "    animal_id=\"B\",\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_56\",\n",
    "    frame_limit=150,\n",
    "    selected_cluster=7,\n",
    "    dpi=60,\n",
    "    #center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0e11",
   "metadata": {},
   "source": [
    "### Global separation dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6dc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habituation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec60eee",
   "metadata": {},
   "source": [
    "### Exploring cluster enrichment across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2258e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "deepof.visuals.plot_cluster_enrichment(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950730c",
   "metadata": {},
   "source": [
    "### Exploring cluster dynamics across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_gantt(\n",
    "    my_deepof_project,\n",
    "    soft_counts=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffb1d3",
   "metadata": {},
   "source": [
    "### Interpreting clusters using SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c18109",
   "metadata": {},
   "source": [
    "### Exporting cluster video snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: they should be saved to a directory that makes sense!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = [i for i in os.listdir(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\")]\n",
    "\n",
    "for f in files:\n",
    "    cur = pd.read_hdf(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f)\n",
    "    \n",
    "    cur.to_csv(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f[:-2] + \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556420",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = cons.encoder([pp[0][:25000], pp[1][:25000]])\n",
    "cls = cons.quantizer([pp[0][:25000], pp[1][:25000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73357da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap = umap.UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=250,\n",
    "    min_dist=1.0,\n",
    ").fit_transform(emb.numpy())\n",
    "# umap = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cls.numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd99b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tt = GaussianMixture(n_components=5, covariance_type=\"diag\", reg_covar=1e-04).fit(emb.numpy())\n",
    "#means = tt.means_\n",
    "#means = cons.get_gmm_params['means'].numpy()\n",
    "\n",
    "sns.scatterplot(x=umap[:, 0], y=umap[:, 1], hue=cls.numpy().argmax(axis=1), palette=\"tab20\")\n",
    "# means = cons.get_layer(\"grouper\").get_layer(\"gaussian_mixture_latent\").c_mu.numpy()\n",
    "# sns.scatterplot(x=means[:,0], y=means[:,1], s=250, c=\"black\")\n",
    "\n",
    "plt.title(\"DeepOF embeddings\")\n",
    "\n",
    "# plt.legend(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21006d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.get_gmm_params['weights'].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.split(np.concatenate(tt), np.cumsum([i.shape[0] for k,i in vqvae_solution[0].items() if k in list(cc.keys())]))\n",
    "\n",
    "for i in tt:\n",
    "    print(i.shape)\n",
    "    print(np.max(np.abs(i.mean(axis=0))))\n",
    "    print(np.mean(np.abs(i.std(axis=0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21595771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# with open(\"../train_models/deepof_unsupervised_VQVAE_encodings_input=coords_k=100_latdim=8_kmeans_loss=0.0_run=1.pkl\", \"rb\") as handle:\n",
    "#     vqvae_solution = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from deepof.post_hoc import get_transitions\n",
    "# from hmmlearn.hmm import GaussianHMM\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# def merge_and_smooth_clusters(\n",
    "#     n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "# ):\n",
    "#     \"\"\"Merges the current clusters using a hierarchical agglomerative approach, and smoothens using a Gaussian HMM.\n",
    "\n",
    "#     Args:\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "#         centroids (np.ndarray): precomputed means per cluster.\n",
    "#         embedding (tabdict): original deepof.TableDict object containing unsupervised embeddings.\n",
    "#         concat_embedding (np.ndarray): concatenated list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (tabdict): original deepof.TableDict object containing cluster assignments.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (np.ndarray): concatenated postprocessed assignments for all animals in the dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Merge clusters ussing a hierarchical agglomerative approach\n",
    "#     new_hard_assignments = AgglomerativeClustering(\n",
    "#         n_clusters=n_clusters, compute_distances=True\n",
    "#     ).fit_predict(centroids)\n",
    "#     cluster_predictor = LinearDiscriminantAnalysis().fit(\n",
    "#         centroids, new_hard_assignments\n",
    "#     )\n",
    "#     centroids = cluster_predictor.means_\n",
    "#     new_soft_assignments = cluster_predictor.predict_proba(concat_embedding)\n",
    "\n",
    "#     # Rebuild the soft assignments dictionary per experimental animal\n",
    "#     new_soft_assignments = np.split(\n",
    "#         new_soft_assignments,\n",
    "#         np.cumsum([i.shape[0] for i in embedding.values()]),\n",
    "#     )\n",
    "#     new_soft_assignments = {\n",
    "#         key: val for key, val in zip(cluster_assignments.keys(), new_soft_assignments)\n",
    "#     }\n",
    "\n",
    "#     # Smooth assignments across time using a Gaussian HMM on the embeddings, with priors based on the clustering results\n",
    "#     for key, val in tqdm(new_soft_assignments.items()):\n",
    "\n",
    "#         hmm = GaussianHMM(\n",
    "#             startprob_prior=np.unique(val.argmax(axis=1), return_counts=True)[1],\n",
    "#             transmat_prior=get_transitions(val.argmax(axis=1), n_states=n_clusters) + 10,\n",
    "#             means_prior=centroids,\n",
    "#             n_components=n_clusters,\n",
    "#             covariance_type=\"diag\",\n",
    "#             n_iter=100,\n",
    "#             tol=0.0001,\n",
    "#         )\n",
    "        \n",
    "#         hmm.fit(embedding[key].numpy())\n",
    "#         new_soft_assignments[key] = hmm.predict_proba(embedding[key].numpy())\n",
    "\n",
    "#     return new_soft_assignments\n",
    "\n",
    "\n",
    "# def cluster_postprocessing(embedding, cluster_assignments, n_clusters=\"auto\"):\n",
    "#     \"\"\"Merges clusters using a hierarchical approach.\n",
    "\n",
    "#     Args:\n",
    "#         embedding (list): list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (list): list of cluster assignments per animal in the dataset.\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (list): list of new (merged) cluster assignments.\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Concatenate embeddings and cluster assignments in to unique np.ndarray objects\n",
    "#     concat_embedding = np.concatenate([tensor.numpy() for tensor in embedding.values()])\n",
    "#     hard_assignments = np.concatenate(\n",
    "#         [tensor.numpy().argmax(axis=1) for tensor in cluster_assignments.values()]\n",
    "#     )\n",
    "\n",
    "#     assert concat_embedding.shape[0] == hard_assignments.shape[0]\n",
    "\n",
    "#     # Get cluster centroids from the concatenated embeddings\n",
    "#     centroids = []\n",
    "#     for cluster in range(np.max(hard_assignments)):\n",
    "#         centroid = concat_embedding[hard_assignments == cluster]\n",
    "#         if len(centroid) == 0:\n",
    "#             continue\n",
    "#         centroid = np.mean(centroid, axis=0)\n",
    "#         centroids.append(centroid)\n",
    "\n",
    "#     centroids = np.stack(centroids)\n",
    "\n",
    "#     # Merge centroids using a hierarchical approach with the given resolution, and soft-assign instances to clusters\n",
    "#     if isinstance(n_clusters, int):\n",
    "#         new_soft_assignments = merge_and_smooth_clusters(\n",
    "#             n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     return new_soft_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# new_ass = cluster_postprocessing(\n",
    "#     vqvae_solution[0], \n",
    "#     vqvae_solution[1],\n",
    "#     n_clusters=12\n",
    "# )\n",
    "# hcc = new_ass['20191203_Day1_SI_JB08_Test_54'].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4d221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# # Cluster on the original embedding space\n",
    "# new_emb = umap.UMAP(n_components=2, n_neighbors=75).fit_transform(vqvae_solution[0]['20191203_Day1_SI_JB08_Test_54'])\n",
    "\n",
    "# sns.scatterplot(x=new_emb[:, 0], y=new_emb[:, 1], hue=hcc, palette=\"tab20\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How prevalent are these clusters?\n",
    "# from collections import Counter\n",
    "# print(Counter(hcc))\n",
    "\n",
    "# new_ass = hcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How often does the model change clusters?\n",
    "# from collections import defaultdict\n",
    "\n",
    "# lengths = defaultdict(list)\n",
    "# cur = 0\n",
    "# for i in range(1, len(new_ass)):\n",
    "#     if new_ass[i-1] == new_ass[i]:\n",
    "#         cur += 1\n",
    "#     else:\n",
    "#         lengths[new_ass[i-1]].append(cur)\n",
    "#         cur = 1\n",
    "\n",
    "# {key:np.mean(val) for key, val in lengths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Duration histograms per cluster\n",
    "# lengths_df = pd.DataFrame([lengths]).melt().explode(\"value\").astype(int)\n",
    "# sns.violinplot(data=lengths_df, x=\"variable\", y=\"value\")\n",
    "\n",
    "# plt.axhline(25, linestyle=\"--\", color=\"black\")\n",
    "        \n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
