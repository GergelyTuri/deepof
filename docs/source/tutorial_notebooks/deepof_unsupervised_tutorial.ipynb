{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b46f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")\n",
    "import deepof.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed73656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41506e",
   "metadata": {},
   "source": [
    "# DeepOF unsupervised pipeline: exploring the behavioral space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e24f1d",
   "metadata": {},
   "source": [
    "##### Tutorial index:\n",
    "    \n",
    "* Brief introduction to unsupervised analysis.\n",
    "* Load your previous project.\n",
    "* Running an unsupervised analysis with default parameters.\n",
    "* Understanding the different available models.\n",
    "* Cluster number selection.\n",
    "* Temporal and global embeddings.\n",
    "* Global separation dynamics.\n",
    "* Exploring cluster enrichment across conditions.\n",
    "* Exploring cluster dynamics across conditions.\n",
    "* Interpreting clusters using SHAP.\n",
    "* Exporting cluster video snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bcf28",
   "metadata": {},
   "source": [
    "### Brief introduction to unsupervised analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a7f23",
   "metadata": {},
   "source": [
    "### Load your previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ce3f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project = deepof.data.load_project(\"./deepof_FC_project_files/\")\n",
    "with open(\"./deepof_FC_project_files/Coordinates/FC_dataset_experimental_conditions.pkl\", \"rb\") as handle:\n",
    "    exp_conditions = pickle.load(handle)\n",
    "my_deepof_project._exp_conditions = exp_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52430042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE LAST SECTION OF THE FIRST TUTORIAL!\n",
    "\n",
    "# Check scales across animals. Can we detect to which animal a given time series belongs to?\n",
    "# Once happy with a solution, check that all animals show comparable cluster interpretations.\n",
    "\n",
    "# Add preprocessing options to include multiple animals, concatenated and together in a graph\n",
    "\n",
    "tt = my_deepof_project.get_coords(center=\"Center\", align=\"Spine_1\")\n",
    "# ss = my_deepof_project.get_coords(speed=1)\n",
    "\n",
    "# tt = cc.merge(ss)\n",
    "\n",
    "tt, _ = tt.preprocess(\n",
    "    window_size=25,\n",
    "    window_step=1,\n",
    "    test_videos=1,\n",
    "    scale=\"standard\",\n",
    "    handle_ids=\"split\", # \"concat\" uses bps from != animals as features, \"split\"\n",
    ")\n",
    "\n",
    "tt = (tt[0][:25000], tt[1][:25000], tt[2][:25000], tt[3][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d25a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/msmt6cvx5xl9t0p2qjd1tn65jtl6l8/T/ipykernel_65841/3184477051.py:14: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G).todense()\n"
     ]
    }
   ],
   "source": [
    "# SAME HERE: Last section of the last tutorial should explore tensor and graph preprocessing\n",
    "\n",
    "from deepof.utils import connect_mouse_topview\n",
    "import networkx as nx\n",
    "\n",
    "pp, G, to_preprocess, global_scaler = my_deepof_project.get_graph_dataset(\n",
    "   # animal_id=\"B\",\n",
    "    center=\"Center\",\n",
    "    align=\"Spine_1\",\n",
    "    preprocess=True,\n",
    "    scale=\"standard\"\n",
    ")\n",
    "\n",
    "adj = nx.adjacency_matrix(G).todense()\n",
    "pp = (pp[0][:25000], pp[1][:25000], pp[2][:25000], pp[3][:25000], pp[4][:25000], pp[5][:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98060f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens if we use a two-stage normalization approach?\n",
    "# 1) Normalize within animals to get rid of body size effects\n",
    "# 2) Normalize across animals to have similar stats across animals, and get the same clusters regardless of the animal\n",
    "# ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bc6d3",
   "metadata": {},
   "source": [
    "### Running an unsupervised analysis with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64d50a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 17:12:32.648861: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-01-26 17:12:32.648871: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-01-26 17:12:32.649195: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "/Users/lucas_miranda/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-01-26 17:12:48.273843: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 4 layers, found 5 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/data.py:1609\u001b[0m, in \u001b[0;36mCoordinates.deep_unsupervised_embedding\u001b[0;34m(preprocessed_object, adjacency_matrix, embedding_model, encoder_type, batch_size, latent_dim, epochs, log_history, log_hparams, n_components, kmeans_loss, temperature, contrastive_similarity_function, contrastive_loss_function, beta, tau, output_path, pretrained, save_checkpoints, save_weights, input_type, run, kl_annealing_mode, kl_warmup, reg_cat_clusters)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeep_unsupervised_embedding\u001b[39m(\n\u001b[1;32m   1546\u001b[0m     preprocessed_object: Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     reg_cat_clusters: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1571\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;124;03m\"\"\"Annotates coordinates using a deep unsupervised autoencoder.\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \n\u001b[1;32m   1574\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;124;03m        Tuple: Tuple containing all trained models. See specific model documentation under deepof.models for details.\u001b[39;00m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1609\u001b[0m     trained_models \u001b[38;5;241m=\u001b[39m \u001b[43mdeepof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder_fitting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessed_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessed_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_hparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_hparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkmeans_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkmeans_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrastive_similarity_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrastive_similarity_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrastive_loss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrastive_loss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_annealing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_annealing_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_warmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_warmup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreg_cat_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_cat_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# returns a list of trained tensorflow models\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/model_utils.py:1482\u001b[0m, in \u001b[0;36mautoencoder_fitting\u001b[0;34m(preprocessed_object, adjacency_matrix, embedding_model, encoder_type, batch_size, latent_dim, epochs, log_history, log_hparams, n_components, output_path, kmeans_loss, pretrained, save_checkpoints, save_weights, input_type, kl_annealing_mode, kl_warmup, reg_cat_clusters, temperature, contrastive_similarity_function, contrastive_loss_function, beta, tau, run)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;66;03m# If pretrained models are specified, load weights and return\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m     ae_full_model\u001b[38;5;241m.\u001b[39mbuild([X_train\u001b[38;5;241m.\u001b[39mshape, a_train\u001b[38;5;241m.\u001b[39mshape])\n\u001b[0;32m-> 1482\u001b[0m     \u001b[43mae_full_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ae_full_model\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/keras/saving/legacy/hdf5_format.py:812\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    810\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m     )\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[1;32m    820\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 4 layers, found 5 saved layers."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cons = my_deepof_project.deep_unsupervised_embedding(\n",
    "    pp,\n",
    "    adjacency_matrix=adj,\n",
    "    embedding_model=\"VQVAE\",\n",
    "    epochs=0,\n",
    "    encoder_type=\"recurrent\",\n",
    "    n_components=15,\n",
    "    latent_dim=8,\n",
    "    kl_warmup=10,\n",
    "    kl_annealing_mode=\"linear\",\n",
    "    batch_size=128,\n",
    "    kmeans_loss=0.0,\n",
    "    reg_cat_clusters=0.0,\n",
    "    pretrained=\"deepof_FC_project_files/Trained_models/deepof_unsupervised_VQVAE_recurrent_encodings_input_type=graph_kmeans_loss=0.0_encoding=8_k=15_20230126-094309_final_weights.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c7f8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "import pickle\n",
    "with open(\n",
    "    \"./deepof_FC_project_files/Trained_models/deepof_unsupervised_VQVAE_encoder_recurrent_encodings_input=graph_k=15_latdim=8_changepoints_False_kmeans_loss=0.0_run=0.pkl\", \"rb\"\n",
    ") as handle:\n",
    "    embeddings, soft_counts, breaks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e0b",
   "metadata": {},
   "source": [
    "### Understanding the different available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.vade.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ec89d",
   "metadata": {},
   "source": [
    "### Cluster number selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b207299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3175278c",
   "metadata": {},
   "source": [
    "### Visualizing temporal and global embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad890",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# embeddings = {}\n",
    "# soft_counts = {}\n",
    "\n",
    "for key in my_deepof_project.get_exp_conditions.keys():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    dd, _, _, _ = my_deepof_project.get_graph_dataset(\n",
    "        animal_id=\"B\",\n",
    "        precomputed_tab_dict=to_preprocess.filter_videos([key]),\n",
    "        preprocess=True,\n",
    "        scale=\"standard\",\n",
    "        shuffle=False,\n",
    "        pretrained_scaler=global_scaler,\n",
    "    )\n",
    "\n",
    "    dd = (dd[0], dd[1], dd[2], dd[3], dd[4], dd[5])\n",
    "\n",
    "#     dd = my_deepof_project.get_coords(\n",
    "#         center=\"Center\", align=\"Spine_1\"\n",
    "#     ).filter_id(\"B\").filter_videos([key]).preprocess(\n",
    "#         shuffle=False,\n",
    "#         test_videos=0,\n",
    "#     )\n",
    "#     dd = [dd[0], dd[0]]\n",
    "    \n",
    "    embeddings[key] = cons.encoder([dd[0], dd[1]])\n",
    "    soft_counts[key] = cons.grouper([dd[0], dd[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION: Load precomputed embeddings. \n",
    "# Solve this later by defaulting embedding saving to a known directory within the project path.\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# emb=\"TCN\"\n",
    "# run=2\n",
    "# with open(f\"../../../Results/CSDS_embeddings/deepof_unsupervised_VaDE_encoder_{emb}_encodings_input=graph_k=25_latdim=8_changepoints_False_kmeans_loss=0.0_run={run}.pkl\", \"rb\") as handle:\n",
    "#     embeddings, soft_counts, breaks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70433618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_patched_breaks(key):\n",
    "        \n",
    "#     tt, g, prep, _ = my_deepof_project.get_graph_dataset(center=\"Center\", align=\"Spine_1\", animal_id=\"B\", shuffle=False)\n",
    "#     tt = my_deepof_project.get_graph_dataset(\n",
    "#         precomputed_tab_dict=prep.filter_videos([key]),\n",
    "#         animal_id=\"B\",\n",
    "#         window_size=25,\n",
    "#         window_step=1,\n",
    "#         shuffle=False,\n",
    "#         automatic_changepoints=False,\n",
    "#         scale=\"standard\",\n",
    "#         test_videos=0,\n",
    "#     )[0]\n",
    "\n",
    "#     return ((~np.all(tt[0] == 0, axis=2)).sum(axis=1) > 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# breaks = {\n",
    "#     key: get_patched_breaks(key) for key in my_deepof_project.get_exp_conditions.keys()\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cb39d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 6}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(soft_counts['Test_10_FA_JB06_2_cut'].numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa50ace5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8)\n",
      "(10000,)\n",
      "(10000, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No hyperplanes of adequate size were found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtalk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m deepof\u001b[38;5;241m.\u001b[39mvisuals\u001b[38;5;241m.\u001b[39mplot_embeddings(\n\u001b[1;32m      8\u001b[0m     my_deepof_project, \n\u001b[1;32m      9\u001b[0m     embeddings, \n\u001b[1;32m     10\u001b[0m     soft_counts,\n\u001b[1;32m     11\u001b[0m     breaks,\n\u001b[1;32m     12\u001b[0m     aggregate_experiments\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     ax\u001b[38;5;241m=\u001b[39max1,\n\u001b[1;32m     14\u001b[0m     save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Set to True, or give a custom name, to save the plot\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m deepof\u001b[38;5;241m.\u001b[39mvisuals\u001b[38;5;241m.\u001b[39mplot_embeddings(\n\u001b[1;32m     18\u001b[0m     my_deepof_project,\n\u001b[1;32m     19\u001b[0m     embeddings, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Set to True, or give a custom name, to save the plot,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "File \u001b[0;32m~/PycharmProjects/deepof/deepof/visuals.py:798\u001b[0m, in \u001b[0;36mplot_embeddings\u001b[0;34m(coordinates, embeddings, soft_counts, breaks, min_confidence, bin_size, bin_index, aggregate_experiments, samples, show_aggregated_density, colour_by, show_break_size_as_radius, ax, save)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28mprint\u001b[39m(cluster_assignments\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# Reduce the dimensionality of the embeddings using UMAP. Set n_neighbors to a large\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# value to see a more global picture\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m reduced_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mdeepof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_hoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_UMAP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_assignments\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Generate unifier dataset using the reduced embeddings, experimental conditions\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# and the corresponding break lengths and cluster assignments\u001b[39;00m\n\u001b[1;32m    804\u001b[0m embedding_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    805\u001b[0m     {\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUMAP-1\u001b[39m\u001b[38;5;124m\"\u001b[39m: reduced_embeddings[:, \u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m     }\n\u001b[1;32m    814\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/umap/umap_.py:2896\u001b[0m, in \u001b[0;36mUMAP.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2895\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_knn_search_index\u001b[38;5;241m.\u001b[39m_angular_trees \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.12\u001b[39m\n\u001b[0;32m-> 2896\u001b[0m     indices, dists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_knn_search_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2900\u001b[0m dists \u001b[38;5;241m=\u001b[39m dists\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;66;03m# Remove any nearest neighbours who's distances are greater than our disconnection_distance\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/pynndescent/pynndescent_.py:1627\u001b[0m, in \u001b[0;36mNNDescent.query\u001b[0;34m(self, query_data, k, epsilon)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;124;03m\"\"\"Query the training graph_data for the k nearest neighbors\u001b[39;00m\n\u001b[1;32m   1597\u001b[0m \n\u001b[1;32m   1598\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;124;03m    training graph_data.\u001b[39;00m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_search_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1627\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_search_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sparse:\n\u001b[1;32m   1630\u001b[0m     \u001b[38;5;66;03m# Standard case\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_search_function\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/pynndescent/pynndescent_.py:981\u001b[0m, in \u001b[0;36mNNDescent._init_search_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    979\u001b[0m         best_trees \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rp_forest[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m best_tree_indices]\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rp_forest\n\u001b[0;32m--> 981\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_forest \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    982\u001b[0m             convert_tree_format(tree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    983\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m best_trees\n\u001b[1;32m    984\u001b[0m         ]\n\u001b[1;32m    986\u001b[0m nnz_pre_diversify \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neighbor_graph[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sparse:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/pynndescent/pynndescent_.py:982\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    979\u001b[0m         best_trees \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rp_forest[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m best_tree_indices]\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rp_forest\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_forest \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 982\u001b[0m             \u001b[43mconvert_tree_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m best_trees\n\u001b[1;32m    984\u001b[0m         ]\n\u001b[1;32m    986\u001b[0m nnz_pre_diversify \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neighbor_graph[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sparse:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/pynndescent/rp_trees.py:1158\u001b[0m, in \u001b[0;36mconvert_tree_format\u001b[0;34m(tree, data_size)\u001b[0m\n\u001b[1;32m   1155\u001b[0m is_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mhyperplanes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# dense hyperplanes\u001b[39;00m\n\u001b[0;32m-> 1158\u001b[0m     hyperplane_dim \u001b[38;5;241m=\u001b[39m \u001b[43mdense_hyperplane_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperplanes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1159\u001b[0m     hyperplanes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_nodes, hyperplane_dim), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m# sparse hyperplanes\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/pynndescent/rp_trees.py:1140\u001b[0m, in \u001b[0;36mdense_hyperplane_dim\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hyperplanes[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hyperplanes[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo hyperplanes of adequate size were found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No hyperplanes of adequate size were found!"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project, \n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=False,\n",
    "    ax=ax1,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot\n",
    ")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project,\n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=\"median\",\n",
    "    ax=ax2,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a99d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "    animal_id=\"B\",\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    "    frame_limit=150,\n",
    "    selected_cluster=7,\n",
    "    dpi=60,\n",
    "    #center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e61418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "    animal_id=\"B\",\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_56\",\n",
    "    frame_limit=150,\n",
    "    selected_cluster=7,\n",
    "    dpi=60,\n",
    "    #center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0e11",
   "metadata": {},
   "source": [
    "### Global separation dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6dc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habituation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec60eee",
   "metadata": {},
   "source": [
    "### Exploring cluster enrichment across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2258e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "deepof.visuals.plot_cluster_enrichment(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    normalize=True,\n",
    "    add_stats=\"Mann-Whitney\",\n",
    "    verbose=False,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950730c",
   "metadata": {},
   "source": [
    "### Exploring cluster dynamics across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d221e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transition matrices and heatmaps\n",
    "deepof.visuals.plot_transitions(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "   # cluster=False,\n",
    "    visualization=\"heatmaps\",\n",
    ")\n",
    "\n",
    "deepof.visuals.plot_transitions(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    visualization=\"networks\",\n",
    "    silence_diagonal=True,\n",
    ")\n",
    "\n",
    "# TODO:\n",
    "\n",
    "# Add option to use umap location on network plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c91cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Entropy plots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "\n",
    "deepof.visuals.plot_stationary_entropy(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787776a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_gantt(\n",
    "    my_deepof_project,\n",
    "    soft_counts=soft_counts,\n",
    "    experiment_id=\"20191204_Day2_SI_JB08_Test_54\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffb1d3",
   "metadata": {},
   "source": [
    "### Interpreting clusters using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45e7f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csds_chunk_stats, hard_counts = deepof.post_hoc.annotate_time_chunks(\n",
    "    deepof_project=my_deepof_project, \n",
    "    soft_counts=soft_counts, \n",
    "    breaks=breaks,\n",
    "    #supervised_annotations=csds_OF_supervised_annotations,\n",
    "    kin_derivative=1,\n",
    "    include_distances=True,\n",
    "    include_angles=True,\n",
    "    include_areas=True,s\n",
    "    aggregate=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csds_chunk_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8667112",
   "metadata": {},
   "outputs": [],
   "source": [
    "len((np.concatenate([soft for soft in soft_counts.values()])).max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_filter = np.concatenate([soft for soft in soft_counts.values()]).max(axis=1) > 0.99999997\n",
    "groups = deepof.post_hoc.chunk_cv_splitter(csds_chunk_stats, breaks, n_folds=6, qual_filter=qual_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cross-validate GBM training across videos\n",
    "cluster_clf = Pipeline([\n",
    "    (\"normalization\", StandardScaler()),\n",
    "    (\"oversampling\", SMOTE()),\n",
    "    (\"classifier\", CatBoostClassifier(verbose=False)),\n",
    "])\n",
    "cluster_gbm_performance = cross_validate(\n",
    "                              cluster_clf,\n",
    "                              csds_chunk_stats.values,\n",
    "                              hard_counts.values,\n",
    "                              scoring=[\n",
    "                                \"roc_auc_ovo_weighted\",\n",
    "                                \"roc_auc_ovr_weighted\",\n",
    "                              ],\n",
    "                              cv=groups,\n",
    "                              return_train_score=True,\n",
    "                              return_estimator=True,\n",
    "                              n_jobs=-1,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a087da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = []\n",
    "\n",
    "for clf, fold in zip(cluster_gbm_performance[\"estimator\"], groups):\n",
    "               \n",
    "    cm = confusion_matrix(\n",
    "        hard_counts.values[fold[1]], clf.predict(csds_chunk_stats.values[fold[1]]), labels=np.unique(hard_counts)\n",
    "    )\n",
    "        \n",
    "    confusion_matrices.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cluster_names = [\"cluster {}\".format(i) for i in range(15)]\n",
    "\n",
    "confusion_matrix = np.stack(confusion_matrices).sum(axis=0)\n",
    "confusion_matrix = confusion_matrix / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix, index=cluster_names, columns=cluster_names)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.title(\"Confusion matrix for multiclass state prediction\")\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\")\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"CSDS_OF_shap_confusion_matrix.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59be16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_balanced_accuracy(confusion_matrix, cluster_index):\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes balanced accuracy for a specific cluster given a confusion matrx\n",
    "    \n",
    "    Formula: ((( TP / (TP+FN) + (TN/(TN+FP))) / 2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    TP = confusion_matrix[cluster_index, cluster_index]\n",
    "    FP = confusion_matrix[:, cluster_index].sum() - TP\n",
    "    FN = confusion_matrix[cluster_index, :].sum() - TP\n",
    "    TN = confusion_matrix.sum() - TP - FP - FN\n",
    "    \n",
    "    return (( TP / (TP+FN)) + (TN / (TN+FP) )) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from collections import defaultdict\n",
    "\n",
    "# Plot performance\n",
    "@interact()\n",
    "def plot_supervised_mapping_eval(metric=[\"balanced_accuracy\"]):\n",
    "    \"plots supervised mapping performance across clusters\"\n",
    "    \n",
    "    dataset = defaultdict(list)\n",
    "\n",
    "    for cluster in range(7):\n",
    "        for cm in confusion_matrices:\n",
    "\n",
    "            ba = compute_balanced_accuracy(cm, cluster)\n",
    "            dataset[cluster].append(ba)\n",
    "\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "        \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    sns.set_context(\"talk\")\n",
    "    \n",
    "    plt.title(\"Supervised cluster mapping performance\")\n",
    "    \n",
    "    sns.barplot(data=dataset, ci=99, color=sns.color_palette(\"Blues\").as_hex()[-3])\n",
    "    plt.axhline(1/7, linestyle=\"--\", color=\"black\")\n",
    "    \n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Balanced accuracy\")\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig(\"CSDS_OF_shap_balanced_acc.pdf\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423019d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train full classifier for explainability testing\n",
    "full_cluster_clf = Pipeline([\n",
    "    (\"oversampling\", SMOTE()),\n",
    "    (\"classifier\", CatBoostClassifier(verbose=False)),\n",
    "])\n",
    "full_cluster_clf.fit(\n",
    "    csds_chunk_stats_filt.values,\n",
    "    hard_counts_filt.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804169d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_explain = csds_chunk_stats_filt.sample(5000)\n",
    "\n",
    "# Get SHAP values for the given model\n",
    "explainer = shap.KernelExplainer(full_cluster_clf.predict_proba, data=shap.kmeans(csds_chunk_stats_filt, 8))\n",
    "shap_values = explainer.shap_values(data_to_explain, nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbfec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as plt_colors\n",
    "\n",
    "# Plot swarm plots per cluster\n",
    "@interact()\n",
    "def plot_shap_swarm_per_cluster(\n",
    "    cluster=[\"all\"] + list(range(7)), aggregate=False, save=False,\n",
    "):\n",
    "\n",
    "    with open(\"./csds_SI_cluster_shap_cat.pkl\", \"rb\") as handle:\n",
    "        shap_vals, full_clf, data_redux = pickle.load(handle)\n",
    "\n",
    "    if aggregate:\n",
    "        \n",
    "        cluster_gbm_full_shap, feature_dict, agg_data = group_shap_values(csds_chunk_stats_filt, shap_vals)\n",
    "        shap_vals = cluster_gbm_full_shap[fold]\n",
    "        \n",
    "        multi_shap_vals = [\n",
    "            np.squeeze(x)\n",
    "            for x in np.split(shap_vals[0], shap_vals.shape[-1], axis=-1)\n",
    "        ]\n",
    "        \n",
    "    if cluster != \"all\":\n",
    "        shap_vals = shap_vals[cluster]\n",
    "        \n",
    "    print(len(shap_vals))\n",
    "        \n",
    "    cmap = plt_colors.ListedColormap(np.array([\"#1f77b4\", \"#ff7f0e\"]))\n",
    "    shap.summary_plot(\n",
    "        shap_vals,\n",
    "        (data_redux if not aggregate else agg_data),\n",
    "        max_display=10,\n",
    "        show=False,\n",
    "        feature_names=(list(csds_chunk_stats_filt.columns) if not aggregate else list(feature_dict.keys())),\n",
    "    )\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            \"csds_shap_cluster_SI={}.pdf\".format(cluster),\n",
    "            format=\"pdf\",\n",
    "            dpi=1000,\n",
    "            bbox_inches=\"tight\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c18109",
   "metadata": {},
   "source": [
    "### Exporting cluster video snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: they should be saved to a directory that makes sense!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = [i for i in os.listdir(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\")]\n",
    "\n",
    "for f in files:\n",
    "    cur = pd.read_hdf(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f)\n",
    "    \n",
    "    cur.to_csv(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f[:-2] + \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556420",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = cons.encoder([pp[0][:25000], pp[1][:25000]])\n",
    "cls = cons.quantizer([pp[0][:25000], pp[1][:25000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73357da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap = umap.UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=250,\n",
    "    min_dist=1.0,\n",
    ").fit_transform(emb.numpy())\n",
    "# umap = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cls.numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd99b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tt = GaussianMixture(n_components=5, covariance_type=\"diag\", reg_covar=1e-04).fit(emb.numpy())\n",
    "#means = tt.means_\n",
    "#means = cons.get_gmm_params['means'].numpy()\n",
    "\n",
    "sns.scatterplot(x=umap[:, 0], y=umap[:, 1], hue=cls.numpy().argmax(axis=1), palette=\"tab20\")\n",
    "# means = cons.get_layer(\"grouper\").get_layer(\"gaussian_mixture_latent\").c_mu.numpy()\n",
    "# sns.scatterplot(x=means[:,0], y=means[:,1], s=250, c=\"black\")\n",
    "\n",
    "plt.title(\"DeepOF embeddings\")\n",
    "\n",
    "# plt.legend(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21006d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.get_gmm_params['weights'].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.split(np.concatenate(tt), np.cumsum([i.shape[0] for k,i in vqvae_solution[0].items() if k in list(cc.keys())]))\n",
    "\n",
    "for i in tt:\n",
    "    print(i.shape)\n",
    "    print(np.max(np.abs(i.mean(axis=0))))\n",
    "    print(np.mean(np.abs(i.std(axis=0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21595771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# with open(\"../train_models/deepof_unsupervised_VQVAE_encodings_input=coords_k=100_latdim=8_kmeans_loss=0.0_run=1.pkl\", \"rb\") as handle:\n",
    "#     vqvae_solution = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from deepof.post_hoc import get_transitions\n",
    "# from hmmlearn.hmm import GaussianHMM\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# def merge_and_smooth_clusters(\n",
    "#     n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "# ):\n",
    "#     \"\"\"Merges the current clusters using a hierarchical agglomerative approach, and smoothens using a Gaussian HMM.\n",
    "\n",
    "#     Args:\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "#         centroids (np.ndarray): precomputed means per cluster.\n",
    "#         embedding (tabdict): original deepof.TableDict object containing unsupervised embeddings.\n",
    "#         concat_embedding (np.ndarray): concatenated list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (tabdict): original deepof.TableDict object containing cluster assignments.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (np.ndarray): concatenated postprocessed assignments for all animals in the dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Merge clusters ussing a hierarchical agglomerative approach\n",
    "#     new_hard_assignments = AgglomerativeClustering(\n",
    "#         n_clusters=n_clusters, compute_distances=True\n",
    "#     ).fit_predict(centroids)\n",
    "#     cluster_predictor = LinearDiscriminantAnalysis().fit(\n",
    "#         centroids, new_hard_assignments\n",
    "#     )\n",
    "#     centroids = cluster_predictor.means_\n",
    "#     new_soft_assignments = cluster_predictor.predict_proba(concat_embedding)\n",
    "\n",
    "#     # Rebuild the soft assignments dictionary per experimental animal\n",
    "#     new_soft_assignments = np.split(\n",
    "#         new_soft_assignments,\n",
    "#         np.cumsum([i.shape[0] for i in embedding.values()]),\n",
    "#     )\n",
    "#     new_soft_assignments = {\n",
    "#         key: val for key, val in zip(cluster_assignments.keys(), new_soft_assignments)\n",
    "#     }\n",
    "\n",
    "#     # Smooth assignments across time using a Gaussian HMM on the embeddings, with priors based on the clustering results\n",
    "#     for key, val in tqdm(new_soft_assignments.items()):\n",
    "\n",
    "#         hmm = GaussianHMM(\n",
    "#             startprob_prior=np.unique(val.argmax(axis=1), return_counts=True)[1],\n",
    "#             transmat_prior=get_transitions(val.argmax(axis=1), n_states=n_clusters) + 10,\n",
    "#             means_prior=centroids,\n",
    "#             n_components=n_clusters,\n",
    "#             covariance_type=\"diag\",\n",
    "#             n_iter=100,\n",
    "#             tol=0.0001,\n",
    "#         )\n",
    "        \n",
    "#         hmm.fit(embedding[key].numpy())\n",
    "#         new_soft_assignments[key] = hmm.predict_proba(embedding[key].numpy())\n",
    "\n",
    "#     return new_soft_assignments\n",
    "\n",
    "\n",
    "# def cluster_postprocessing(embedding, cluster_assignments, n_clusters=\"auto\"):\n",
    "#     \"\"\"Merges clusters using a hierarchical approach.\n",
    "\n",
    "#     Args:\n",
    "#         embedding (list): list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (list): list of cluster assignments per animal in the dataset.\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (list): list of new (merged) cluster assignments.\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Concatenate embeddings and cluster assignments in to unique np.ndarray objects\n",
    "#     concat_embedding = np.concatenate([tensor.numpy() for tensor in embedding.values()])\n",
    "#     hard_assignments = np.concatenate(\n",
    "#         [tensor.numpy().argmax(axis=1) for tensor in cluster_assignments.values()]\n",
    "#     )\n",
    "\n",
    "#     assert concat_embedding.shape[0] == hard_assignments.shape[0]\n",
    "\n",
    "#     # Get cluster centroids from the concatenated embeddings\n",
    "#     centroids = []\n",
    "#     for cluster in range(np.max(hard_assignments)):\n",
    "#         centroid = concat_embedding[hard_assignments == cluster]\n",
    "#         if len(centroid) == 0:\n",
    "#             continue\n",
    "#         centroid = np.mean(centroid, axis=0)\n",
    "#         centroids.append(centroid)\n",
    "\n",
    "#     centroids = np.stack(centroids)\n",
    "\n",
    "#     # Merge centroids using a hierarchical approach with the given resolution, and soft-assign instances to clusters\n",
    "#     if isinstance(n_clusters, int):\n",
    "#         new_soft_assignments = merge_and_smooth_clusters(\n",
    "#             n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     return new_soft_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# new_ass = cluster_postprocessing(\n",
    "#     vqvae_solution[0], \n",
    "#     vqvae_solution[1],\n",
    "#     n_clusters=12\n",
    "# )\n",
    "# hcc = new_ass['20191203_Day1_SI_JB08_Test_54'].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4d221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# # Cluster on the original embedding space\n",
    "# new_emb = umap.UMAP(n_components=2, n_neighbors=75).fit_transform(vqvae_solution[0]['20191203_Day1_SI_JB08_Test_54'])\n",
    "\n",
    "# sns.scatterplot(x=new_emb[:, 0], y=new_emb[:, 1], hue=hcc, palette=\"tab20\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How prevalent are these clusters?\n",
    "# from collections import Counter\n",
    "# print(Counter(hcc))\n",
    "\n",
    "# new_ass = hcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How often does the model change clusters?\n",
    "# from collections import defaultdict\n",
    "\n",
    "# lengths = defaultdict(list)\n",
    "# cur = 0\n",
    "# for i in range(1, len(new_ass)):\n",
    "#     if new_ass[i-1] == new_ass[i]:\n",
    "#         cur += 1\n",
    "#     else:\n",
    "#         lengths[new_ass[i-1]].append(cur)\n",
    "#         cur = 1\n",
    "\n",
    "# {key:np.mean(val) for key, val in lengths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Duration histograms per cluster\n",
    "# lengths_df = pd.DataFrame([lengths]).melt().explode(\"value\").astype(int)\n",
    "# sns.violinplot(data=lengths_df, x=\"variable\", y=\"value\")\n",
    "\n",
    "# plt.axhline(25, linestyle=\"--\", color=\"black\")\n",
    "        \n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepof",
   "language": "python",
   "name": "deepof"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
