{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b46f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")\n",
    "import deepof.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed73656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41506e",
   "metadata": {},
   "source": [
    "# DeepOF unsupervised pipeline: exploring the behavioral space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e24f1d",
   "metadata": {},
   "source": [
    "##### Tutorial index:\n",
    "    \n",
    "* Brief introduction to unsupervised analysis.\n",
    "* Load your previous project.\n",
    "* Running an unsupervised analysis with default parameters.\n",
    "* Understanding the different available models.\n",
    "* Cluster number selection.\n",
    "* Temporal and global embeddings.\n",
    "* Global separation dynamics.\n",
    "* Exploring cluster enrichment across conditions.\n",
    "* Exploring cluster dynamics across conditions.\n",
    "* Interpreting clusters using SHAP.\n",
    "* Exporting cluster video snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bcf28",
   "metadata": {},
   "source": [
    "### Brief introduction to unsupervised analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a7f23",
   "metadata": {},
   "source": [
    "### Load your previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce3f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deepof_project = deepof.data.load_project(\"./deepof_FC_project_files/\")\n",
    "\n",
    "import pickle\n",
    "with open(\"./deepof_FC_project_files/Coordinates/FC_dataset_experimental_conditions.pkl\", \"rb\") as handle:\n",
    "    exp_conditions = pickle.load(handle)\n",
    "my_deepof_project._exp_conditions = exp_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52430042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE LAST SECTION OF THE FIRST TUTORIAL!\n",
    "\n",
    "# Check scales across animals. Can we detect to which animal a given time series belongs to?\n",
    "# Once happy with a solution, check that all animals show comparable cluster interpretations.\n",
    "\n",
    "# Add preprocessing options to include multiple animals, concatenated and together in a graph\n",
    "\n",
    "tt = my_deepof_project.get_coords(center=\"Center\", align=\"Spine_1\")\n",
    "tt, _ = tt.preprocess(\n",
    "    window_size=13,\n",
    "    window_step=1,\n",
    "    test_videos=1,\n",
    "    scale=\"standard\",\n",
    "    handle_ids=\"split\", # \"concat\" uses bps from != animals as features, \"split\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d25a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/msmt6cvx5xl9t0p2qjd1tn65jtl6l8/T/ipykernel_19303/2788945428.py:15: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G).todense()\n"
     ]
    }
   ],
   "source": [
    "# SAME HERE: Last section of the last tutorial should explore tensor and graph preprocessing\n",
    "from deepof.utils import connect_mouse_topview\n",
    "import networkx as nx\n",
    "\n",
    "pp, G, to_preprocess, global_scaler = my_deepof_project.get_graph_dataset(\n",
    "   # animal_id=\"B\",\n",
    "    center=\"Center\",\n",
    "    align=\"Spine_1\",\n",
    "    window_size=13,\n",
    "    window_step=1,\n",
    "    preprocess=True,\n",
    "    scale=\"standard\"\n",
    ")\n",
    "\n",
    "adj = nx.adjacency_matrix(G).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98060f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens if we use a two-stage normalization approach?\n",
    "# 1) Normalize within animals to get rid of body size effects\n",
    "# 2) Normalize across animals to have similar stats across animals, and get the same clusters regardless of the animal\n",
    "# ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bc6d3",
   "metadata": {},
   "source": [
    "### Running an unsupervised analysis with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d50a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 22:30:09.737718: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-01-27 22:30:09.737727: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-01-27 22:30:09.737810: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "/Users/lucas_miranda/Library/Caches/pypoetry/virtualenvs/deepof-qxwF8hwh-py3.9/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-01-27 22:30:39.215275: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 16.5 s, total: 47.3 s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cons = my_deepof_project.deep_unsupervised_embedding(\n",
    "    pp,\n",
    "    adjacency_matrix=adj,\n",
    "    embedding_model=\"VaDE\",\n",
    "    epochs=2,\n",
    "    encoder_type=\"recurrent\",\n",
    "    n_components=15,\n",
    "    latent_dim=8,\n",
    "    kl_warmup=10,\n",
    "    kl_annealing_mode=\"linear\",\n",
    "    batch_size=1024,\n",
    "    kmeans_loss=0.0,\n",
    "    reg_cat_clusters=0.0,\n",
    "  pretrained=\"deepof_FC_project_files/Trained_models/train_models/trained_weights/deepof_unsupervised_VaDE_recurrent_encodings_input_type=graph_kmeans_loss=0.0_encoding=8_k=15_20230126-094309_final_weights.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "import pickle\n",
    "with open(\n",
    "    \"./deepof_FC_project_files/Trained_models/train_models/deepof_unsupervised_VaDE_encoder_recurrent_encodings_input=graph_k=15_latdim=8_changepoints_False_kmeans_loss=0.0_run=0.pkl\", \"rb\"\n",
    ") as handle:\n",
    "    embeddings, soft_counts, breaks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e0b",
   "metadata": {},
   "source": [
    "### Understanding the different available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.vade.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ec89d",
   "metadata": {},
   "source": [
    "### Cluster number selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175278c",
   "metadata": {},
   "source": [
    "### Visualizing temporal and global embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d2ad890",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [05:06<00:00,  7.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# INCORPORATE AS A METHOD FOR EACH MODEL\n",
    "embeddings, soft_counts, breaks = deepof.model_utils.embedding_per_video(my_deepof_project, to_preprocess, cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50ace5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project, \n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=False,\n",
    "    samples=100,\n",
    "    ax=ax1,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot\n",
    ")\n",
    "\n",
    "deepof.visuals.plot_embeddings(\n",
    "    my_deepof_project,\n",
    "    embeddings, \n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    aggregate_experiments=\"median\",\n",
    "    exp_condition=\"Sex\",\n",
    "    show_aggregated_density=False,\n",
    "    ax=ax2,\n",
    "    save=False, # Set to True, or give a custom name, to save the plot,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a99d1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "  #  animal_id=\"B\",\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"Test_10_FA_JB06_2_cut\",\n",
    "    frame_limit=6,\n",
    "    selected_cluster=10,\n",
    "    dpi=50,\n",
    "    center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e61418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "video = deepof.visuals.animate_skeleton(\n",
    "    my_deepof_project,\n",
    "    embedding=embeddings,\n",
    "  #  animal_id=\"B\",\n",
    "    cluster_assignments=soft_counts,\n",
    "    experiment_id=\"Test_10_FA_JB07_cut\",\n",
    "    frame_limit=10,\n",
    "    selected_cluster=0,\n",
    "    dpi=60,\n",
    "    center=\"Center\",\n",
    "    #align=\"Spine_1\",\n",
    "    #embedding=[emb_B, emb_W],\n",
    ")\n",
    "\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0e11",
   "metadata": {},
   "source": [
    "### Global separation dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6dc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habituation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec60eee",
   "metadata": {},
   "source": [
    "### Exploring cluster enrichment across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2258e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "deepof.visuals.plot_cluster_enrichment(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    normalize=True,\n",
    "    add_stats=\"Mann-Whitney\",\n",
    "    exp_condition=\"Sex\",\n",
    "    verbose=False,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950730c",
   "metadata": {},
   "source": [
    "### Exploring cluster dynamics across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695cb25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transition matrices and heatmaps\n",
    "deepof.visuals.plot_transitions(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "   # cluster=False,\n",
    "    visualization=\"heatmaps\",\n",
    "    exp_condition=\"Condition\",\n",
    ")\n",
    "\n",
    "deepof.visuals.plot_transitions(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    visualization=\"networks\",\n",
    "    silence_diagonal=True,\n",
    "    exp_condition=\"Condition\",\n",
    ")\n",
    "\n",
    "# TODO:\n",
    "\n",
    "# Add option to use umap location on network plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c91cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Entropy plots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "\n",
    "deepof.visuals.plot_stationary_entropy(\n",
    "    my_deepof_project,\n",
    "    embeddings,\n",
    "    soft_counts,\n",
    "    breaks,\n",
    "    exp_condition=\"Condition\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787776a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "deepof.visuals.plot_gantt(\n",
    "    my_deepof_project,\n",
    "    soft_counts=soft_counts,\n",
    "    experiment_id=\"Test_10_FA_JB06_2_cut\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffb1d3",
   "metadata": {},
   "source": [
    "### Interpreting clusters using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90430483",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csds_chunk_stats, hard_counts = deepof.post_hoc.annotate_time_chunks(\n",
    "    deepof_project=my_deepof_project, \n",
    "    soft_counts=soft_counts, \n",
    "    breaks=breaks,\n",
    "    #supervised_annotations=csds_OF_supervised_annotations,\n",
    "    kin_derivative=1,\n",
    "    include_distances=True,\n",
    "    include_angles=True,\n",
    "    include_areas=True,s\n",
    "    aggregate=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csds_chunk_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len((np.concatenate([soft for soft in soft_counts.values()])).max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_filter = np.concatenate([soft for soft in soft_counts.values()]).max(axis=1) > 0.99999997\n",
    "groups = deepof.post_hoc.chunk_cv_splitter(csds_chunk_stats, breaks, n_folds=6, qual_filter=qual_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7773d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cross-validate GBM training across videos\n",
    "cluster_clf = Pipeline([\n",
    "    (\"normalization\", StandardScaler()),\n",
    "    (\"oversampling\", SMOTE()),\n",
    "    (\"classifier\", CatBoostClassifier(verbose=False)),\n",
    "])\n",
    "cluster_gbm_performance = cross_validate(\n",
    "                              cluster_clf,\n",
    "                              csds_chunk_stats.values,\n",
    "                              hard_counts.values,\n",
    "                              scoring=[\n",
    "                                \"roc_auc_ovo_weighted\",\n",
    "                                \"roc_auc_ovr_weighted\",\n",
    "                              ],\n",
    "                              cv=groups,\n",
    "                              return_train_score=True,\n",
    "                              return_estimator=True,\n",
    "                              n_jobs=-1,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = []\n",
    "\n",
    "for clf, fold in zip(cluster_gbm_performance[\"estimator\"], groups):\n",
    "               \n",
    "    cm = confusion_matrix(\n",
    "        hard_counts.values[fold[1]], clf.predict(csds_chunk_stats.values[fold[1]]), labels=np.unique(hard_counts)\n",
    "    )\n",
    "        \n",
    "    confusion_matrices.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9511cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cluster_names = [\"cluster {}\".format(i) for i in range(15)]\n",
    "\n",
    "confusion_matrix = np.stack(confusion_matrices).sum(axis=0)\n",
    "confusion_matrix = confusion_matrix / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix, index=cluster_names, columns=cluster_names)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.title(\"Confusion matrix for multiclass state prediction\")\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\")\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"CSDS_OF_shap_confusion_matrix.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_balanced_accuracy(confusion_matrix, cluster_index):\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes balanced accuracy for a specific cluster given a confusion matrx\n",
    "    \n",
    "    Formula: ((( TP / (TP+FN) + (TN/(TN+FP))) / 2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    TP = confusion_matrix[cluster_index, cluster_index]\n",
    "    FP = confusion_matrix[:, cluster_index].sum() - TP\n",
    "    FN = confusion_matrix[cluster_index, :].sum() - TP\n",
    "    TN = confusion_matrix.sum() - TP - FP - FN\n",
    "    \n",
    "    return (( TP / (TP+FN)) + (TN / (TN+FP) )) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5064142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from collections import defaultdict\n",
    "\n",
    "# Plot performance\n",
    "@interact()\n",
    "def plot_supervised_mapping_eval(metric=[\"balanced_accuracy\"]):\n",
    "    \"plots supervised mapping performance across clusters\"\n",
    "    \n",
    "    dataset = defaultdict(list)\n",
    "\n",
    "    for cluster in range(7):\n",
    "        for cm in confusion_matrices:\n",
    "\n",
    "            ba = compute_balanced_accuracy(cm, cluster)\n",
    "            dataset[cluster].append(ba)\n",
    "\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "        \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    sns.set_context(\"talk\")\n",
    "    \n",
    "    plt.title(\"Supervised cluster mapping performance\")\n",
    "    \n",
    "    sns.barplot(data=dataset, ci=99, color=sns.color_palette(\"Blues\").as_hex()[-3])\n",
    "    plt.axhline(1/7, linestyle=\"--\", color=\"black\")\n",
    "    \n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Balanced accuracy\")\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig(\"CSDS_OF_shap_balanced_acc.pdf\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train full classifier for explainability testing\n",
    "full_cluster_clf = Pipeline([\n",
    "    (\"oversampling\", SMOTE()),\n",
    "    (\"classifier\", CatBoostClassifier(verbose=False)),\n",
    "])\n",
    "full_cluster_clf.fit(\n",
    "    csds_chunk_stats_filt.values,\n",
    "    hard_counts_filt.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_explain = csds_chunk_stats_filt.sample(5000)\n",
    "\n",
    "# Get SHAP values for the given model\n",
    "explainer = shap.KernelExplainer(full_cluster_clf.predict_proba, data=shap.kmeans(csds_chunk_stats_filt, 8))\n",
    "shap_values = explainer.shap_values(data_to_explain, nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as plt_colors\n",
    "\n",
    "# Plot swarm plots per cluster\n",
    "@interact()\n",
    "def plot_shap_swarm_per_cluster(\n",
    "    cluster=[\"all\"] + list(range(7)), aggregate=False, save=False,\n",
    "):\n",
    "\n",
    "    with open(\"./csds_SI_cluster_shap_cat.pkl\", \"rb\") as handle:\n",
    "        shap_vals, full_clf, data_redux = pickle.load(handle)\n",
    "\n",
    "    if aggregate:\n",
    "        \n",
    "        cluster_gbm_full_shap, feature_dict, agg_data = group_shap_values(csds_chunk_stats_filt, shap_vals)\n",
    "        shap_vals = cluster_gbm_full_shap[fold]\n",
    "        \n",
    "        multi_shap_vals = [\n",
    "            np.squeeze(x)\n",
    "            for x in np.split(shap_vals[0], shap_vals.shape[-1], axis=-1)\n",
    "        ]\n",
    "        \n",
    "    if cluster != \"all\":\n",
    "        shap_vals = shap_vals[cluster]\n",
    "        \n",
    "    print(len(shap_vals))\n",
    "        \n",
    "    cmap = plt_colors.ListedColormap(np.array([\"#1f77b4\", \"#ff7f0e\"]))\n",
    "    shap.summary_plot(\n",
    "        shap_vals,\n",
    "        (data_redux if not aggregate else agg_data),\n",
    "        max_display=10,\n",
    "        show=False,\n",
    "        feature_names=(list(csds_chunk_stats_filt.columns) if not aggregate else list(feature_dict.keys())),\n",
    "    )\n",
    "    if save:\n",
    "        plt.savefig(\n",
    "            \"csds_shap_cluster_SI={}.pdf\".format(cluster),\n",
    "            format=\"pdf\",\n",
    "            dpi=1000,\n",
    "            bbox_inches=\"tight\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c18109",
   "metadata": {},
   "source": [
    "### Exporting cluster video snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: they should be saved to a directory that makes sense!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = [i for i in os.listdir(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\")]\n",
    "\n",
    "for f in files:\n",
    "    cur = pd.read_hdf(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f)\n",
    "    \n",
    "    cur.to_csv(\"../../Desktop/deepOF_datasets/Tagged_videos/Data_for_deepof_OF/JB08_files_OF/Tables/\" + f[:-2] + \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556420",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = cons.encoder([pp[0][:25000], pp[1][:25000]])\n",
    "cls = cons.quantizer([pp[0][:25000], pp[1][:25000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73357da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap = umap.UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=250,\n",
    "    min_dist=1.0,\n",
    ").fit_transform(emb.numpy())\n",
    "# umap = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cls.numpy().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd99b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tt = GaussianMixture(n_components=5, covariance_type=\"diag\", reg_covar=1e-04).fit(emb.numpy())\n",
    "#means = tt.means_\n",
    "#means = cons.get_gmm_params['means'].numpy()\n",
    "\n",
    "sns.scatterplot(x=umap[:, 0], y=umap[:, 1], hue=cls.numpy().argmax(axis=1), palette=\"tab20\")\n",
    "# means = cons.get_layer(\"grouper\").get_layer(\"gaussian_mixture_latent\").c_mu.numpy()\n",
    "# sns.scatterplot(x=means[:,0], y=means[:,1], s=250, c=\"black\")\n",
    "\n",
    "plt.title(\"DeepOF embeddings\")\n",
    "\n",
    "# plt.legend(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21006d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons.get_gmm_params['weights'].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.split(np.concatenate(tt), np.cumsum([i.shape[0] for k,i in vqvae_solution[0].items() if k in list(cc.keys())]))\n",
    "\n",
    "for i in tt:\n",
    "    print(i.shape)\n",
    "    print(np.max(np.abs(i.mean(axis=0))))\n",
    "    print(np.mean(np.abs(i.std(axis=0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21595771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# with open(\"../train_models/deepof_unsupervised_VQVAE_encodings_input=coords_k=100_latdim=8_kmeans_loss=0.0_run=1.pkl\", \"rb\") as handle:\n",
    "#     vqvae_solution = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from deepof.post_hoc import get_transitions\n",
    "# from hmmlearn.hmm import GaussianHMM\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# def merge_and_smooth_clusters(\n",
    "#     n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "# ):\n",
    "#     \"\"\"Merges the current clusters using a hierarchical agglomerative approach, and smoothens using a Gaussian HMM.\n",
    "\n",
    "#     Args:\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "#         centroids (np.ndarray): precomputed means per cluster.\n",
    "#         embedding (tabdict): original deepof.TableDict object containing unsupervised embeddings.\n",
    "#         concat_embedding (np.ndarray): concatenated list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (tabdict): original deepof.TableDict object containing cluster assignments.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (np.ndarray): concatenated postprocessed assignments for all animals in the dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Merge clusters ussing a hierarchical agglomerative approach\n",
    "#     new_hard_assignments = AgglomerativeClustering(\n",
    "#         n_clusters=n_clusters, compute_distances=True\n",
    "#     ).fit_predict(centroids)\n",
    "#     cluster_predictor = LinearDiscriminantAnalysis().fit(\n",
    "#         centroids, new_hard_assignments\n",
    "#     )\n",
    "#     centroids = cluster_predictor.means_\n",
    "#     new_soft_assignments = cluster_predictor.predict_proba(concat_embedding)\n",
    "\n",
    "#     # Rebuild the soft assignments dictionary per experimental animal\n",
    "#     new_soft_assignments = np.split(\n",
    "#         new_soft_assignments,\n",
    "#         np.cumsum([i.shape[0] for i in embedding.values()]),\n",
    "#     )\n",
    "#     new_soft_assignments = {\n",
    "#         key: val for key, val in zip(cluster_assignments.keys(), new_soft_assignments)\n",
    "#     }\n",
    "\n",
    "#     # Smooth assignments across time using a Gaussian HMM on the embeddings, with priors based on the clustering results\n",
    "#     for key, val in tqdm(new_soft_assignments.items()):\n",
    "\n",
    "#         hmm = GaussianHMM(\n",
    "#             startprob_prior=np.unique(val.argmax(axis=1), return_counts=True)[1],\n",
    "#             transmat_prior=get_transitions(val.argmax(axis=1), n_states=n_clusters) + 10,\n",
    "#             means_prior=centroids,\n",
    "#             n_components=n_clusters,\n",
    "#             covariance_type=\"diag\",\n",
    "#             n_iter=100,\n",
    "#             tol=0.0001,\n",
    "#         )\n",
    "        \n",
    "#         hmm.fit(embedding[key].numpy())\n",
    "#         new_soft_assignments[key] = hmm.predict_proba(embedding[key].numpy())\n",
    "\n",
    "#     return new_soft_assignments\n",
    "\n",
    "\n",
    "# def cluster_postprocessing(embedding, cluster_assignments, n_clusters=\"auto\"):\n",
    "#     \"\"\"Merges clusters using a hierarchical approach.\n",
    "\n",
    "#     Args:\n",
    "#         embedding (list): list of embeddings per animal in the dataset.\n",
    "#         cluster_assignments (list): list of cluster assignments per animal in the dataset.\n",
    "#         n_clusters (int): number of clusters to report.\n",
    "\n",
    "#     Returns:\n",
    "#         new_soft_assignments (list): list of new (merged) cluster assignments.\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Concatenate embeddings and cluster assignments in to unique np.ndarray objects\n",
    "#     concat_embedding = np.concatenate([tensor.numpy() for tensor in embedding.values()])\n",
    "#     hard_assignments = np.concatenate(\n",
    "#         [tensor.numpy().argmax(axis=1) for tensor in cluster_assignments.values()]\n",
    "#     )\n",
    "\n",
    "#     assert concat_embedding.shape[0] == hard_assignments.shape[0]\n",
    "\n",
    "#     # Get cluster centroids from the concatenated embeddings\n",
    "#     centroids = []\n",
    "#     for cluster in range(np.max(hard_assignments)):\n",
    "#         centroid = concat_embedding[hard_assignments == cluster]\n",
    "#         if len(centroid) == 0:\n",
    "#             continue\n",
    "#         centroid = np.mean(centroid, axis=0)\n",
    "#         centroids.append(centroid)\n",
    "\n",
    "#     centroids = np.stack(centroids)\n",
    "\n",
    "#     # Merge centroids using a hierarchical approach with the given resolution, and soft-assign instances to clusters\n",
    "#     if isinstance(n_clusters, int):\n",
    "#         new_soft_assignments = merge_and_smooth_clusters(\n",
    "#             n_clusters, centroids, embedding, concat_embedding, cluster_assignments\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     return new_soft_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# new_ass = cluster_postprocessing(\n",
    "#     vqvae_solution[0], \n",
    "#     vqvae_solution[1],\n",
    "#     n_clusters=12\n",
    "# )\n",
    "# hcc = new_ass['20191203_Day1_SI_JB08_Test_54'].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4d221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# # Cluster on the original embedding space\n",
    "# new_emb = umap.UMAP(n_components=2, n_neighbors=75).fit_transform(vqvae_solution[0]['20191203_Day1_SI_JB08_Test_54'])\n",
    "\n",
    "# sns.scatterplot(x=new_emb[:, 0], y=new_emb[:, 1], hue=hcc, palette=\"tab20\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How prevalent are these clusters?\n",
    "# from collections import Counter\n",
    "# print(Counter(hcc))\n",
    "\n",
    "# new_ass = hcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How often does the model change clusters?\n",
    "# from collections import defaultdict\n",
    "\n",
    "# lengths = defaultdict(list)\n",
    "# cur = 0\n",
    "# for i in range(1, len(new_ass)):\n",
    "#     if new_ass[i-1] == new_ass[i]:\n",
    "#         cur += 1\n",
    "#     else:\n",
    "#         lengths[new_ass[i-1]].append(cur)\n",
    "#         cur = 1\n",
    "\n",
    "# {key:np.mean(val) for key, val in lengths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Duration histograms per cluster\n",
    "# lengths_df = pd.DataFrame([lengths]).melt().explode(\"value\").astype(int)\n",
    "# sns.violinplot(data=lengths_df, x=\"variable\", y=\"value\")\n",
    "\n",
    "# plt.axhline(25, linestyle=\"--\", color=\"black\")\n",
    "        \n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepof",
   "language": "python",
   "name": "deepof"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
